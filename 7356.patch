From efb6b53bd39fe73ef6d076c4a6c3a1c6370d7828 Mon Sep 17 00:00:00 2001
From: Peter Huang <huangzhenqiu0825@gmail.com>
Date: Sun, 21 Oct 2018 21:00:08 -0700
Subject: [PATCH 1/8] pull upstream master

---
 .../formats/parquet/generated/ArrayItem.java  | 308 ++++++++
 .../flink/formats/parquet/generated/Bar.java  | 238 ++++++
 .../formats/parquet/generated/MapItem.java    | 308 ++++++++
 .../parquet/generated/NestedRecord.java       | 702 ++++++++++++++++++
 .../parquet/generated/SimpleRecord.java       | 378 ++++++++++
 5 files changed, 1934 insertions(+)
 create mode 100644 flink-formats/flink-parquet/src/test/java/org/apache/flink/formats/parquet/generated/ArrayItem.java
 create mode 100644 flink-formats/flink-parquet/src/test/java/org/apache/flink/formats/parquet/generated/Bar.java
 create mode 100644 flink-formats/flink-parquet/src/test/java/org/apache/flink/formats/parquet/generated/MapItem.java
 create mode 100644 flink-formats/flink-parquet/src/test/java/org/apache/flink/formats/parquet/generated/NestedRecord.java
 create mode 100644 flink-formats/flink-parquet/src/test/java/org/apache/flink/formats/parquet/generated/SimpleRecord.java

diff --git a/flink-formats/flink-parquet/src/test/java/org/apache/flink/formats/parquet/generated/ArrayItem.java b/flink-formats/flink-parquet/src/test/java/org/apache/flink/formats/parquet/generated/ArrayItem.java
new file mode 100644
index 000000000000..ad9cc589fda7
--- /dev/null
+++ b/flink-formats/flink-parquet/src/test/java/org/apache/flink/formats/parquet/generated/ArrayItem.java
@@ -0,0 +1,308 @@
+/**
+ * Autogenerated by Avro
+ *
+ * DO NOT EDIT DIRECTLY
+ */
+package org.apache.flink.formats.parquet.generated;
+
+import org.apache.avro.specific.SpecificData;
+import org.apache.avro.message.BinaryMessageEncoder;
+import org.apache.avro.message.BinaryMessageDecoder;
+import org.apache.avro.message.SchemaStore;
+
+@SuppressWarnings("all")
+@org.apache.avro.specific.AvroGenerated
+public class ArrayItem extends org.apache.avro.specific.SpecificRecordBase implements org.apache.avro.specific.SpecificRecord {
+  private static final long serialVersionUID = 3888914259224596748L;
+  public static final org.apache.avro.Schema SCHEMA$ = new org.apache.avro.Schema.Parser().parse("{\"type\":\"record\",\"name\":\"ArrayItem\",\"namespace\":\"org.apache.flink.formats.parquet.generated\",\"fields\":[{\"name\":\"type\",\"type\":[\"null\",\"string\"]},{\"name\":\"value\",\"type\":[\"null\",\"string\"]}]}");
+  public static org.apache.avro.Schema getClassSchema() { return SCHEMA$; }
+
+  private static SpecificData MODEL$ = new SpecificData();
+
+  private static final BinaryMessageEncoder<ArrayItem> ENCODER =
+      new BinaryMessageEncoder<ArrayItem>(MODEL$, SCHEMA$);
+
+  private static final BinaryMessageDecoder<ArrayItem> DECODER =
+      new BinaryMessageDecoder<ArrayItem>(MODEL$, SCHEMA$);
+
+  /**
+   * Return the BinaryMessageDecoder instance used by this class.
+   */
+  public static BinaryMessageDecoder<ArrayItem> getDecoder() {
+    return DECODER;
+  }
+
+  /**
+   * Create a new BinaryMessageDecoder instance for this class that uses the specified {@link SchemaStore}.
+   * @param resolver a {@link SchemaStore} used to find schemas by fingerprint
+   */
+  public static BinaryMessageDecoder<ArrayItem> createDecoder(SchemaStore resolver) {
+    return new BinaryMessageDecoder<ArrayItem>(MODEL$, SCHEMA$, resolver);
+  }
+
+  /** Serializes this ArrayItem to a ByteBuffer. */
+  public java.nio.ByteBuffer toByteBuffer() throws java.io.IOException {
+    return ENCODER.encode(this);
+  }
+
+  /** Deserializes a ArrayItem from a ByteBuffer. */
+  public static ArrayItem fromByteBuffer(
+      java.nio.ByteBuffer b) throws java.io.IOException {
+    return DECODER.decode(b);
+  }
+
+  @Deprecated public java.lang.CharSequence type;
+  @Deprecated public java.lang.CharSequence value;
+
+  /**
+   * Default constructor.  Note that this does not initialize fields
+   * to their default values from the schema.  If that is desired then
+   * one should use <code>newBuilder()</code>.
+   */
+  public ArrayItem() {}
+
+  /**
+   * All-args constructor.
+   * @param type The new value for type
+   * @param value The new value for value
+   */
+  public ArrayItem(java.lang.CharSequence type, java.lang.CharSequence value) {
+    this.type = type;
+    this.value = value;
+  }
+
+  public org.apache.avro.Schema getSchema() { return SCHEMA$; }
+  // Used by DatumWriter.  Applications should not call.
+  public java.lang.Object get(int field$) {
+    switch (field$) {
+    case 0: return type;
+    case 1: return value;
+    default: throw new org.apache.avro.AvroRuntimeException("Bad index");
+    }
+  }
+
+  // Used by DatumReader.  Applications should not call.
+  @SuppressWarnings(value="unchecked")
+  public void put(int field$, java.lang.Object value$) {
+    switch (field$) {
+    case 0: type = (java.lang.CharSequence)value$; break;
+    case 1: value = (java.lang.CharSequence)value$; break;
+    default: throw new org.apache.avro.AvroRuntimeException("Bad index");
+    }
+  }
+
+  /**
+   * Gets the value of the 'type' field.
+   * @return The value of the 'type' field.
+   */
+  public java.lang.CharSequence getType() {
+    return type;
+  }
+
+  /**
+   * Sets the value of the 'type' field.
+   * @param value the value to set.
+   */
+  public void setType(java.lang.CharSequence value) {
+    this.type = value;
+  }
+
+  /**
+   * Gets the value of the 'value' field.
+   * @return The value of the 'value' field.
+   */
+  public java.lang.CharSequence getValue() {
+    return value;
+  }
+
+  /**
+   * Sets the value of the 'value' field.
+   * @param value the value to set.
+   */
+  public void setValue(java.lang.CharSequence value) {
+    this.value = value;
+  }
+
+  /**
+   * Creates a new ArrayItem RecordBuilder.
+   * @return A new ArrayItem RecordBuilder
+   */
+  public static org.apache.flink.formats.parquet.generated.ArrayItem.Builder newBuilder() {
+    return new org.apache.flink.formats.parquet.generated.ArrayItem.Builder();
+  }
+
+  /**
+   * Creates a new ArrayItem RecordBuilder by copying an existing Builder.
+   * @param other The existing builder to copy.
+   * @return A new ArrayItem RecordBuilder
+   */
+  public static org.apache.flink.formats.parquet.generated.ArrayItem.Builder newBuilder(org.apache.flink.formats.parquet.generated.ArrayItem.Builder other) {
+    return new org.apache.flink.formats.parquet.generated.ArrayItem.Builder(other);
+  }
+
+  /**
+   * Creates a new ArrayItem RecordBuilder by copying an existing ArrayItem instance.
+   * @param other The existing instance to copy.
+   * @return A new ArrayItem RecordBuilder
+   */
+  public static org.apache.flink.formats.parquet.generated.ArrayItem.Builder newBuilder(org.apache.flink.formats.parquet.generated.ArrayItem other) {
+    return new org.apache.flink.formats.parquet.generated.ArrayItem.Builder(other);
+  }
+
+  /**
+   * RecordBuilder for ArrayItem instances.
+   */
+  public static class Builder extends org.apache.avro.specific.SpecificRecordBuilderBase<ArrayItem>
+    implements org.apache.avro.data.RecordBuilder<ArrayItem> {
+
+    private java.lang.CharSequence type;
+    private java.lang.CharSequence value;
+
+    /** Creates a new Builder */
+    private Builder() {
+      super(SCHEMA$);
+    }
+
+    /**
+     * Creates a Builder by copying an existing Builder.
+     * @param other The existing Builder to copy.
+     */
+    private Builder(org.apache.flink.formats.parquet.generated.ArrayItem.Builder other) {
+      super(other);
+      if (isValidValue(fields()[0], other.type)) {
+        this.type = data().deepCopy(fields()[0].schema(), other.type);
+        fieldSetFlags()[0] = true;
+      }
+      if (isValidValue(fields()[1], other.value)) {
+        this.value = data().deepCopy(fields()[1].schema(), other.value);
+        fieldSetFlags()[1] = true;
+      }
+    }
+
+    /**
+     * Creates a Builder by copying an existing ArrayItem instance
+     * @param other The existing instance to copy.
+     */
+    private Builder(org.apache.flink.formats.parquet.generated.ArrayItem other) {
+            super(SCHEMA$);
+      if (isValidValue(fields()[0], other.type)) {
+        this.type = data().deepCopy(fields()[0].schema(), other.type);
+        fieldSetFlags()[0] = true;
+      }
+      if (isValidValue(fields()[1], other.value)) {
+        this.value = data().deepCopy(fields()[1].schema(), other.value);
+        fieldSetFlags()[1] = true;
+      }
+    }
+
+    /**
+      * Gets the value of the 'type' field.
+      * @return The value.
+      */
+    public java.lang.CharSequence getType() {
+      return type;
+    }
+
+    /**
+      * Sets the value of the 'type' field.
+      * @param value The value of 'type'.
+      * @return This builder.
+      */
+    public org.apache.flink.formats.parquet.generated.ArrayItem.Builder setType(java.lang.CharSequence value) {
+      validate(fields()[0], value);
+      this.type = value;
+      fieldSetFlags()[0] = true;
+      return this;
+    }
+
+    /**
+      * Checks whether the 'type' field has been set.
+      * @return True if the 'type' field has been set, false otherwise.
+      */
+    public boolean hasType() {
+      return fieldSetFlags()[0];
+    }
+
+
+    /**
+      * Clears the value of the 'type' field.
+      * @return This builder.
+      */
+    public org.apache.flink.formats.parquet.generated.ArrayItem.Builder clearType() {
+      type = null;
+      fieldSetFlags()[0] = false;
+      return this;
+    }
+
+    /**
+      * Gets the value of the 'value' field.
+      * @return The value.
+      */
+    public java.lang.CharSequence getValue() {
+      return value;
+    }
+
+    /**
+      * Sets the value of the 'value' field.
+      * @param value The value of 'value'.
+      * @return This builder.
+      */
+    public org.apache.flink.formats.parquet.generated.ArrayItem.Builder setValue(java.lang.CharSequence value) {
+      validate(fields()[1], value);
+      this.value = value;
+      fieldSetFlags()[1] = true;
+      return this;
+    }
+
+    /**
+      * Checks whether the 'value' field has been set.
+      * @return True if the 'value' field has been set, false otherwise.
+      */
+    public boolean hasValue() {
+      return fieldSetFlags()[1];
+    }
+
+
+    /**
+      * Clears the value of the 'value' field.
+      * @return This builder.
+      */
+    public org.apache.flink.formats.parquet.generated.ArrayItem.Builder clearValue() {
+      value = null;
+      fieldSetFlags()[1] = false;
+      return this;
+    }
+
+    @Override
+    @SuppressWarnings("unchecked")
+    public ArrayItem build() {
+      try {
+        ArrayItem record = new ArrayItem();
+        record.type = fieldSetFlags()[0] ? this.type : (java.lang.CharSequence) defaultValue(fields()[0]);
+        record.value = fieldSetFlags()[1] ? this.value : (java.lang.CharSequence) defaultValue(fields()[1]);
+        return record;
+      } catch (java.lang.Exception e) {
+        throw new org.apache.avro.AvroRuntimeException(e);
+      }
+    }
+  }
+
+  @SuppressWarnings("unchecked")
+  private static final org.apache.avro.io.DatumWriter<ArrayItem>
+    WRITER$ = (org.apache.avro.io.DatumWriter<ArrayItem>)MODEL$.createDatumWriter(SCHEMA$);
+
+  @Override public void writeExternal(java.io.ObjectOutput out)
+    throws java.io.IOException {
+    WRITER$.write(this, SpecificData.getEncoder(out));
+  }
+
+  @SuppressWarnings("unchecked")
+  private static final org.apache.avro.io.DatumReader<ArrayItem>
+    READER$ = (org.apache.avro.io.DatumReader<ArrayItem>)MODEL$.createDatumReader(SCHEMA$);
+
+  @Override public void readExternal(java.io.ObjectInput in)
+    throws java.io.IOException {
+    READER$.read(this, SpecificData.getDecoder(in));
+  }
+
+}
diff --git a/flink-formats/flink-parquet/src/test/java/org/apache/flink/formats/parquet/generated/Bar.java b/flink-formats/flink-parquet/src/test/java/org/apache/flink/formats/parquet/generated/Bar.java
new file mode 100644
index 000000000000..e84f8b6e2492
--- /dev/null
+++ b/flink-formats/flink-parquet/src/test/java/org/apache/flink/formats/parquet/generated/Bar.java
@@ -0,0 +1,238 @@
+/**
+ * Autogenerated by Avro
+ *
+ * DO NOT EDIT DIRECTLY
+ */
+package org.apache.flink.formats.parquet.generated;
+
+import org.apache.avro.specific.SpecificData;
+import org.apache.avro.message.BinaryMessageEncoder;
+import org.apache.avro.message.BinaryMessageDecoder;
+import org.apache.avro.message.SchemaStore;
+
+@SuppressWarnings("all")
+@org.apache.avro.specific.AvroGenerated
+public class Bar extends org.apache.avro.specific.SpecificRecordBase implements org.apache.avro.specific.SpecificRecord {
+  private static final long serialVersionUID = 1175980344330188560L;
+  public static final org.apache.avro.Schema SCHEMA$ = new org.apache.avro.Schema.Parser().parse("{\"type\":\"record\",\"name\":\"Bar\",\"namespace\":\"org.apache.flink.formats.parquet.generated\",\"fields\":[{\"name\":\"spam\",\"type\":[\"null\",\"long\"],\"default\":null}]}");
+  public static org.apache.avro.Schema getClassSchema() { return SCHEMA$; }
+
+  private static SpecificData MODEL$ = new SpecificData();
+
+  private static final BinaryMessageEncoder<Bar> ENCODER =
+      new BinaryMessageEncoder<Bar>(MODEL$, SCHEMA$);
+
+  private static final BinaryMessageDecoder<Bar> DECODER =
+      new BinaryMessageDecoder<Bar>(MODEL$, SCHEMA$);
+
+  /**
+   * Return the BinaryMessageDecoder instance used by this class.
+   */
+  public static BinaryMessageDecoder<Bar> getDecoder() {
+    return DECODER;
+  }
+
+  /**
+   * Create a new BinaryMessageDecoder instance for this class that uses the specified {@link SchemaStore}.
+   * @param resolver a {@link SchemaStore} used to find schemas by fingerprint
+   */
+  public static BinaryMessageDecoder<Bar> createDecoder(SchemaStore resolver) {
+    return new BinaryMessageDecoder<Bar>(MODEL$, SCHEMA$, resolver);
+  }
+
+  /** Serializes this Bar to a ByteBuffer. */
+  public java.nio.ByteBuffer toByteBuffer() throws java.io.IOException {
+    return ENCODER.encode(this);
+  }
+
+  /** Deserializes a Bar from a ByteBuffer. */
+  public static Bar fromByteBuffer(
+      java.nio.ByteBuffer b) throws java.io.IOException {
+    return DECODER.decode(b);
+  }
+
+  @Deprecated public java.lang.Long spam;
+
+  /**
+   * Default constructor.  Note that this does not initialize fields
+   * to their default values from the schema.  If that is desired then
+   * one should use <code>newBuilder()</code>.
+   */
+  public Bar() {}
+
+  /**
+   * All-args constructor.
+   * @param spam The new value for spam
+   */
+  public Bar(java.lang.Long spam) {
+    this.spam = spam;
+  }
+
+  public org.apache.avro.Schema getSchema() { return SCHEMA$; }
+  // Used by DatumWriter.  Applications should not call.
+  public java.lang.Object get(int field$) {
+    switch (field$) {
+    case 0: return spam;
+    default: throw new org.apache.avro.AvroRuntimeException("Bad index");
+    }
+  }
+
+  // Used by DatumReader.  Applications should not call.
+  @SuppressWarnings(value="unchecked")
+  public void put(int field$, java.lang.Object value$) {
+    switch (field$) {
+    case 0: spam = (java.lang.Long)value$; break;
+    default: throw new org.apache.avro.AvroRuntimeException("Bad index");
+    }
+  }
+
+  /**
+   * Gets the value of the 'spam' field.
+   * @return The value of the 'spam' field.
+   */
+  public java.lang.Long getSpam() {
+    return spam;
+  }
+
+  /**
+   * Sets the value of the 'spam' field.
+   * @param value the value to set.
+   */
+  public void setSpam(java.lang.Long value) {
+    this.spam = value;
+  }
+
+  /**
+   * Creates a new Bar RecordBuilder.
+   * @return A new Bar RecordBuilder
+   */
+  public static org.apache.flink.formats.parquet.generated.Bar.Builder newBuilder() {
+    return new org.apache.flink.formats.parquet.generated.Bar.Builder();
+  }
+
+  /**
+   * Creates a new Bar RecordBuilder by copying an existing Builder.
+   * @param other The existing builder to copy.
+   * @return A new Bar RecordBuilder
+   */
+  public static org.apache.flink.formats.parquet.generated.Bar.Builder newBuilder(org.apache.flink.formats.parquet.generated.Bar.Builder other) {
+    return new org.apache.flink.formats.parquet.generated.Bar.Builder(other);
+  }
+
+  /**
+   * Creates a new Bar RecordBuilder by copying an existing Bar instance.
+   * @param other The existing instance to copy.
+   * @return A new Bar RecordBuilder
+   */
+  public static org.apache.flink.formats.parquet.generated.Bar.Builder newBuilder(org.apache.flink.formats.parquet.generated.Bar other) {
+    return new org.apache.flink.formats.parquet.generated.Bar.Builder(other);
+  }
+
+  /**
+   * RecordBuilder for Bar instances.
+   */
+  public static class Builder extends org.apache.avro.specific.SpecificRecordBuilderBase<Bar>
+    implements org.apache.avro.data.RecordBuilder<Bar> {
+
+    private java.lang.Long spam;
+
+    /** Creates a new Builder */
+    private Builder() {
+      super(SCHEMA$);
+    }
+
+    /**
+     * Creates a Builder by copying an existing Builder.
+     * @param other The existing Builder to copy.
+     */
+    private Builder(org.apache.flink.formats.parquet.generated.Bar.Builder other) {
+      super(other);
+      if (isValidValue(fields()[0], other.spam)) {
+        this.spam = data().deepCopy(fields()[0].schema(), other.spam);
+        fieldSetFlags()[0] = true;
+      }
+    }
+
+    /**
+     * Creates a Builder by copying an existing Bar instance
+     * @param other The existing instance to copy.
+     */
+    private Builder(org.apache.flink.formats.parquet.generated.Bar other) {
+            super(SCHEMA$);
+      if (isValidValue(fields()[0], other.spam)) {
+        this.spam = data().deepCopy(fields()[0].schema(), other.spam);
+        fieldSetFlags()[0] = true;
+      }
+    }
+
+    /**
+      * Gets the value of the 'spam' field.
+      * @return The value.
+      */
+    public java.lang.Long getSpam() {
+      return spam;
+    }
+
+    /**
+      * Sets the value of the 'spam' field.
+      * @param value The value of 'spam'.
+      * @return This builder.
+      */
+    public org.apache.flink.formats.parquet.generated.Bar.Builder setSpam(java.lang.Long value) {
+      validate(fields()[0], value);
+      this.spam = value;
+      fieldSetFlags()[0] = true;
+      return this;
+    }
+
+    /**
+      * Checks whether the 'spam' field has been set.
+      * @return True if the 'spam' field has been set, false otherwise.
+      */
+    public boolean hasSpam() {
+      return fieldSetFlags()[0];
+    }
+
+
+    /**
+      * Clears the value of the 'spam' field.
+      * @return This builder.
+      */
+    public org.apache.flink.formats.parquet.generated.Bar.Builder clearSpam() {
+      spam = null;
+      fieldSetFlags()[0] = false;
+      return this;
+    }
+
+    @Override
+    @SuppressWarnings("unchecked")
+    public Bar build() {
+      try {
+        Bar record = new Bar();
+        record.spam = fieldSetFlags()[0] ? this.spam : (java.lang.Long) defaultValue(fields()[0]);
+        return record;
+      } catch (java.lang.Exception e) {
+        throw new org.apache.avro.AvroRuntimeException(e);
+      }
+    }
+  }
+
+  @SuppressWarnings("unchecked")
+  private static final org.apache.avro.io.DatumWriter<Bar>
+    WRITER$ = (org.apache.avro.io.DatumWriter<Bar>)MODEL$.createDatumWriter(SCHEMA$);
+
+  @Override public void writeExternal(java.io.ObjectOutput out)
+    throws java.io.IOException {
+    WRITER$.write(this, SpecificData.getEncoder(out));
+  }
+
+  @SuppressWarnings("unchecked")
+  private static final org.apache.avro.io.DatumReader<Bar>
+    READER$ = (org.apache.avro.io.DatumReader<Bar>)MODEL$.createDatumReader(SCHEMA$);
+
+  @Override public void readExternal(java.io.ObjectInput in)
+    throws java.io.IOException {
+    READER$.read(this, SpecificData.getDecoder(in));
+  }
+
+}
diff --git a/flink-formats/flink-parquet/src/test/java/org/apache/flink/formats/parquet/generated/MapItem.java b/flink-formats/flink-parquet/src/test/java/org/apache/flink/formats/parquet/generated/MapItem.java
new file mode 100644
index 000000000000..5c055e132452
--- /dev/null
+++ b/flink-formats/flink-parquet/src/test/java/org/apache/flink/formats/parquet/generated/MapItem.java
@@ -0,0 +1,308 @@
+/**
+ * Autogenerated by Avro
+ *
+ * DO NOT EDIT DIRECTLY
+ */
+package org.apache.flink.formats.parquet.generated;
+
+import org.apache.avro.specific.SpecificData;
+import org.apache.avro.message.BinaryMessageEncoder;
+import org.apache.avro.message.BinaryMessageDecoder;
+import org.apache.avro.message.SchemaStore;
+
+@SuppressWarnings("all")
+@org.apache.avro.specific.AvroGenerated
+public class MapItem extends org.apache.avro.specific.SpecificRecordBase implements org.apache.avro.specific.SpecificRecord {
+  private static final long serialVersionUID = 8474786067806918777L;
+  public static final org.apache.avro.Schema SCHEMA$ = new org.apache.avro.Schema.Parser().parse("{\"type\":\"record\",\"name\":\"MapItem\",\"namespace\":\"org.apache.flink.formats.parquet.generated\",\"fields\":[{\"name\":\"type\",\"type\":[\"null\",\"string\"]},{\"name\":\"value\",\"type\":[\"null\",\"string\"]}]}");
+  public static org.apache.avro.Schema getClassSchema() { return SCHEMA$; }
+
+  private static SpecificData MODEL$ = new SpecificData();
+
+  private static final BinaryMessageEncoder<MapItem> ENCODER =
+      new BinaryMessageEncoder<MapItem>(MODEL$, SCHEMA$);
+
+  private static final BinaryMessageDecoder<MapItem> DECODER =
+      new BinaryMessageDecoder<MapItem>(MODEL$, SCHEMA$);
+
+  /**
+   * Return the BinaryMessageDecoder instance used by this class.
+   */
+  public static BinaryMessageDecoder<MapItem> getDecoder() {
+    return DECODER;
+  }
+
+  /**
+   * Create a new BinaryMessageDecoder instance for this class that uses the specified {@link SchemaStore}.
+   * @param resolver a {@link SchemaStore} used to find schemas by fingerprint
+   */
+  public static BinaryMessageDecoder<MapItem> createDecoder(SchemaStore resolver) {
+    return new BinaryMessageDecoder<MapItem>(MODEL$, SCHEMA$, resolver);
+  }
+
+  /** Serializes this MapItem to a ByteBuffer. */
+  public java.nio.ByteBuffer toByteBuffer() throws java.io.IOException {
+    return ENCODER.encode(this);
+  }
+
+  /** Deserializes a MapItem from a ByteBuffer. */
+  public static MapItem fromByteBuffer(
+      java.nio.ByteBuffer b) throws java.io.IOException {
+    return DECODER.decode(b);
+  }
+
+  @Deprecated public java.lang.CharSequence type;
+  @Deprecated public java.lang.CharSequence value;
+
+  /**
+   * Default constructor.  Note that this does not initialize fields
+   * to their default values from the schema.  If that is desired then
+   * one should use <code>newBuilder()</code>.
+   */
+  public MapItem() {}
+
+  /**
+   * All-args constructor.
+   * @param type The new value for type
+   * @param value The new value for value
+   */
+  public MapItem(java.lang.CharSequence type, java.lang.CharSequence value) {
+    this.type = type;
+    this.value = value;
+  }
+
+  public org.apache.avro.Schema getSchema() { return SCHEMA$; }
+  // Used by DatumWriter.  Applications should not call.
+  public java.lang.Object get(int field$) {
+    switch (field$) {
+    case 0: return type;
+    case 1: return value;
+    default: throw new org.apache.avro.AvroRuntimeException("Bad index");
+    }
+  }
+
+  // Used by DatumReader.  Applications should not call.
+  @SuppressWarnings(value="unchecked")
+  public void put(int field$, java.lang.Object value$) {
+    switch (field$) {
+    case 0: type = (java.lang.CharSequence)value$; break;
+    case 1: value = (java.lang.CharSequence)value$; break;
+    default: throw new org.apache.avro.AvroRuntimeException("Bad index");
+    }
+  }
+
+  /**
+   * Gets the value of the 'type' field.
+   * @return The value of the 'type' field.
+   */
+  public java.lang.CharSequence getType() {
+    return type;
+  }
+
+  /**
+   * Sets the value of the 'type' field.
+   * @param value the value to set.
+   */
+  public void setType(java.lang.CharSequence value) {
+    this.type = value;
+  }
+
+  /**
+   * Gets the value of the 'value' field.
+   * @return The value of the 'value' field.
+   */
+  public java.lang.CharSequence getValue() {
+    return value;
+  }
+
+  /**
+   * Sets the value of the 'value' field.
+   * @param value the value to set.
+   */
+  public void setValue(java.lang.CharSequence value) {
+    this.value = value;
+  }
+
+  /**
+   * Creates a new MapItem RecordBuilder.
+   * @return A new MapItem RecordBuilder
+   */
+  public static org.apache.flink.formats.parquet.generated.MapItem.Builder newBuilder() {
+    return new org.apache.flink.formats.parquet.generated.MapItem.Builder();
+  }
+
+  /**
+   * Creates a new MapItem RecordBuilder by copying an existing Builder.
+   * @param other The existing builder to copy.
+   * @return A new MapItem RecordBuilder
+   */
+  public static org.apache.flink.formats.parquet.generated.MapItem.Builder newBuilder(org.apache.flink.formats.parquet.generated.MapItem.Builder other) {
+    return new org.apache.flink.formats.parquet.generated.MapItem.Builder(other);
+  }
+
+  /**
+   * Creates a new MapItem RecordBuilder by copying an existing MapItem instance.
+   * @param other The existing instance to copy.
+   * @return A new MapItem RecordBuilder
+   */
+  public static org.apache.flink.formats.parquet.generated.MapItem.Builder newBuilder(org.apache.flink.formats.parquet.generated.MapItem other) {
+    return new org.apache.flink.formats.parquet.generated.MapItem.Builder(other);
+  }
+
+  /**
+   * RecordBuilder for MapItem instances.
+   */
+  public static class Builder extends org.apache.avro.specific.SpecificRecordBuilderBase<MapItem>
+    implements org.apache.avro.data.RecordBuilder<MapItem> {
+
+    private java.lang.CharSequence type;
+    private java.lang.CharSequence value;
+
+    /** Creates a new Builder */
+    private Builder() {
+      super(SCHEMA$);
+    }
+
+    /**
+     * Creates a Builder by copying an existing Builder.
+     * @param other The existing Builder to copy.
+     */
+    private Builder(org.apache.flink.formats.parquet.generated.MapItem.Builder other) {
+      super(other);
+      if (isValidValue(fields()[0], other.type)) {
+        this.type = data().deepCopy(fields()[0].schema(), other.type);
+        fieldSetFlags()[0] = true;
+      }
+      if (isValidValue(fields()[1], other.value)) {
+        this.value = data().deepCopy(fields()[1].schema(), other.value);
+        fieldSetFlags()[1] = true;
+      }
+    }
+
+    /**
+     * Creates a Builder by copying an existing MapItem instance
+     * @param other The existing instance to copy.
+     */
+    private Builder(org.apache.flink.formats.parquet.generated.MapItem other) {
+            super(SCHEMA$);
+      if (isValidValue(fields()[0], other.type)) {
+        this.type = data().deepCopy(fields()[0].schema(), other.type);
+        fieldSetFlags()[0] = true;
+      }
+      if (isValidValue(fields()[1], other.value)) {
+        this.value = data().deepCopy(fields()[1].schema(), other.value);
+        fieldSetFlags()[1] = true;
+      }
+    }
+
+    /**
+      * Gets the value of the 'type' field.
+      * @return The value.
+      */
+    public java.lang.CharSequence getType() {
+      return type;
+    }
+
+    /**
+      * Sets the value of the 'type' field.
+      * @param value The value of 'type'.
+      * @return This builder.
+      */
+    public org.apache.flink.formats.parquet.generated.MapItem.Builder setType(java.lang.CharSequence value) {
+      validate(fields()[0], value);
+      this.type = value;
+      fieldSetFlags()[0] = true;
+      return this;
+    }
+
+    /**
+      * Checks whether the 'type' field has been set.
+      * @return True if the 'type' field has been set, false otherwise.
+      */
+    public boolean hasType() {
+      return fieldSetFlags()[0];
+    }
+
+
+    /**
+      * Clears the value of the 'type' field.
+      * @return This builder.
+      */
+    public org.apache.flink.formats.parquet.generated.MapItem.Builder clearType() {
+      type = null;
+      fieldSetFlags()[0] = false;
+      return this;
+    }
+
+    /**
+      * Gets the value of the 'value' field.
+      * @return The value.
+      */
+    public java.lang.CharSequence getValue() {
+      return value;
+    }
+
+    /**
+      * Sets the value of the 'value' field.
+      * @param value The value of 'value'.
+      * @return This builder.
+      */
+    public org.apache.flink.formats.parquet.generated.MapItem.Builder setValue(java.lang.CharSequence value) {
+      validate(fields()[1], value);
+      this.value = value;
+      fieldSetFlags()[1] = true;
+      return this;
+    }
+
+    /**
+      * Checks whether the 'value' field has been set.
+      * @return True if the 'value' field has been set, false otherwise.
+      */
+    public boolean hasValue() {
+      return fieldSetFlags()[1];
+    }
+
+
+    /**
+      * Clears the value of the 'value' field.
+      * @return This builder.
+      */
+    public org.apache.flink.formats.parquet.generated.MapItem.Builder clearValue() {
+      value = null;
+      fieldSetFlags()[1] = false;
+      return this;
+    }
+
+    @Override
+    @SuppressWarnings("unchecked")
+    public MapItem build() {
+      try {
+        MapItem record = new MapItem();
+        record.type = fieldSetFlags()[0] ? this.type : (java.lang.CharSequence) defaultValue(fields()[0]);
+        record.value = fieldSetFlags()[1] ? this.value : (java.lang.CharSequence) defaultValue(fields()[1]);
+        return record;
+      } catch (java.lang.Exception e) {
+        throw new org.apache.avro.AvroRuntimeException(e);
+      }
+    }
+  }
+
+  @SuppressWarnings("unchecked")
+  private static final org.apache.avro.io.DatumWriter<MapItem>
+    WRITER$ = (org.apache.avro.io.DatumWriter<MapItem>)MODEL$.createDatumWriter(SCHEMA$);
+
+  @Override public void writeExternal(java.io.ObjectOutput out)
+    throws java.io.IOException {
+    WRITER$.write(this, SpecificData.getEncoder(out));
+  }
+
+  @SuppressWarnings("unchecked")
+  private static final org.apache.avro.io.DatumReader<MapItem>
+    READER$ = (org.apache.avro.io.DatumReader<MapItem>)MODEL$.createDatumReader(SCHEMA$);
+
+  @Override public void readExternal(java.io.ObjectInput in)
+    throws java.io.IOException {
+    READER$.read(this, SpecificData.getDecoder(in));
+  }
+
+}
diff --git a/flink-formats/flink-parquet/src/test/java/org/apache/flink/formats/parquet/generated/NestedRecord.java b/flink-formats/flink-parquet/src/test/java/org/apache/flink/formats/parquet/generated/NestedRecord.java
new file mode 100644
index 000000000000..8563e5473d91
--- /dev/null
+++ b/flink-formats/flink-parquet/src/test/java/org/apache/flink/formats/parquet/generated/NestedRecord.java
@@ -0,0 +1,702 @@
+/**
+ * Autogenerated by Avro
+ *
+ * DO NOT EDIT DIRECTLY
+ */
+package org.apache.flink.formats.parquet.generated;
+
+import org.apache.avro.specific.SpecificData;
+import org.apache.avro.message.BinaryMessageEncoder;
+import org.apache.avro.message.BinaryMessageDecoder;
+import org.apache.avro.message.SchemaStore;
+
+@SuppressWarnings("all")
+@org.apache.avro.specific.AvroGenerated
+public class NestedRecord extends org.apache.avro.specific.SpecificRecordBase implements org.apache.avro.specific.SpecificRecord {
+  private static final long serialVersionUID = -8594103348180891649L;
+  public static final org.apache.avro.Schema SCHEMA$ = new org.apache.avro.Schema.Parser().parse("{\"type\":\"record\",\"name\":\"NestedRecord\",\"namespace\":\"org.apache.flink.formats.parquet.generated\",\"fields\":[{\"name\":\"foo\",\"type\":[\"null\",\"long\"],\"default\":null},{\"name\":\"spamMap\",\"type\":[\"null\",{\"type\":\"map\",\"values\":\"string\"}],\"default\":null},{\"name\":\"bar\",\"type\":[\"null\",{\"type\":\"record\",\"name\":\"Bar\",\"fields\":[{\"name\":\"spam\",\"type\":[\"null\",\"long\"],\"default\":null}]}],\"default\":null},{\"name\":\"arr\",\"type\":[\"null\",{\"type\":\"array\",\"items\":\"long\"}],\"default\":null},{\"name\":\"strArray\",\"type\":[\"null\",{\"type\":\"array\",\"items\":\"string\"}],\"default\":null},{\"name\":\"nestedMap\",\"type\":[\"null\",{\"type\":\"map\",\"values\":{\"type\":\"record\",\"name\":\"MapItem\",\"fields\":[{\"name\":\"type\",\"type\":[\"null\",\"string\"]},{\"name\":\"value\",\"type\":[\"null\",\"string\"]}]}}],\"default\":null},{\"name\":\"nestedArray\",\"type\":[\"null\",{\"type\":\"array\",\"items\":{\"type\":\"record\",\"name\":\"ArrayItem\",\"fields\":[{\"name\":\"type\",\"type\":[\"null\",\"string\"]},{\"name\":\"value\",\"type\":[\"null\",\"string\"]}]}}],\"default\":null}],\"schema_id\":1}");
+  public static org.apache.avro.Schema getClassSchema() { return SCHEMA$; }
+
+  private static SpecificData MODEL$ = new SpecificData();
+
+  private static final BinaryMessageEncoder<NestedRecord> ENCODER =
+      new BinaryMessageEncoder<NestedRecord>(MODEL$, SCHEMA$);
+
+  private static final BinaryMessageDecoder<NestedRecord> DECODER =
+      new BinaryMessageDecoder<NestedRecord>(MODEL$, SCHEMA$);
+
+  /**
+   * Return the BinaryMessageDecoder instance used by this class.
+   */
+  public static BinaryMessageDecoder<NestedRecord> getDecoder() {
+    return DECODER;
+  }
+
+  /**
+   * Create a new BinaryMessageDecoder instance for this class that uses the specified {@link SchemaStore}.
+   * @param resolver a {@link SchemaStore} used to find schemas by fingerprint
+   */
+  public static BinaryMessageDecoder<NestedRecord> createDecoder(SchemaStore resolver) {
+    return new BinaryMessageDecoder<NestedRecord>(MODEL$, SCHEMA$, resolver);
+  }
+
+  /** Serializes this NestedRecord to a ByteBuffer. */
+  public java.nio.ByteBuffer toByteBuffer() throws java.io.IOException {
+    return ENCODER.encode(this);
+  }
+
+  /** Deserializes a NestedRecord from a ByteBuffer. */
+  public static NestedRecord fromByteBuffer(
+      java.nio.ByteBuffer b) throws java.io.IOException {
+    return DECODER.decode(b);
+  }
+
+  @Deprecated public java.lang.Long foo;
+  @Deprecated public java.util.Map<java.lang.CharSequence,java.lang.CharSequence> spamMap;
+  @Deprecated public org.apache.flink.formats.parquet.generated.Bar bar;
+  @Deprecated public java.util.List<java.lang.Long> arr;
+  @Deprecated public java.util.List<java.lang.CharSequence> strArray;
+  @Deprecated public java.util.Map<java.lang.CharSequence,org.apache.flink.formats.parquet.generated.MapItem> nestedMap;
+  @Deprecated public java.util.List<org.apache.flink.formats.parquet.generated.ArrayItem> nestedArray;
+
+  /**
+   * Default constructor.  Note that this does not initialize fields
+   * to their default values from the schema.  If that is desired then
+   * one should use <code>newBuilder()</code>.
+   */
+  public NestedRecord() {}
+
+  /**
+   * All-args constructor.
+   * @param foo The new value for foo
+   * @param spamMap The new value for spamMap
+   * @param bar The new value for bar
+   * @param arr The new value for arr
+   * @param strArray The new value for strArray
+   * @param nestedMap The new value for nestedMap
+   * @param nestedArray The new value for nestedArray
+   */
+  public NestedRecord(java.lang.Long foo, java.util.Map<java.lang.CharSequence,java.lang.CharSequence> spamMap, org.apache.flink.formats.parquet.generated.Bar bar, java.util.List<java.lang.Long> arr, java.util.List<java.lang.CharSequence> strArray, java.util.Map<java.lang.CharSequence,org.apache.flink.formats.parquet.generated.MapItem> nestedMap, java.util.List<org.apache.flink.formats.parquet.generated.ArrayItem> nestedArray) {
+    this.foo = foo;
+    this.spamMap = spamMap;
+    this.bar = bar;
+    this.arr = arr;
+    this.strArray = strArray;
+    this.nestedMap = nestedMap;
+    this.nestedArray = nestedArray;
+  }
+
+  public org.apache.avro.Schema getSchema() { return SCHEMA$; }
+  // Used by DatumWriter.  Applications should not call.
+  public java.lang.Object get(int field$) {
+    switch (field$) {
+    case 0: return foo;
+    case 1: return spamMap;
+    case 2: return bar;
+    case 3: return arr;
+    case 4: return strArray;
+    case 5: return nestedMap;
+    case 6: return nestedArray;
+    default: throw new org.apache.avro.AvroRuntimeException("Bad index");
+    }
+  }
+
+  // Used by DatumReader.  Applications should not call.
+  @SuppressWarnings(value="unchecked")
+  public void put(int field$, java.lang.Object value$) {
+    switch (field$) {
+    case 0: foo = (java.lang.Long)value$; break;
+    case 1: spamMap = (java.util.Map<java.lang.CharSequence,java.lang.CharSequence>)value$; break;
+    case 2: bar = (org.apache.flink.formats.parquet.generated.Bar)value$; break;
+    case 3: arr = (java.util.List<java.lang.Long>)value$; break;
+    case 4: strArray = (java.util.List<java.lang.CharSequence>)value$; break;
+    case 5: nestedMap = (java.util.Map<java.lang.CharSequence,org.apache.flink.formats.parquet.generated.MapItem>)value$; break;
+    case 6: nestedArray = (java.util.List<org.apache.flink.formats.parquet.generated.ArrayItem>)value$; break;
+    default: throw new org.apache.avro.AvroRuntimeException("Bad index");
+    }
+  }
+
+  /**
+   * Gets the value of the 'foo' field.
+   * @return The value of the 'foo' field.
+   */
+  public java.lang.Long getFoo() {
+    return foo;
+  }
+
+  /**
+   * Sets the value of the 'foo' field.
+   * @param value the value to set.
+   */
+  public void setFoo(java.lang.Long value) {
+    this.foo = value;
+  }
+
+  /**
+   * Gets the value of the 'spamMap' field.
+   * @return The value of the 'spamMap' field.
+   */
+  public java.util.Map<java.lang.CharSequence,java.lang.CharSequence> getSpamMap() {
+    return spamMap;
+  }
+
+  /**
+   * Sets the value of the 'spamMap' field.
+   * @param value the value to set.
+   */
+  public void setSpamMap(java.util.Map<java.lang.CharSequence,java.lang.CharSequence> value) {
+    this.spamMap = value;
+  }
+
+  /**
+   * Gets the value of the 'bar' field.
+   * @return The value of the 'bar' field.
+   */
+  public org.apache.flink.formats.parquet.generated.Bar getBar() {
+    return bar;
+  }
+
+  /**
+   * Sets the value of the 'bar' field.
+   * @param value the value to set.
+   */
+  public void setBar(org.apache.flink.formats.parquet.generated.Bar value) {
+    this.bar = value;
+  }
+
+  /**
+   * Gets the value of the 'arr' field.
+   * @return The value of the 'arr' field.
+   */
+  public java.util.List<java.lang.Long> getArr() {
+    return arr;
+  }
+
+  /**
+   * Sets the value of the 'arr' field.
+   * @param value the value to set.
+   */
+  public void setArr(java.util.List<java.lang.Long> value) {
+    this.arr = value;
+  }
+
+  /**
+   * Gets the value of the 'strArray' field.
+   * @return The value of the 'strArray' field.
+   */
+  public java.util.List<java.lang.CharSequence> getStrArray() {
+    return strArray;
+  }
+
+  /**
+   * Sets the value of the 'strArray' field.
+   * @param value the value to set.
+   */
+  public void setStrArray(java.util.List<java.lang.CharSequence> value) {
+    this.strArray = value;
+  }
+
+  /**
+   * Gets the value of the 'nestedMap' field.
+   * @return The value of the 'nestedMap' field.
+   */
+  public java.util.Map<java.lang.CharSequence,org.apache.flink.formats.parquet.generated.MapItem> getNestedMap() {
+    return nestedMap;
+  }
+
+  /**
+   * Sets the value of the 'nestedMap' field.
+   * @param value the value to set.
+   */
+  public void setNestedMap(java.util.Map<java.lang.CharSequence,org.apache.flink.formats.parquet.generated.MapItem> value) {
+    this.nestedMap = value;
+  }
+
+  /**
+   * Gets the value of the 'nestedArray' field.
+   * @return The value of the 'nestedArray' field.
+   */
+  public java.util.List<org.apache.flink.formats.parquet.generated.ArrayItem> getNestedArray() {
+    return nestedArray;
+  }
+
+  /**
+   * Sets the value of the 'nestedArray' field.
+   * @param value the value to set.
+   */
+  public void setNestedArray(java.util.List<org.apache.flink.formats.parquet.generated.ArrayItem> value) {
+    this.nestedArray = value;
+  }
+
+  /**
+   * Creates a new NestedRecord RecordBuilder.
+   * @return A new NestedRecord RecordBuilder
+   */
+  public static org.apache.flink.formats.parquet.generated.NestedRecord.Builder newBuilder() {
+    return new org.apache.flink.formats.parquet.generated.NestedRecord.Builder();
+  }
+
+  /**
+   * Creates a new NestedRecord RecordBuilder by copying an existing Builder.
+   * @param other The existing builder to copy.
+   * @return A new NestedRecord RecordBuilder
+   */
+  public static org.apache.flink.formats.parquet.generated.NestedRecord.Builder newBuilder(org.apache.flink.formats.parquet.generated.NestedRecord.Builder other) {
+    return new org.apache.flink.formats.parquet.generated.NestedRecord.Builder(other);
+  }
+
+  /**
+   * Creates a new NestedRecord RecordBuilder by copying an existing NestedRecord instance.
+   * @param other The existing instance to copy.
+   * @return A new NestedRecord RecordBuilder
+   */
+  public static org.apache.flink.formats.parquet.generated.NestedRecord.Builder newBuilder(org.apache.flink.formats.parquet.generated.NestedRecord other) {
+    return new org.apache.flink.formats.parquet.generated.NestedRecord.Builder(other);
+  }
+
+  /**
+   * RecordBuilder for NestedRecord instances.
+   */
+  public static class Builder extends org.apache.avro.specific.SpecificRecordBuilderBase<NestedRecord>
+    implements org.apache.avro.data.RecordBuilder<NestedRecord> {
+
+    private java.lang.Long foo;
+    private java.util.Map<java.lang.CharSequence,java.lang.CharSequence> spamMap;
+    private org.apache.flink.formats.parquet.generated.Bar bar;
+    private org.apache.flink.formats.parquet.generated.Bar.Builder barBuilder;
+    private java.util.List<java.lang.Long> arr;
+    private java.util.List<java.lang.CharSequence> strArray;
+    private java.util.Map<java.lang.CharSequence,org.apache.flink.formats.parquet.generated.MapItem> nestedMap;
+    private java.util.List<org.apache.flink.formats.parquet.generated.ArrayItem> nestedArray;
+
+    /** Creates a new Builder */
+    private Builder() {
+      super(SCHEMA$);
+    }
+
+    /**
+     * Creates a Builder by copying an existing Builder.
+     * @param other The existing Builder to copy.
+     */
+    private Builder(org.apache.flink.formats.parquet.generated.NestedRecord.Builder other) {
+      super(other);
+      if (isValidValue(fields()[0], other.foo)) {
+        this.foo = data().deepCopy(fields()[0].schema(), other.foo);
+        fieldSetFlags()[0] = true;
+      }
+      if (isValidValue(fields()[1], other.spamMap)) {
+        this.spamMap = data().deepCopy(fields()[1].schema(), other.spamMap);
+        fieldSetFlags()[1] = true;
+      }
+      if (isValidValue(fields()[2], other.bar)) {
+        this.bar = data().deepCopy(fields()[2].schema(), other.bar);
+        fieldSetFlags()[2] = true;
+      }
+      if (other.hasBarBuilder()) {
+        this.barBuilder = org.apache.flink.formats.parquet.generated.Bar.newBuilder(other.getBarBuilder());
+      }
+      if (isValidValue(fields()[3], other.arr)) {
+        this.arr = data().deepCopy(fields()[3].schema(), other.arr);
+        fieldSetFlags()[3] = true;
+      }
+      if (isValidValue(fields()[4], other.strArray)) {
+        this.strArray = data().deepCopy(fields()[4].schema(), other.strArray);
+        fieldSetFlags()[4] = true;
+      }
+      if (isValidValue(fields()[5], other.nestedMap)) {
+        this.nestedMap = data().deepCopy(fields()[5].schema(), other.nestedMap);
+        fieldSetFlags()[5] = true;
+      }
+      if (isValidValue(fields()[6], other.nestedArray)) {
+        this.nestedArray = data().deepCopy(fields()[6].schema(), other.nestedArray);
+        fieldSetFlags()[6] = true;
+      }
+    }
+
+    /**
+     * Creates a Builder by copying an existing NestedRecord instance
+     * @param other The existing instance to copy.
+     */
+    private Builder(org.apache.flink.formats.parquet.generated.NestedRecord other) {
+            super(SCHEMA$);
+      if (isValidValue(fields()[0], other.foo)) {
+        this.foo = data().deepCopy(fields()[0].schema(), other.foo);
+        fieldSetFlags()[0] = true;
+      }
+      if (isValidValue(fields()[1], other.spamMap)) {
+        this.spamMap = data().deepCopy(fields()[1].schema(), other.spamMap);
+        fieldSetFlags()[1] = true;
+      }
+      if (isValidValue(fields()[2], other.bar)) {
+        this.bar = data().deepCopy(fields()[2].schema(), other.bar);
+        fieldSetFlags()[2] = true;
+      }
+      this.barBuilder = null;
+      if (isValidValue(fields()[3], other.arr)) {
+        this.arr = data().deepCopy(fields()[3].schema(), other.arr);
+        fieldSetFlags()[3] = true;
+      }
+      if (isValidValue(fields()[4], other.strArray)) {
+        this.strArray = data().deepCopy(fields()[4].schema(), other.strArray);
+        fieldSetFlags()[4] = true;
+      }
+      if (isValidValue(fields()[5], other.nestedMap)) {
+        this.nestedMap = data().deepCopy(fields()[5].schema(), other.nestedMap);
+        fieldSetFlags()[5] = true;
+      }
+      if (isValidValue(fields()[6], other.nestedArray)) {
+        this.nestedArray = data().deepCopy(fields()[6].schema(), other.nestedArray);
+        fieldSetFlags()[6] = true;
+      }
+    }
+
+    /**
+      * Gets the value of the 'foo' field.
+      * @return The value.
+      */
+    public java.lang.Long getFoo() {
+      return foo;
+    }
+
+    /**
+      * Sets the value of the 'foo' field.
+      * @param value The value of 'foo'.
+      * @return This builder.
+      */
+    public org.apache.flink.formats.parquet.generated.NestedRecord.Builder setFoo(java.lang.Long value) {
+      validate(fields()[0], value);
+      this.foo = value;
+      fieldSetFlags()[0] = true;
+      return this;
+    }
+
+    /**
+      * Checks whether the 'foo' field has been set.
+      * @return True if the 'foo' field has been set, false otherwise.
+      */
+    public boolean hasFoo() {
+      return fieldSetFlags()[0];
+    }
+
+
+    /**
+      * Clears the value of the 'foo' field.
+      * @return This builder.
+      */
+    public org.apache.flink.formats.parquet.generated.NestedRecord.Builder clearFoo() {
+      foo = null;
+      fieldSetFlags()[0] = false;
+      return this;
+    }
+
+    /**
+      * Gets the value of the 'spamMap' field.
+      * @return The value.
+      */
+    public java.util.Map<java.lang.CharSequence,java.lang.CharSequence> getSpamMap() {
+      return spamMap;
+    }
+
+    /**
+      * Sets the value of the 'spamMap' field.
+      * @param value The value of 'spamMap'.
+      * @return This builder.
+      */
+    public org.apache.flink.formats.parquet.generated.NestedRecord.Builder setSpamMap(java.util.Map<java.lang.CharSequence,java.lang.CharSequence> value) {
+      validate(fields()[1], value);
+      this.spamMap = value;
+      fieldSetFlags()[1] = true;
+      return this;
+    }
+
+    /**
+      * Checks whether the 'spamMap' field has been set.
+      * @return True if the 'spamMap' field has been set, false otherwise.
+      */
+    public boolean hasSpamMap() {
+      return fieldSetFlags()[1];
+    }
+
+
+    /**
+      * Clears the value of the 'spamMap' field.
+      * @return This builder.
+      */
+    public org.apache.flink.formats.parquet.generated.NestedRecord.Builder clearSpamMap() {
+      spamMap = null;
+      fieldSetFlags()[1] = false;
+      return this;
+    }
+
+    /**
+      * Gets the value of the 'bar' field.
+      * @return The value.
+      */
+    public org.apache.flink.formats.parquet.generated.Bar getBar() {
+      return bar;
+    }
+
+    /**
+      * Sets the value of the 'bar' field.
+      * @param value The value of 'bar'.
+      * @return This builder.
+      */
+    public org.apache.flink.formats.parquet.generated.NestedRecord.Builder setBar(org.apache.flink.formats.parquet.generated.Bar value) {
+      validate(fields()[2], value);
+      this.barBuilder = null;
+      this.bar = value;
+      fieldSetFlags()[2] = true;
+      return this;
+    }
+
+    /**
+      * Checks whether the 'bar' field has been set.
+      * @return True if the 'bar' field has been set, false otherwise.
+      */
+    public boolean hasBar() {
+      return fieldSetFlags()[2];
+    }
+
+    /**
+     * Gets the Builder instance for the 'bar' field and creates one if it doesn't exist yet.
+     * @return This builder.
+     */
+    public org.apache.flink.formats.parquet.generated.Bar.Builder getBarBuilder() {
+      if (barBuilder == null) {
+        if (hasBar()) {
+          setBarBuilder(org.apache.flink.formats.parquet.generated.Bar.newBuilder(bar));
+        } else {
+          setBarBuilder(org.apache.flink.formats.parquet.generated.Bar.newBuilder());
+        }
+      }
+      return barBuilder;
+    }
+
+    /**
+     * Sets the Builder instance for the 'bar' field
+     * @param value The builder instance that must be set.
+     * @return This builder.
+     */
+    public org.apache.flink.formats.parquet.generated.NestedRecord.Builder setBarBuilder(org.apache.flink.formats.parquet.generated.Bar.Builder value) {
+      clearBar();
+      barBuilder = value;
+      return this;
+    }
+
+    /**
+     * Checks whether the 'bar' field has an active Builder instance
+     * @return True if the 'bar' field has an active Builder instance
+     */
+    public boolean hasBarBuilder() {
+      return barBuilder != null;
+    }
+
+    /**
+      * Clears the value of the 'bar' field.
+      * @return This builder.
+      */
+    public org.apache.flink.formats.parquet.generated.NestedRecord.Builder clearBar() {
+      bar = null;
+      barBuilder = null;
+      fieldSetFlags()[2] = false;
+      return this;
+    }
+
+    /**
+      * Gets the value of the 'arr' field.
+      * @return The value.
+      */
+    public java.util.List<java.lang.Long> getArr() {
+      return arr;
+    }
+
+    /**
+      * Sets the value of the 'arr' field.
+      * @param value The value of 'arr'.
+      * @return This builder.
+      */
+    public org.apache.flink.formats.parquet.generated.NestedRecord.Builder setArr(java.util.List<java.lang.Long> value) {
+      validate(fields()[3], value);
+      this.arr = value;
+      fieldSetFlags()[3] = true;
+      return this;
+    }
+
+    /**
+      * Checks whether the 'arr' field has been set.
+      * @return True if the 'arr' field has been set, false otherwise.
+      */
+    public boolean hasArr() {
+      return fieldSetFlags()[3];
+    }
+
+
+    /**
+      * Clears the value of the 'arr' field.
+      * @return This builder.
+      */
+    public org.apache.flink.formats.parquet.generated.NestedRecord.Builder clearArr() {
+      arr = null;
+      fieldSetFlags()[3] = false;
+      return this;
+    }
+
+    /**
+      * Gets the value of the 'strArray' field.
+      * @return The value.
+      */
+    public java.util.List<java.lang.CharSequence> getStrArray() {
+      return strArray;
+    }
+
+    /**
+      * Sets the value of the 'strArray' field.
+      * @param value The value of 'strArray'.
+      * @return This builder.
+      */
+    public org.apache.flink.formats.parquet.generated.NestedRecord.Builder setStrArray(java.util.List<java.lang.CharSequence> value) {
+      validate(fields()[4], value);
+      this.strArray = value;
+      fieldSetFlags()[4] = true;
+      return this;
+    }
+
+    /**
+      * Checks whether the 'strArray' field has been set.
+      * @return True if the 'strArray' field has been set, false otherwise.
+      */
+    public boolean hasStrArray() {
+      return fieldSetFlags()[4];
+    }
+
+
+    /**
+      * Clears the value of the 'strArray' field.
+      * @return This builder.
+      */
+    public org.apache.flink.formats.parquet.generated.NestedRecord.Builder clearStrArray() {
+      strArray = null;
+      fieldSetFlags()[4] = false;
+      return this;
+    }
+
+    /**
+      * Gets the value of the 'nestedMap' field.
+      * @return The value.
+      */
+    public java.util.Map<java.lang.CharSequence,org.apache.flink.formats.parquet.generated.MapItem> getNestedMap() {
+      return nestedMap;
+    }
+
+    /**
+      * Sets the value of the 'nestedMap' field.
+      * @param value The value of 'nestedMap'.
+      * @return This builder.
+      */
+    public org.apache.flink.formats.parquet.generated.NestedRecord.Builder setNestedMap(java.util.Map<java.lang.CharSequence,org.apache.flink.formats.parquet.generated.MapItem> value) {
+      validate(fields()[5], value);
+      this.nestedMap = value;
+      fieldSetFlags()[5] = true;
+      return this;
+    }
+
+    /**
+      * Checks whether the 'nestedMap' field has been set.
+      * @return True if the 'nestedMap' field has been set, false otherwise.
+      */
+    public boolean hasNestedMap() {
+      return fieldSetFlags()[5];
+    }
+
+
+    /**
+      * Clears the value of the 'nestedMap' field.
+      * @return This builder.
+      */
+    public org.apache.flink.formats.parquet.generated.NestedRecord.Builder clearNestedMap() {
+      nestedMap = null;
+      fieldSetFlags()[5] = false;
+      return this;
+    }
+
+    /**
+      * Gets the value of the 'nestedArray' field.
+      * @return The value.
+      */
+    public java.util.List<org.apache.flink.formats.parquet.generated.ArrayItem> getNestedArray() {
+      return nestedArray;
+    }
+
+    /**
+      * Sets the value of the 'nestedArray' field.
+      * @param value The value of 'nestedArray'.
+      * @return This builder.
+      */
+    public org.apache.flink.formats.parquet.generated.NestedRecord.Builder setNestedArray(java.util.List<org.apache.flink.formats.parquet.generated.ArrayItem> value) {
+      validate(fields()[6], value);
+      this.nestedArray = value;
+      fieldSetFlags()[6] = true;
+      return this;
+    }
+
+    /**
+      * Checks whether the 'nestedArray' field has been set.
+      * @return True if the 'nestedArray' field has been set, false otherwise.
+      */
+    public boolean hasNestedArray() {
+      return fieldSetFlags()[6];
+    }
+
+
+    /**
+      * Clears the value of the 'nestedArray' field.
+      * @return This builder.
+      */
+    public org.apache.flink.formats.parquet.generated.NestedRecord.Builder clearNestedArray() {
+      nestedArray = null;
+      fieldSetFlags()[6] = false;
+      return this;
+    }
+
+    @Override
+    @SuppressWarnings("unchecked")
+    public NestedRecord build() {
+      try {
+        NestedRecord record = new NestedRecord();
+        record.foo = fieldSetFlags()[0] ? this.foo : (java.lang.Long) defaultValue(fields()[0]);
+        record.spamMap = fieldSetFlags()[1] ? this.spamMap : (java.util.Map<java.lang.CharSequence,java.lang.CharSequence>) defaultValue(fields()[1]);
+        if (barBuilder != null) {
+          record.bar = this.barBuilder.build();
+        } else {
+          record.bar = fieldSetFlags()[2] ? this.bar : (org.apache.flink.formats.parquet.generated.Bar) defaultValue(fields()[2]);
+        }
+        record.arr = fieldSetFlags()[3] ? this.arr : (java.util.List<java.lang.Long>) defaultValue(fields()[3]);
+        record.strArray = fieldSetFlags()[4] ? this.strArray : (java.util.List<java.lang.CharSequence>) defaultValue(fields()[4]);
+        record.nestedMap = fieldSetFlags()[5] ? this.nestedMap : (java.util.Map<java.lang.CharSequence,org.apache.flink.formats.parquet.generated.MapItem>) defaultValue(fields()[5]);
+        record.nestedArray = fieldSetFlags()[6] ? this.nestedArray : (java.util.List<org.apache.flink.formats.parquet.generated.ArrayItem>) defaultValue(fields()[6]);
+        return record;
+      } catch (java.lang.Exception e) {
+        throw new org.apache.avro.AvroRuntimeException(e);
+      }
+    }
+  }
+
+  @SuppressWarnings("unchecked")
+  private static final org.apache.avro.io.DatumWriter<NestedRecord>
+    WRITER$ = (org.apache.avro.io.DatumWriter<NestedRecord>)MODEL$.createDatumWriter(SCHEMA$);
+
+  @Override public void writeExternal(java.io.ObjectOutput out)
+    throws java.io.IOException {
+    WRITER$.write(this, SpecificData.getEncoder(out));
+  }
+
+  @SuppressWarnings("unchecked")
+  private static final org.apache.avro.io.DatumReader<NestedRecord>
+    READER$ = (org.apache.avro.io.DatumReader<NestedRecord>)MODEL$.createDatumReader(SCHEMA$);
+
+  @Override public void readExternal(java.io.ObjectInput in)
+    throws java.io.IOException {
+    READER$.read(this, SpecificData.getDecoder(in));
+  }
+
+}
diff --git a/flink-formats/flink-parquet/src/test/java/org/apache/flink/formats/parquet/generated/SimpleRecord.java b/flink-formats/flink-parquet/src/test/java/org/apache/flink/formats/parquet/generated/SimpleRecord.java
new file mode 100644
index 000000000000..e0833a57e28c
--- /dev/null
+++ b/flink-formats/flink-parquet/src/test/java/org/apache/flink/formats/parquet/generated/SimpleRecord.java
@@ -0,0 +1,378 @@
+/**
+ * Autogenerated by Avro
+ *
+ * DO NOT EDIT DIRECTLY
+ */
+package org.apache.flink.formats.parquet.generated;
+
+import org.apache.avro.specific.SpecificData;
+import org.apache.avro.message.BinaryMessageEncoder;
+import org.apache.avro.message.BinaryMessageDecoder;
+import org.apache.avro.message.SchemaStore;
+
+@SuppressWarnings("all")
+@org.apache.avro.specific.AvroGenerated
+public class SimpleRecord extends org.apache.avro.specific.SpecificRecordBase implements org.apache.avro.specific.SpecificRecord {
+  private static final long serialVersionUID = -6587201329523714120L;
+  public static final org.apache.avro.Schema SCHEMA$ = new org.apache.avro.Schema.Parser().parse("{\"type\":\"record\",\"name\":\"SimpleRecord\",\"namespace\":\"org.apache.flink.formats.parquet.generated\",\"fields\":[{\"name\":\"foo\",\"type\":[\"null\",\"long\"],\"default\":null},{\"name\":\"bar\",\"type\":[\"null\",\"string\"],\"default\":null},{\"name\":\"arr\",\"type\":[\"null\",{\"type\":\"array\",\"items\":\"long\"}],\"default\":null}],\"schema_id\":1}");
+  public static org.apache.avro.Schema getClassSchema() { return SCHEMA$; }
+
+  private static SpecificData MODEL$ = new SpecificData();
+
+  private static final BinaryMessageEncoder<SimpleRecord> ENCODER =
+      new BinaryMessageEncoder<SimpleRecord>(MODEL$, SCHEMA$);
+
+  private static final BinaryMessageDecoder<SimpleRecord> DECODER =
+      new BinaryMessageDecoder<SimpleRecord>(MODEL$, SCHEMA$);
+
+  /**
+   * Return the BinaryMessageDecoder instance used by this class.
+   */
+  public static BinaryMessageDecoder<SimpleRecord> getDecoder() {
+    return DECODER;
+  }
+
+  /**
+   * Create a new BinaryMessageDecoder instance for this class that uses the specified {@link SchemaStore}.
+   * @param resolver a {@link SchemaStore} used to find schemas by fingerprint
+   */
+  public static BinaryMessageDecoder<SimpleRecord> createDecoder(SchemaStore resolver) {
+    return new BinaryMessageDecoder<SimpleRecord>(MODEL$, SCHEMA$, resolver);
+  }
+
+  /** Serializes this SimpleRecord to a ByteBuffer. */
+  public java.nio.ByteBuffer toByteBuffer() throws java.io.IOException {
+    return ENCODER.encode(this);
+  }
+
+  /** Deserializes a SimpleRecord from a ByteBuffer. */
+  public static SimpleRecord fromByteBuffer(
+      java.nio.ByteBuffer b) throws java.io.IOException {
+    return DECODER.decode(b);
+  }
+
+  @Deprecated public java.lang.Long foo;
+  @Deprecated public java.lang.CharSequence bar;
+  @Deprecated public java.util.List<java.lang.Long> arr;
+
+  /**
+   * Default constructor.  Note that this does not initialize fields
+   * to their default values from the schema.  If that is desired then
+   * one should use <code>newBuilder()</code>.
+   */
+  public SimpleRecord() {}
+
+  /**
+   * All-args constructor.
+   * @param foo The new value for foo
+   * @param bar The new value for bar
+   * @param arr The new value for arr
+   */
+  public SimpleRecord(java.lang.Long foo, java.lang.CharSequence bar, java.util.List<java.lang.Long> arr) {
+    this.foo = foo;
+    this.bar = bar;
+    this.arr = arr;
+  }
+
+  public org.apache.avro.Schema getSchema() { return SCHEMA$; }
+  // Used by DatumWriter.  Applications should not call.
+  public java.lang.Object get(int field$) {
+    switch (field$) {
+    case 0: return foo;
+    case 1: return bar;
+    case 2: return arr;
+    default: throw new org.apache.avro.AvroRuntimeException("Bad index");
+    }
+  }
+
+  // Used by DatumReader.  Applications should not call.
+  @SuppressWarnings(value="unchecked")
+  public void put(int field$, java.lang.Object value$) {
+    switch (field$) {
+    case 0: foo = (java.lang.Long)value$; break;
+    case 1: bar = (java.lang.CharSequence)value$; break;
+    case 2: arr = (java.util.List<java.lang.Long>)value$; break;
+    default: throw new org.apache.avro.AvroRuntimeException("Bad index");
+    }
+  }
+
+  /**
+   * Gets the value of the 'foo' field.
+   * @return The value of the 'foo' field.
+   */
+  public java.lang.Long getFoo() {
+    return foo;
+  }
+
+  /**
+   * Sets the value of the 'foo' field.
+   * @param value the value to set.
+   */
+  public void setFoo(java.lang.Long value) {
+    this.foo = value;
+  }
+
+  /**
+   * Gets the value of the 'bar' field.
+   * @return The value of the 'bar' field.
+   */
+  public java.lang.CharSequence getBar() {
+    return bar;
+  }
+
+  /**
+   * Sets the value of the 'bar' field.
+   * @param value the value to set.
+   */
+  public void setBar(java.lang.CharSequence value) {
+    this.bar = value;
+  }
+
+  /**
+   * Gets the value of the 'arr' field.
+   * @return The value of the 'arr' field.
+   */
+  public java.util.List<java.lang.Long> getArr() {
+    return arr;
+  }
+
+  /**
+   * Sets the value of the 'arr' field.
+   * @param value the value to set.
+   */
+  public void setArr(java.util.List<java.lang.Long> value) {
+    this.arr = value;
+  }
+
+  /**
+   * Creates a new SimpleRecord RecordBuilder.
+   * @return A new SimpleRecord RecordBuilder
+   */
+  public static org.apache.flink.formats.parquet.generated.SimpleRecord.Builder newBuilder() {
+    return new org.apache.flink.formats.parquet.generated.SimpleRecord.Builder();
+  }
+
+  /**
+   * Creates a new SimpleRecord RecordBuilder by copying an existing Builder.
+   * @param other The existing builder to copy.
+   * @return A new SimpleRecord RecordBuilder
+   */
+  public static org.apache.flink.formats.parquet.generated.SimpleRecord.Builder newBuilder(org.apache.flink.formats.parquet.generated.SimpleRecord.Builder other) {
+    return new org.apache.flink.formats.parquet.generated.SimpleRecord.Builder(other);
+  }
+
+  /**
+   * Creates a new SimpleRecord RecordBuilder by copying an existing SimpleRecord instance.
+   * @param other The existing instance to copy.
+   * @return A new SimpleRecord RecordBuilder
+   */
+  public static org.apache.flink.formats.parquet.generated.SimpleRecord.Builder newBuilder(org.apache.flink.formats.parquet.generated.SimpleRecord other) {
+    return new org.apache.flink.formats.parquet.generated.SimpleRecord.Builder(other);
+  }
+
+  /**
+   * RecordBuilder for SimpleRecord instances.
+   */
+  public static class Builder extends org.apache.avro.specific.SpecificRecordBuilderBase<SimpleRecord>
+    implements org.apache.avro.data.RecordBuilder<SimpleRecord> {
+
+    private java.lang.Long foo;
+    private java.lang.CharSequence bar;
+    private java.util.List<java.lang.Long> arr;
+
+    /** Creates a new Builder */
+    private Builder() {
+      super(SCHEMA$);
+    }
+
+    /**
+     * Creates a Builder by copying an existing Builder.
+     * @param other The existing Builder to copy.
+     */
+    private Builder(org.apache.flink.formats.parquet.generated.SimpleRecord.Builder other) {
+      super(other);
+      if (isValidValue(fields()[0], other.foo)) {
+        this.foo = data().deepCopy(fields()[0].schema(), other.foo);
+        fieldSetFlags()[0] = true;
+      }
+      if (isValidValue(fields()[1], other.bar)) {
+        this.bar = data().deepCopy(fields()[1].schema(), other.bar);
+        fieldSetFlags()[1] = true;
+      }
+      if (isValidValue(fields()[2], other.arr)) {
+        this.arr = data().deepCopy(fields()[2].schema(), other.arr);
+        fieldSetFlags()[2] = true;
+      }
+    }
+
+    /**
+     * Creates a Builder by copying an existing SimpleRecord instance
+     * @param other The existing instance to copy.
+     */
+    private Builder(org.apache.flink.formats.parquet.generated.SimpleRecord other) {
+            super(SCHEMA$);
+      if (isValidValue(fields()[0], other.foo)) {
+        this.foo = data().deepCopy(fields()[0].schema(), other.foo);
+        fieldSetFlags()[0] = true;
+      }
+      if (isValidValue(fields()[1], other.bar)) {
+        this.bar = data().deepCopy(fields()[1].schema(), other.bar);
+        fieldSetFlags()[1] = true;
+      }
+      if (isValidValue(fields()[2], other.arr)) {
+        this.arr = data().deepCopy(fields()[2].schema(), other.arr);
+        fieldSetFlags()[2] = true;
+      }
+    }
+
+    /**
+      * Gets the value of the 'foo' field.
+      * @return The value.
+      */
+    public java.lang.Long getFoo() {
+      return foo;
+    }
+
+    /**
+      * Sets the value of the 'foo' field.
+      * @param value The value of 'foo'.
+      * @return This builder.
+      */
+    public org.apache.flink.formats.parquet.generated.SimpleRecord.Builder setFoo(java.lang.Long value) {
+      validate(fields()[0], value);
+      this.foo = value;
+      fieldSetFlags()[0] = true;
+      return this;
+    }
+
+    /**
+      * Checks whether the 'foo' field has been set.
+      * @return True if the 'foo' field has been set, false otherwise.
+      */
+    public boolean hasFoo() {
+      return fieldSetFlags()[0];
+    }
+
+
+    /**
+      * Clears the value of the 'foo' field.
+      * @return This builder.
+      */
+    public org.apache.flink.formats.parquet.generated.SimpleRecord.Builder clearFoo() {
+      foo = null;
+      fieldSetFlags()[0] = false;
+      return this;
+    }
+
+    /**
+      * Gets the value of the 'bar' field.
+      * @return The value.
+      */
+    public java.lang.CharSequence getBar() {
+      return bar;
+    }
+
+    /**
+      * Sets the value of the 'bar' field.
+      * @param value The value of 'bar'.
+      * @return This builder.
+      */
+    public org.apache.flink.formats.parquet.generated.SimpleRecord.Builder setBar(java.lang.CharSequence value) {
+      validate(fields()[1], value);
+      this.bar = value;
+      fieldSetFlags()[1] = true;
+      return this;
+    }
+
+    /**
+      * Checks whether the 'bar' field has been set.
+      * @return True if the 'bar' field has been set, false otherwise.
+      */
+    public boolean hasBar() {
+      return fieldSetFlags()[1];
+    }
+
+
+    /**
+      * Clears the value of the 'bar' field.
+      * @return This builder.
+      */
+    public org.apache.flink.formats.parquet.generated.SimpleRecord.Builder clearBar() {
+      bar = null;
+      fieldSetFlags()[1] = false;
+      return this;
+    }
+
+    /**
+      * Gets the value of the 'arr' field.
+      * @return The value.
+      */
+    public java.util.List<java.lang.Long> getArr() {
+      return arr;
+    }
+
+    /**
+      * Sets the value of the 'arr' field.
+      * @param value The value of 'arr'.
+      * @return This builder.
+      */
+    public org.apache.flink.formats.parquet.generated.SimpleRecord.Builder setArr(java.util.List<java.lang.Long> value) {
+      validate(fields()[2], value);
+      this.arr = value;
+      fieldSetFlags()[2] = true;
+      return this;
+    }
+
+    /**
+      * Checks whether the 'arr' field has been set.
+      * @return True if the 'arr' field has been set, false otherwise.
+      */
+    public boolean hasArr() {
+      return fieldSetFlags()[2];
+    }
+
+
+    /**
+      * Clears the value of the 'arr' field.
+      * @return This builder.
+      */
+    public org.apache.flink.formats.parquet.generated.SimpleRecord.Builder clearArr() {
+      arr = null;
+      fieldSetFlags()[2] = false;
+      return this;
+    }
+
+    @Override
+    @SuppressWarnings("unchecked")
+    public SimpleRecord build() {
+      try {
+        SimpleRecord record = new SimpleRecord();
+        record.foo = fieldSetFlags()[0] ? this.foo : (java.lang.Long) defaultValue(fields()[0]);
+        record.bar = fieldSetFlags()[1] ? this.bar : (java.lang.CharSequence) defaultValue(fields()[1]);
+        record.arr = fieldSetFlags()[2] ? this.arr : (java.util.List<java.lang.Long>) defaultValue(fields()[2]);
+        return record;
+      } catch (java.lang.Exception e) {
+        throw new org.apache.avro.AvroRuntimeException(e);
+      }
+    }
+  }
+
+  @SuppressWarnings("unchecked")
+  private static final org.apache.avro.io.DatumWriter<SimpleRecord>
+    WRITER$ = (org.apache.avro.io.DatumWriter<SimpleRecord>)MODEL$.createDatumWriter(SCHEMA$);
+
+  @Override public void writeExternal(java.io.ObjectOutput out)
+    throws java.io.IOException {
+    WRITER$.write(this, SpecificData.getEncoder(out));
+  }
+
+  @SuppressWarnings("unchecked")
+  private static final org.apache.avro.io.DatumReader<SimpleRecord>
+    READER$ = (org.apache.avro.io.DatumReader<SimpleRecord>)MODEL$.createDatumReader(SCHEMA$);
+
+  @Override public void readExternal(java.io.ObjectInput in)
+    throws java.io.IOException {
+    READER$.read(this, SpecificData.getDecoder(in));
+  }
+
+}

From 8fcd45fde31227133dedb1b21c4fb06dafad1900 Mon Sep 17 00:00:00 2001
From: Peter Huang <huangzhenqiu0825@gmail.com>
Date: Sat, 12 Jan 2019 21:47:23 -0800
Subject: [PATCH 2/8] rebase master

---
 .../runtime/jobmaster/slotpool/SlotPool.java  |  9 +++
 .../resourcemanager/ResourceManager.java      |  9 +++
 .../MaximumFailedContainersException.java     | 35 ++++++++++
 .../slotmanager/SlotManager.java              | 23 +++++--
 .../flink/yarn/YarnResourceManager.java       | 24 ++++++-
 .../flink/yarn/YarnResourceManagerTest.java   | 64 ++++++++++++++++++-
 6 files changed, 154 insertions(+), 10 deletions(-)
 create mode 100644 flink-runtime/src/main/java/org/apache/flink/runtime/resourcemanager/exceptions/MaximumFailedContainersException.java

diff --git a/flink-runtime/src/main/java/org/apache/flink/runtime/jobmaster/slotpool/SlotPool.java b/flink-runtime/src/main/java/org/apache/flink/runtime/jobmaster/slotpool/SlotPool.java
index b3fb36cabc49..4839de36d0ac 100644
--- a/flink-runtime/src/main/java/org/apache/flink/runtime/jobmaster/slotpool/SlotPool.java
+++ b/flink-runtime/src/main/java/org/apache/flink/runtime/jobmaster/slotpool/SlotPool.java
@@ -44,6 +44,7 @@
 import org.apache.flink.runtime.messages.Acknowledge;
 import org.apache.flink.runtime.resourcemanager.ResourceManagerGateway;
 import org.apache.flink.runtime.resourcemanager.SlotRequest;
+import org.apache.flink.runtime.resourcemanager.exceptions.MaximumFailedContainersException;
 import org.apache.flink.runtime.rpc.RpcEndpoint;
 import org.apache.flink.runtime.rpc.RpcService;
 import org.apache.flink.runtime.taskexecutor.slot.SlotOffer;
@@ -688,6 +689,8 @@ public void disconnectResourceManager() {
 				(AllocatedSlot ignored, Throwable throwable) -> {
 					if (throwable instanceof TimeoutException) {
 						timeoutPendingSlotRequest(slotRequestId);
+					} else if (throwable instanceof MaximumFailedContainersException) {
+						rejectPendingSlotRequest(slotRequestId);
 					}
 				},
 				getMainThreadExecutor());
@@ -1123,6 +1126,12 @@ protected void timeoutPendingSlotRequest(SlotRequestId slotRequestId) {
 		removePendingRequest(slotRequestId);
 	}
 
+	@VisibleForTesting
+	protected void rejectPendingSlotRequest(SlotRequestId slotRequestId) {
+		log.info("Pending slot request [{}] is rejected by resource manager.", slotRequestId);
+		removePendingRequest(slotRequestId);
+	}
+
 	private void releaseTaskManagerInternal(final ResourceID resourceId, final Exception cause) {
 		final Set<AllocatedSlot> removedSlots = new HashSet<>(allocatedSlots.removeSlotsForTaskManager(resourceId));
 
diff --git a/flink-runtime/src/main/java/org/apache/flink/runtime/resourcemanager/ResourceManager.java b/flink-runtime/src/main/java/org/apache/flink/runtime/resourcemanager/ResourceManager.java
index f90f63a2d916..58a4c785d4d8 100644
--- a/flink-runtime/src/main/java/org/apache/flink/runtime/resourcemanager/ResourceManager.java
+++ b/flink-runtime/src/main/java/org/apache/flink/runtime/resourcemanager/ResourceManager.java
@@ -82,6 +82,7 @@
 import java.util.concurrent.ConcurrentHashMap;
 import java.util.concurrent.ConcurrentMap;
 import java.util.concurrent.TimeoutException;
+import java.util.concurrent.atomic.AtomicInteger;
 import java.util.stream.Collectors;
 
 import static org.apache.flink.util.Preconditions.checkNotNull;
@@ -145,6 +146,9 @@
 	/** All registered listeners for status updates of the ResourceManager. */
 	private ConcurrentMap<String, InfoMessageListenerRpcGateway> infoMessageListeners;
 
+	/** The number of failed containers since the master became active. */
+	protected AtomicInteger failedContainerSoFar = new AtomicInteger(0);
+
 	/**
 	 * Represents asynchronous state clearing work.
 	 *
@@ -626,6 +630,10 @@ public void unRegisterInfoMessageListener(final String address) {
 		}
 	}
 
+	protected void rejectAllPendingSlotRequests(Exception e) {
+		slotManager.rejectAllPendingSlotRequests(e);
+	}
+
 	// ------------------------------------------------------------------------
 	//  Internal methods
 	// ------------------------------------------------------------------------
@@ -825,6 +833,7 @@ protected void closeTaskManagerConnection(final ResourceID resourceID, final Exc
 			slotManager.unregisterTaskManager(workerRegistration.getInstanceID());
 
 			workerRegistration.getTaskExecutorGateway().disconnectResourceManager(cause);
+			failedContainerSoFar.getAndAdd(1);
 		} else {
 			log.debug(
 				"No open TaskExecutor connection {}. Ignoring close TaskExecutor connection. Closing reason was: {}",
diff --git a/flink-runtime/src/main/java/org/apache/flink/runtime/resourcemanager/exceptions/MaximumFailedContainersException.java b/flink-runtime/src/main/java/org/apache/flink/runtime/resourcemanager/exceptions/MaximumFailedContainersException.java
new file mode 100644
index 000000000000..12b65fbf54f4
--- /dev/null
+++ b/flink-runtime/src/main/java/org/apache/flink/runtime/resourcemanager/exceptions/MaximumFailedContainersException.java
@@ -0,0 +1,35 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.flink.runtime.resourcemanager.exceptions;
+
+import org.apache.flink.runtime.resourcemanager.ResourceManager;
+
+/**
+ * Exception for {@link ResourceManager} when it identified that the maximum number of failed containers is hit.
+ */
+public class MaximumFailedContainersException extends ResourceManagerException {
+	private static final long serialVersionUID = -2333228226519195160L;
+
+
+	public MaximumFailedContainersException(String message) { super(message); }
+
+	public MaximumFailedContainersException(String message, Throwable cause) { super(message, cause); }
+
+	public MaximumFailedContainersException(Throwable cause) { super(cause); }
+}
diff --git a/flink-runtime/src/main/java/org/apache/flink/runtime/resourcemanager/slotmanager/SlotManager.java b/flink-runtime/src/main/java/org/apache/flink/runtime/resourcemanager/slotmanager/SlotManager.java
index 1dd16934280a..af27a762c2f4 100644
--- a/flink-runtime/src/main/java/org/apache/flink/runtime/resourcemanager/slotmanager/SlotManager.java
+++ b/flink-runtime/src/main/java/org/apache/flink/runtime/resourcemanager/slotmanager/SlotManager.java
@@ -301,12 +301,25 @@ public boolean registerSlotRequest(SlotRequest slotRequest) throws SlotManagerEx
 	}
 
 	/**
-	 * Cancels and removes a pending slot request with the given allocation id. If there is no such
-	 * pending request, then nothing is done.
-	 *
-	 * @param allocationId identifying the pending slot request
-	 * @return True if a pending slot request was found; otherwise false
+	 * Rejects all pending slot requests.
+	 * @param cause the exception caused the rejection
 	 */
+	public void rejectAllPendingSlotRequests(Exception cause) {
+		for (PendingSlotRequest pendingSlotRequest : pendingSlotRequests.values()) {
+			rejectPendingSlotRequest(pendingSlotRequest, cause);
+		}
+
+		pendingSlotRequests.clear();
+	}
+
+
+		/**
+         * Cancels and removes a pending slot request with the given allocation id. If there is no such
+         * pending request, then nothing is done.
+         *
+         * @param allocationId identifying the pending slot request
+         * @return True if a pending slot request was found; otherwise false
+         */
 	public boolean unregisterSlotRequest(AllocationID allocationId) {
 		checkInit();
 
diff --git a/flink-yarn/src/main/java/org/apache/flink/yarn/YarnResourceManager.java b/flink-yarn/src/main/java/org/apache/flink/yarn/YarnResourceManager.java
index 5a85daaf9c3b..9de281a0b2b8 100644
--- a/flink-yarn/src/main/java/org/apache/flink/yarn/YarnResourceManager.java
+++ b/flink-yarn/src/main/java/org/apache/flink/yarn/YarnResourceManager.java
@@ -36,6 +36,7 @@
 import org.apache.flink.runtime.metrics.groups.JobManagerMetricGroup;
 import org.apache.flink.runtime.resourcemanager.JobLeaderIdService;
 import org.apache.flink.runtime.resourcemanager.ResourceManager;
+import org.apache.flink.runtime.resourcemanager.exceptions.MaximumFailedContainersException;
 import org.apache.flink.runtime.resourcemanager.exceptions.ResourceManagerException;
 import org.apache.flink.runtime.resourcemanager.slotmanager.SlotManager;
 import org.apache.flink.runtime.rpc.FatalErrorHandler;
@@ -94,6 +95,9 @@
 	 * Container ID generation may vary across Hadoop versions. */
 	static final String ENV_FLINK_CONTAINER_ID = "_FLINK_CONTAINER_ID";
 
+	/** The default initial number of task manager. **/
+	private static final String DEFAULT_INITIAL_NUM_TASK_MANAGER = "2";
+
 	/** Environment variable name of the hostname given by the YARN.
 	 * In task executor we use the hostnames given by YARN consistently throughout akka */
 	static final String ENV_FLINK_NODE_ID = "_FLINK_NODE_ID";
@@ -114,6 +118,9 @@
 
 	private final int defaultCpus;
 
+	/** Number of failed TaskManager containers before stopping the application. -1 means infinite. */
+	private final int maxFailedContainers;
+
 	/** Client to communicate with the Resource Manager (YARN's master). */
 	private AMRMClientAsync<AMRMClient.ContainerRequest> resourceManagerClient;
 
@@ -172,6 +179,11 @@ public YarnResourceManager(
 					"than YARN's expiry interval ({}). The application is likely to be killed by YARN.",
 					yarnHeartbeatIntervalMS, yarnExpiryIntervalMS);
 		}
+
+		final int numInitialTM = Integer.parseInt(env.getOrDefault(
+			YarnConfigKeys.ENV_TM_COUNT, DEFAULT_INITIAL_NUM_TASK_MANAGER));
+		this.maxFailedContainers = flinkConfig.getInteger(YarnConfigOptions.MAX_FAILED_CONTAINERS.key(), numInitialTM);
+
 		yarnHeartbeatIntervalMillis = yarnHeartbeatIntervalMS;
 		numPendingContainerRequests = 0;
 
@@ -396,11 +408,19 @@ public void onContainersAllocated(List<Container> containers) {
 					} catch (Throwable t) {
 						log.error("Could not start TaskManager in container {}.", container.getId(), t);
 
+						failedContainerSoFar.getAndAdd(1);
 						// release the failed container
 						workerNodeMap.remove(resourceId);
 						resourceManagerClient.releaseAssignedContainer(container.getId());
-						// and ask for a new one
-						requestYarnContainerIfRequired();
+						if (failedContainerSoFar.intValue() < maxFailedContainers) {
+							// and ask for a new one
+							requestYarnContainerIfRequired();
+						} else {
+							log.error("Could not start TaskManager in container {}.", container.getId(), t);
+							rejectAllPendingSlotRequests(new MaximumFailedContainersException(
+								String.format("Maximum number of failed container %d "
+										+ "is detected in Resource Manager", failedContainerSoFar.intValue())));
+						}
 					}
 				} else {
 					// return the excessive containers
diff --git a/flink-yarn/src/test/java/org/apache/flink/yarn/YarnResourceManagerTest.java b/flink-yarn/src/test/java/org/apache/flink/yarn/YarnResourceManagerTest.java
index 368e95cc7603..4997699e5362 100644
--- a/flink-yarn/src/test/java/org/apache/flink/yarn/YarnResourceManagerTest.java
+++ b/flink-yarn/src/test/java/org/apache/flink/yarn/YarnResourceManagerTest.java
@@ -45,6 +45,7 @@
 import org.apache.flink.runtime.resourcemanager.JobLeaderIdService;
 import org.apache.flink.runtime.resourcemanager.ResourceManagerGateway;
 import org.apache.flink.runtime.resourcemanager.SlotRequest;
+import org.apache.flink.runtime.resourcemanager.exceptions.MaximumFailedContainersException;
 import org.apache.flink.runtime.resourcemanager.slotmanager.SlotManager;
 import org.apache.flink.runtime.rpc.FatalErrorHandler;
 import org.apache.flink.runtime.rpc.RpcService;
@@ -74,6 +75,7 @@
 import org.apache.hadoop.yarn.client.api.NMClient;
 import org.apache.hadoop.yarn.client.api.async.AMRMClientAsync;
 import org.apache.hadoop.yarn.conf.YarnConfiguration;
+import org.apache.hadoop.yarn.exceptions.YarnException;
 import org.hamcrest.Matchers;
 import org.junit.After;
 import org.junit.Before;
@@ -92,12 +94,14 @@
 import java.util.concurrent.Callable;
 import java.util.concurrent.CompletableFuture;
 import java.util.concurrent.TimeUnit;
+import java.util.concurrent.TimeoutException;
 
 import static org.apache.flink.yarn.YarnConfigKeys.ENV_APP_ID;
 import static org.apache.flink.yarn.YarnConfigKeys.ENV_CLIENT_HOME_DIR;
 import static org.apache.flink.yarn.YarnConfigKeys.ENV_CLIENT_SHIP_FILES;
 import static org.apache.flink.yarn.YarnConfigKeys.ENV_FLINK_CLASSPATH;
 import static org.apache.flink.yarn.YarnConfigKeys.ENV_HADOOP_USER_NAME;
+import static org.apache.flink.yarn.YarnConfigKeys.ENV_TM_COUNT;
 import static org.apache.flink.yarn.YarnConfigKeys.FLINK_JAR_PATH;
 import static org.apache.flink.yarn.YarnConfigKeys.FLINK_YARN_FILES;
 import static org.hamcrest.Matchers.instanceOf;
@@ -110,6 +114,7 @@
 import static org.mockito.Matchers.eq;
 import static org.mockito.Mockito.doReturn;
 import static org.mockito.Mockito.mock;
+import static org.mockito.Mockito.spy;
 import static org.mockito.Mockito.times;
 import static org.mockito.Mockito.verify;
 import static org.mockito.Mockito.when;
@@ -149,6 +154,7 @@ public void setup() {
 		env.put(ENV_FLINK_CLASSPATH, "");
 		env.put(ENV_HADOOP_USER_NAME, "foo");
 		env.put(FLINK_JAR_PATH, root.toURI().toString());
+		env.put(ENV_TM_COUNT, "1");
 	}
 
 	@After
@@ -227,7 +233,6 @@ protected NMClient createAndStartNodeManagerClient(YarnConfiguration yarnConfigu
 		protected void runAsync(final Runnable runnable) {
 			runnable.run();
 		}
-
 	}
 
 	class Context {
@@ -308,9 +313,9 @@ protected void runAsync(final Runnable runnable) {
 				highAvailabilityServices.setResourceManagerLeaderElectionService(rmLeaderElectionService);
 				heartbeatServices = new TestingHeartbeatServices(5L, 5L, scheduledExecutor);
 				metricRegistry = NoOpMetricRegistry.INSTANCE;
-				slotManager = new SlotManager(
+				slotManager = spy(new SlotManager(
 						new ScheduledExecutorServiceAdapter(new DirectScheduledExecutorService()),
-						Time.seconds(10), Time.seconds(10), Time.minutes(1));
+						Time.seconds(10), Time.seconds(10), Time.minutes(1)));
 				jobLeaderIdService = new JobLeaderIdService(
 						highAvailabilityServices,
 						rpcService.getScheduledExecutor(),
@@ -521,4 +526,57 @@ public void testOnContainerCompleted() throws Exception {
 			});
 		}};
 	}
+
+	/**
+	 * 	Tests that YarnResourceManager will trigger to cancel all pending slot request, when maximum number of failed
+	 * 	contains is hit.
+	 */
+	@Test
+	public void testOnContainersAllocatedWithFailure() throws Exception {
+		new Context() {{
+			runTest(() -> {
+				CompletableFuture<?> registerSlotRequestFuture = resourceManager.runInMainThread(() -> {
+					rmServices.slotManager.registerSlotRequest(
+						new SlotRequest(new JobID(), new AllocationID(), resourceProfile1, taskHost));
+					return null;
+				});
+
+				// wait for the registerSlotRequest completion
+				registerSlotRequestFuture.get();
+
+				// Callback from YARN when container is allocated.
+				Container disconnectedContainer1 = mockContainer("container1", 1234, 1, resourceManager.getContainerResource());
+
+				doReturn(Collections.singletonList(Collections.singletonList(resourceManager.getContainerRequest())))
+					.when(mockResourceManagerClient).getMatchingRequests(any(Priority.class), anyString(), any(Resource.class));
+
+				resourceManager.onContainersAllocated(ImmutableList.of(disconnectedContainer1));
+				verify(mockResourceManagerClient).addContainerRequest(any(AMRMClient.ContainerRequest.class));
+				verify(mockNMClient).startContainer(eq(disconnectedContainer1), any(ContainerLaunchContext.class));
+
+				ResourceID connectedTM = new ResourceID(disconnectedContainer1.getId().toString());
+
+				resourceManager.registerTaskExecutor("container1", connectedTM, 1234,
+					hardwareDescription, Time.seconds(10L));
+
+				// force to unregister the task manager
+				resourceManager.disconnectTaskManager(connectedTM, new TimeoutException());
+
+				// request second slot
+				registerSlotRequestFuture = resourceManager.runInMainThread(() -> {
+					rmServices.slotManager.registerSlotRequest(
+						new SlotRequest(new JobID(), new AllocationID(), resourceProfile1, taskHost));
+					return null;
+				});
+
+				// wait for the registerSlotRequest completion
+				registerSlotRequestFuture.get();
+				Container failedContainer = mockContainer("container2", 2345, 2, resourceManager.getContainerResource());
+				when(mockNMClient.startContainer(eq(failedContainer), any())).thenThrow(new YarnException("Failed"));
+				resourceManager.onContainersAllocated(ImmutableList.of(failedContainer));
+				verify(rmServices.slotManager, times(1))
+					.rejectAllPendingSlotRequests(any(MaximumFailedContainersException.class));
+			});
+		}};
+	}
 }

From 788f82138f0386f5a2055a0ac0257f0c1f8e4add Mon Sep 17 00:00:00 2001
From: Peter Huang <huangzhenqiu0825@gmail.com>
Date: Mon, 24 Dec 2018 20:58:47 -0800
Subject: [PATCH 3/8] remove generated parquet classes

---
 .../formats/parquet/generated/ArrayItem.java  | 308 --------
 .../flink/formats/parquet/generated/Bar.java  | 238 ------
 .../formats/parquet/generated/MapItem.java    | 308 --------
 .../parquet/generated/NestedRecord.java       | 702 ------------------
 .../parquet/generated/SimpleRecord.java       | 378 ----------
 .../runtime/jobmaster/slotpool/SlotPool.java  |   9 -
 6 files changed, 1943 deletions(-)
 delete mode 100644 flink-formats/flink-parquet/src/test/java/org/apache/flink/formats/parquet/generated/ArrayItem.java
 delete mode 100644 flink-formats/flink-parquet/src/test/java/org/apache/flink/formats/parquet/generated/Bar.java
 delete mode 100644 flink-formats/flink-parquet/src/test/java/org/apache/flink/formats/parquet/generated/MapItem.java
 delete mode 100644 flink-formats/flink-parquet/src/test/java/org/apache/flink/formats/parquet/generated/NestedRecord.java
 delete mode 100644 flink-formats/flink-parquet/src/test/java/org/apache/flink/formats/parquet/generated/SimpleRecord.java

diff --git a/flink-formats/flink-parquet/src/test/java/org/apache/flink/formats/parquet/generated/ArrayItem.java b/flink-formats/flink-parquet/src/test/java/org/apache/flink/formats/parquet/generated/ArrayItem.java
deleted file mode 100644
index ad9cc589fda7..000000000000
--- a/flink-formats/flink-parquet/src/test/java/org/apache/flink/formats/parquet/generated/ArrayItem.java
+++ /dev/null
@@ -1,308 +0,0 @@
-/**
- * Autogenerated by Avro
- *
- * DO NOT EDIT DIRECTLY
- */
-package org.apache.flink.formats.parquet.generated;
-
-import org.apache.avro.specific.SpecificData;
-import org.apache.avro.message.BinaryMessageEncoder;
-import org.apache.avro.message.BinaryMessageDecoder;
-import org.apache.avro.message.SchemaStore;
-
-@SuppressWarnings("all")
-@org.apache.avro.specific.AvroGenerated
-public class ArrayItem extends org.apache.avro.specific.SpecificRecordBase implements org.apache.avro.specific.SpecificRecord {
-  private static final long serialVersionUID = 3888914259224596748L;
-  public static final org.apache.avro.Schema SCHEMA$ = new org.apache.avro.Schema.Parser().parse("{\"type\":\"record\",\"name\":\"ArrayItem\",\"namespace\":\"org.apache.flink.formats.parquet.generated\",\"fields\":[{\"name\":\"type\",\"type\":[\"null\",\"string\"]},{\"name\":\"value\",\"type\":[\"null\",\"string\"]}]}");
-  public static org.apache.avro.Schema getClassSchema() { return SCHEMA$; }
-
-  private static SpecificData MODEL$ = new SpecificData();
-
-  private static final BinaryMessageEncoder<ArrayItem> ENCODER =
-      new BinaryMessageEncoder<ArrayItem>(MODEL$, SCHEMA$);
-
-  private static final BinaryMessageDecoder<ArrayItem> DECODER =
-      new BinaryMessageDecoder<ArrayItem>(MODEL$, SCHEMA$);
-
-  /**
-   * Return the BinaryMessageDecoder instance used by this class.
-   */
-  public static BinaryMessageDecoder<ArrayItem> getDecoder() {
-    return DECODER;
-  }
-
-  /**
-   * Create a new BinaryMessageDecoder instance for this class that uses the specified {@link SchemaStore}.
-   * @param resolver a {@link SchemaStore} used to find schemas by fingerprint
-   */
-  public static BinaryMessageDecoder<ArrayItem> createDecoder(SchemaStore resolver) {
-    return new BinaryMessageDecoder<ArrayItem>(MODEL$, SCHEMA$, resolver);
-  }
-
-  /** Serializes this ArrayItem to a ByteBuffer. */
-  public java.nio.ByteBuffer toByteBuffer() throws java.io.IOException {
-    return ENCODER.encode(this);
-  }
-
-  /** Deserializes a ArrayItem from a ByteBuffer. */
-  public static ArrayItem fromByteBuffer(
-      java.nio.ByteBuffer b) throws java.io.IOException {
-    return DECODER.decode(b);
-  }
-
-  @Deprecated public java.lang.CharSequence type;
-  @Deprecated public java.lang.CharSequence value;
-
-  /**
-   * Default constructor.  Note that this does not initialize fields
-   * to their default values from the schema.  If that is desired then
-   * one should use <code>newBuilder()</code>.
-   */
-  public ArrayItem() {}
-
-  /**
-   * All-args constructor.
-   * @param type The new value for type
-   * @param value The new value for value
-   */
-  public ArrayItem(java.lang.CharSequence type, java.lang.CharSequence value) {
-    this.type = type;
-    this.value = value;
-  }
-
-  public org.apache.avro.Schema getSchema() { return SCHEMA$; }
-  // Used by DatumWriter.  Applications should not call.
-  public java.lang.Object get(int field$) {
-    switch (field$) {
-    case 0: return type;
-    case 1: return value;
-    default: throw new org.apache.avro.AvroRuntimeException("Bad index");
-    }
-  }
-
-  // Used by DatumReader.  Applications should not call.
-  @SuppressWarnings(value="unchecked")
-  public void put(int field$, java.lang.Object value$) {
-    switch (field$) {
-    case 0: type = (java.lang.CharSequence)value$; break;
-    case 1: value = (java.lang.CharSequence)value$; break;
-    default: throw new org.apache.avro.AvroRuntimeException("Bad index");
-    }
-  }
-
-  /**
-   * Gets the value of the 'type' field.
-   * @return The value of the 'type' field.
-   */
-  public java.lang.CharSequence getType() {
-    return type;
-  }
-
-  /**
-   * Sets the value of the 'type' field.
-   * @param value the value to set.
-   */
-  public void setType(java.lang.CharSequence value) {
-    this.type = value;
-  }
-
-  /**
-   * Gets the value of the 'value' field.
-   * @return The value of the 'value' field.
-   */
-  public java.lang.CharSequence getValue() {
-    return value;
-  }
-
-  /**
-   * Sets the value of the 'value' field.
-   * @param value the value to set.
-   */
-  public void setValue(java.lang.CharSequence value) {
-    this.value = value;
-  }
-
-  /**
-   * Creates a new ArrayItem RecordBuilder.
-   * @return A new ArrayItem RecordBuilder
-   */
-  public static org.apache.flink.formats.parquet.generated.ArrayItem.Builder newBuilder() {
-    return new org.apache.flink.formats.parquet.generated.ArrayItem.Builder();
-  }
-
-  /**
-   * Creates a new ArrayItem RecordBuilder by copying an existing Builder.
-   * @param other The existing builder to copy.
-   * @return A new ArrayItem RecordBuilder
-   */
-  public static org.apache.flink.formats.parquet.generated.ArrayItem.Builder newBuilder(org.apache.flink.formats.parquet.generated.ArrayItem.Builder other) {
-    return new org.apache.flink.formats.parquet.generated.ArrayItem.Builder(other);
-  }
-
-  /**
-   * Creates a new ArrayItem RecordBuilder by copying an existing ArrayItem instance.
-   * @param other The existing instance to copy.
-   * @return A new ArrayItem RecordBuilder
-   */
-  public static org.apache.flink.formats.parquet.generated.ArrayItem.Builder newBuilder(org.apache.flink.formats.parquet.generated.ArrayItem other) {
-    return new org.apache.flink.formats.parquet.generated.ArrayItem.Builder(other);
-  }
-
-  /**
-   * RecordBuilder for ArrayItem instances.
-   */
-  public static class Builder extends org.apache.avro.specific.SpecificRecordBuilderBase<ArrayItem>
-    implements org.apache.avro.data.RecordBuilder<ArrayItem> {
-
-    private java.lang.CharSequence type;
-    private java.lang.CharSequence value;
-
-    /** Creates a new Builder */
-    private Builder() {
-      super(SCHEMA$);
-    }
-
-    /**
-     * Creates a Builder by copying an existing Builder.
-     * @param other The existing Builder to copy.
-     */
-    private Builder(org.apache.flink.formats.parquet.generated.ArrayItem.Builder other) {
-      super(other);
-      if (isValidValue(fields()[0], other.type)) {
-        this.type = data().deepCopy(fields()[0].schema(), other.type);
-        fieldSetFlags()[0] = true;
-      }
-      if (isValidValue(fields()[1], other.value)) {
-        this.value = data().deepCopy(fields()[1].schema(), other.value);
-        fieldSetFlags()[1] = true;
-      }
-    }
-
-    /**
-     * Creates a Builder by copying an existing ArrayItem instance
-     * @param other The existing instance to copy.
-     */
-    private Builder(org.apache.flink.formats.parquet.generated.ArrayItem other) {
-            super(SCHEMA$);
-      if (isValidValue(fields()[0], other.type)) {
-        this.type = data().deepCopy(fields()[0].schema(), other.type);
-        fieldSetFlags()[0] = true;
-      }
-      if (isValidValue(fields()[1], other.value)) {
-        this.value = data().deepCopy(fields()[1].schema(), other.value);
-        fieldSetFlags()[1] = true;
-      }
-    }
-
-    /**
-      * Gets the value of the 'type' field.
-      * @return The value.
-      */
-    public java.lang.CharSequence getType() {
-      return type;
-    }
-
-    /**
-      * Sets the value of the 'type' field.
-      * @param value The value of 'type'.
-      * @return This builder.
-      */
-    public org.apache.flink.formats.parquet.generated.ArrayItem.Builder setType(java.lang.CharSequence value) {
-      validate(fields()[0], value);
-      this.type = value;
-      fieldSetFlags()[0] = true;
-      return this;
-    }
-
-    /**
-      * Checks whether the 'type' field has been set.
-      * @return True if the 'type' field has been set, false otherwise.
-      */
-    public boolean hasType() {
-      return fieldSetFlags()[0];
-    }
-
-
-    /**
-      * Clears the value of the 'type' field.
-      * @return This builder.
-      */
-    public org.apache.flink.formats.parquet.generated.ArrayItem.Builder clearType() {
-      type = null;
-      fieldSetFlags()[0] = false;
-      return this;
-    }
-
-    /**
-      * Gets the value of the 'value' field.
-      * @return The value.
-      */
-    public java.lang.CharSequence getValue() {
-      return value;
-    }
-
-    /**
-      * Sets the value of the 'value' field.
-      * @param value The value of 'value'.
-      * @return This builder.
-      */
-    public org.apache.flink.formats.parquet.generated.ArrayItem.Builder setValue(java.lang.CharSequence value) {
-      validate(fields()[1], value);
-      this.value = value;
-      fieldSetFlags()[1] = true;
-      return this;
-    }
-
-    /**
-      * Checks whether the 'value' field has been set.
-      * @return True if the 'value' field has been set, false otherwise.
-      */
-    public boolean hasValue() {
-      return fieldSetFlags()[1];
-    }
-
-
-    /**
-      * Clears the value of the 'value' field.
-      * @return This builder.
-      */
-    public org.apache.flink.formats.parquet.generated.ArrayItem.Builder clearValue() {
-      value = null;
-      fieldSetFlags()[1] = false;
-      return this;
-    }
-
-    @Override
-    @SuppressWarnings("unchecked")
-    public ArrayItem build() {
-      try {
-        ArrayItem record = new ArrayItem();
-        record.type = fieldSetFlags()[0] ? this.type : (java.lang.CharSequence) defaultValue(fields()[0]);
-        record.value = fieldSetFlags()[1] ? this.value : (java.lang.CharSequence) defaultValue(fields()[1]);
-        return record;
-      } catch (java.lang.Exception e) {
-        throw new org.apache.avro.AvroRuntimeException(e);
-      }
-    }
-  }
-
-  @SuppressWarnings("unchecked")
-  private static final org.apache.avro.io.DatumWriter<ArrayItem>
-    WRITER$ = (org.apache.avro.io.DatumWriter<ArrayItem>)MODEL$.createDatumWriter(SCHEMA$);
-
-  @Override public void writeExternal(java.io.ObjectOutput out)
-    throws java.io.IOException {
-    WRITER$.write(this, SpecificData.getEncoder(out));
-  }
-
-  @SuppressWarnings("unchecked")
-  private static final org.apache.avro.io.DatumReader<ArrayItem>
-    READER$ = (org.apache.avro.io.DatumReader<ArrayItem>)MODEL$.createDatumReader(SCHEMA$);
-
-  @Override public void readExternal(java.io.ObjectInput in)
-    throws java.io.IOException {
-    READER$.read(this, SpecificData.getDecoder(in));
-  }
-
-}
diff --git a/flink-formats/flink-parquet/src/test/java/org/apache/flink/formats/parquet/generated/Bar.java b/flink-formats/flink-parquet/src/test/java/org/apache/flink/formats/parquet/generated/Bar.java
deleted file mode 100644
index e84f8b6e2492..000000000000
--- a/flink-formats/flink-parquet/src/test/java/org/apache/flink/formats/parquet/generated/Bar.java
+++ /dev/null
@@ -1,238 +0,0 @@
-/**
- * Autogenerated by Avro
- *
- * DO NOT EDIT DIRECTLY
- */
-package org.apache.flink.formats.parquet.generated;
-
-import org.apache.avro.specific.SpecificData;
-import org.apache.avro.message.BinaryMessageEncoder;
-import org.apache.avro.message.BinaryMessageDecoder;
-import org.apache.avro.message.SchemaStore;
-
-@SuppressWarnings("all")
-@org.apache.avro.specific.AvroGenerated
-public class Bar extends org.apache.avro.specific.SpecificRecordBase implements org.apache.avro.specific.SpecificRecord {
-  private static final long serialVersionUID = 1175980344330188560L;
-  public static final org.apache.avro.Schema SCHEMA$ = new org.apache.avro.Schema.Parser().parse("{\"type\":\"record\",\"name\":\"Bar\",\"namespace\":\"org.apache.flink.formats.parquet.generated\",\"fields\":[{\"name\":\"spam\",\"type\":[\"null\",\"long\"],\"default\":null}]}");
-  public static org.apache.avro.Schema getClassSchema() { return SCHEMA$; }
-
-  private static SpecificData MODEL$ = new SpecificData();
-
-  private static final BinaryMessageEncoder<Bar> ENCODER =
-      new BinaryMessageEncoder<Bar>(MODEL$, SCHEMA$);
-
-  private static final BinaryMessageDecoder<Bar> DECODER =
-      new BinaryMessageDecoder<Bar>(MODEL$, SCHEMA$);
-
-  /**
-   * Return the BinaryMessageDecoder instance used by this class.
-   */
-  public static BinaryMessageDecoder<Bar> getDecoder() {
-    return DECODER;
-  }
-
-  /**
-   * Create a new BinaryMessageDecoder instance for this class that uses the specified {@link SchemaStore}.
-   * @param resolver a {@link SchemaStore} used to find schemas by fingerprint
-   */
-  public static BinaryMessageDecoder<Bar> createDecoder(SchemaStore resolver) {
-    return new BinaryMessageDecoder<Bar>(MODEL$, SCHEMA$, resolver);
-  }
-
-  /** Serializes this Bar to a ByteBuffer. */
-  public java.nio.ByteBuffer toByteBuffer() throws java.io.IOException {
-    return ENCODER.encode(this);
-  }
-
-  /** Deserializes a Bar from a ByteBuffer. */
-  public static Bar fromByteBuffer(
-      java.nio.ByteBuffer b) throws java.io.IOException {
-    return DECODER.decode(b);
-  }
-
-  @Deprecated public java.lang.Long spam;
-
-  /**
-   * Default constructor.  Note that this does not initialize fields
-   * to their default values from the schema.  If that is desired then
-   * one should use <code>newBuilder()</code>.
-   */
-  public Bar() {}
-
-  /**
-   * All-args constructor.
-   * @param spam The new value for spam
-   */
-  public Bar(java.lang.Long spam) {
-    this.spam = spam;
-  }
-
-  public org.apache.avro.Schema getSchema() { return SCHEMA$; }
-  // Used by DatumWriter.  Applications should not call.
-  public java.lang.Object get(int field$) {
-    switch (field$) {
-    case 0: return spam;
-    default: throw new org.apache.avro.AvroRuntimeException("Bad index");
-    }
-  }
-
-  // Used by DatumReader.  Applications should not call.
-  @SuppressWarnings(value="unchecked")
-  public void put(int field$, java.lang.Object value$) {
-    switch (field$) {
-    case 0: spam = (java.lang.Long)value$; break;
-    default: throw new org.apache.avro.AvroRuntimeException("Bad index");
-    }
-  }
-
-  /**
-   * Gets the value of the 'spam' field.
-   * @return The value of the 'spam' field.
-   */
-  public java.lang.Long getSpam() {
-    return spam;
-  }
-
-  /**
-   * Sets the value of the 'spam' field.
-   * @param value the value to set.
-   */
-  public void setSpam(java.lang.Long value) {
-    this.spam = value;
-  }
-
-  /**
-   * Creates a new Bar RecordBuilder.
-   * @return A new Bar RecordBuilder
-   */
-  public static org.apache.flink.formats.parquet.generated.Bar.Builder newBuilder() {
-    return new org.apache.flink.formats.parquet.generated.Bar.Builder();
-  }
-
-  /**
-   * Creates a new Bar RecordBuilder by copying an existing Builder.
-   * @param other The existing builder to copy.
-   * @return A new Bar RecordBuilder
-   */
-  public static org.apache.flink.formats.parquet.generated.Bar.Builder newBuilder(org.apache.flink.formats.parquet.generated.Bar.Builder other) {
-    return new org.apache.flink.formats.parquet.generated.Bar.Builder(other);
-  }
-
-  /**
-   * Creates a new Bar RecordBuilder by copying an existing Bar instance.
-   * @param other The existing instance to copy.
-   * @return A new Bar RecordBuilder
-   */
-  public static org.apache.flink.formats.parquet.generated.Bar.Builder newBuilder(org.apache.flink.formats.parquet.generated.Bar other) {
-    return new org.apache.flink.formats.parquet.generated.Bar.Builder(other);
-  }
-
-  /**
-   * RecordBuilder for Bar instances.
-   */
-  public static class Builder extends org.apache.avro.specific.SpecificRecordBuilderBase<Bar>
-    implements org.apache.avro.data.RecordBuilder<Bar> {
-
-    private java.lang.Long spam;
-
-    /** Creates a new Builder */
-    private Builder() {
-      super(SCHEMA$);
-    }
-
-    /**
-     * Creates a Builder by copying an existing Builder.
-     * @param other The existing Builder to copy.
-     */
-    private Builder(org.apache.flink.formats.parquet.generated.Bar.Builder other) {
-      super(other);
-      if (isValidValue(fields()[0], other.spam)) {
-        this.spam = data().deepCopy(fields()[0].schema(), other.spam);
-        fieldSetFlags()[0] = true;
-      }
-    }
-
-    /**
-     * Creates a Builder by copying an existing Bar instance
-     * @param other The existing instance to copy.
-     */
-    private Builder(org.apache.flink.formats.parquet.generated.Bar other) {
-            super(SCHEMA$);
-      if (isValidValue(fields()[0], other.spam)) {
-        this.spam = data().deepCopy(fields()[0].schema(), other.spam);
-        fieldSetFlags()[0] = true;
-      }
-    }
-
-    /**
-      * Gets the value of the 'spam' field.
-      * @return The value.
-      */
-    public java.lang.Long getSpam() {
-      return spam;
-    }
-
-    /**
-      * Sets the value of the 'spam' field.
-      * @param value The value of 'spam'.
-      * @return This builder.
-      */
-    public org.apache.flink.formats.parquet.generated.Bar.Builder setSpam(java.lang.Long value) {
-      validate(fields()[0], value);
-      this.spam = value;
-      fieldSetFlags()[0] = true;
-      return this;
-    }
-
-    /**
-      * Checks whether the 'spam' field has been set.
-      * @return True if the 'spam' field has been set, false otherwise.
-      */
-    public boolean hasSpam() {
-      return fieldSetFlags()[0];
-    }
-
-
-    /**
-      * Clears the value of the 'spam' field.
-      * @return This builder.
-      */
-    public org.apache.flink.formats.parquet.generated.Bar.Builder clearSpam() {
-      spam = null;
-      fieldSetFlags()[0] = false;
-      return this;
-    }
-
-    @Override
-    @SuppressWarnings("unchecked")
-    public Bar build() {
-      try {
-        Bar record = new Bar();
-        record.spam = fieldSetFlags()[0] ? this.spam : (java.lang.Long) defaultValue(fields()[0]);
-        return record;
-      } catch (java.lang.Exception e) {
-        throw new org.apache.avro.AvroRuntimeException(e);
-      }
-    }
-  }
-
-  @SuppressWarnings("unchecked")
-  private static final org.apache.avro.io.DatumWriter<Bar>
-    WRITER$ = (org.apache.avro.io.DatumWriter<Bar>)MODEL$.createDatumWriter(SCHEMA$);
-
-  @Override public void writeExternal(java.io.ObjectOutput out)
-    throws java.io.IOException {
-    WRITER$.write(this, SpecificData.getEncoder(out));
-  }
-
-  @SuppressWarnings("unchecked")
-  private static final org.apache.avro.io.DatumReader<Bar>
-    READER$ = (org.apache.avro.io.DatumReader<Bar>)MODEL$.createDatumReader(SCHEMA$);
-
-  @Override public void readExternal(java.io.ObjectInput in)
-    throws java.io.IOException {
-    READER$.read(this, SpecificData.getDecoder(in));
-  }
-
-}
diff --git a/flink-formats/flink-parquet/src/test/java/org/apache/flink/formats/parquet/generated/MapItem.java b/flink-formats/flink-parquet/src/test/java/org/apache/flink/formats/parquet/generated/MapItem.java
deleted file mode 100644
index 5c055e132452..000000000000
--- a/flink-formats/flink-parquet/src/test/java/org/apache/flink/formats/parquet/generated/MapItem.java
+++ /dev/null
@@ -1,308 +0,0 @@
-/**
- * Autogenerated by Avro
- *
- * DO NOT EDIT DIRECTLY
- */
-package org.apache.flink.formats.parquet.generated;
-
-import org.apache.avro.specific.SpecificData;
-import org.apache.avro.message.BinaryMessageEncoder;
-import org.apache.avro.message.BinaryMessageDecoder;
-import org.apache.avro.message.SchemaStore;
-
-@SuppressWarnings("all")
-@org.apache.avro.specific.AvroGenerated
-public class MapItem extends org.apache.avro.specific.SpecificRecordBase implements org.apache.avro.specific.SpecificRecord {
-  private static final long serialVersionUID = 8474786067806918777L;
-  public static final org.apache.avro.Schema SCHEMA$ = new org.apache.avro.Schema.Parser().parse("{\"type\":\"record\",\"name\":\"MapItem\",\"namespace\":\"org.apache.flink.formats.parquet.generated\",\"fields\":[{\"name\":\"type\",\"type\":[\"null\",\"string\"]},{\"name\":\"value\",\"type\":[\"null\",\"string\"]}]}");
-  public static org.apache.avro.Schema getClassSchema() { return SCHEMA$; }
-
-  private static SpecificData MODEL$ = new SpecificData();
-
-  private static final BinaryMessageEncoder<MapItem> ENCODER =
-      new BinaryMessageEncoder<MapItem>(MODEL$, SCHEMA$);
-
-  private static final BinaryMessageDecoder<MapItem> DECODER =
-      new BinaryMessageDecoder<MapItem>(MODEL$, SCHEMA$);
-
-  /**
-   * Return the BinaryMessageDecoder instance used by this class.
-   */
-  public static BinaryMessageDecoder<MapItem> getDecoder() {
-    return DECODER;
-  }
-
-  /**
-   * Create a new BinaryMessageDecoder instance for this class that uses the specified {@link SchemaStore}.
-   * @param resolver a {@link SchemaStore} used to find schemas by fingerprint
-   */
-  public static BinaryMessageDecoder<MapItem> createDecoder(SchemaStore resolver) {
-    return new BinaryMessageDecoder<MapItem>(MODEL$, SCHEMA$, resolver);
-  }
-
-  /** Serializes this MapItem to a ByteBuffer. */
-  public java.nio.ByteBuffer toByteBuffer() throws java.io.IOException {
-    return ENCODER.encode(this);
-  }
-
-  /** Deserializes a MapItem from a ByteBuffer. */
-  public static MapItem fromByteBuffer(
-      java.nio.ByteBuffer b) throws java.io.IOException {
-    return DECODER.decode(b);
-  }
-
-  @Deprecated public java.lang.CharSequence type;
-  @Deprecated public java.lang.CharSequence value;
-
-  /**
-   * Default constructor.  Note that this does not initialize fields
-   * to their default values from the schema.  If that is desired then
-   * one should use <code>newBuilder()</code>.
-   */
-  public MapItem() {}
-
-  /**
-   * All-args constructor.
-   * @param type The new value for type
-   * @param value The new value for value
-   */
-  public MapItem(java.lang.CharSequence type, java.lang.CharSequence value) {
-    this.type = type;
-    this.value = value;
-  }
-
-  public org.apache.avro.Schema getSchema() { return SCHEMA$; }
-  // Used by DatumWriter.  Applications should not call.
-  public java.lang.Object get(int field$) {
-    switch (field$) {
-    case 0: return type;
-    case 1: return value;
-    default: throw new org.apache.avro.AvroRuntimeException("Bad index");
-    }
-  }
-
-  // Used by DatumReader.  Applications should not call.
-  @SuppressWarnings(value="unchecked")
-  public void put(int field$, java.lang.Object value$) {
-    switch (field$) {
-    case 0: type = (java.lang.CharSequence)value$; break;
-    case 1: value = (java.lang.CharSequence)value$; break;
-    default: throw new org.apache.avro.AvroRuntimeException("Bad index");
-    }
-  }
-
-  /**
-   * Gets the value of the 'type' field.
-   * @return The value of the 'type' field.
-   */
-  public java.lang.CharSequence getType() {
-    return type;
-  }
-
-  /**
-   * Sets the value of the 'type' field.
-   * @param value the value to set.
-   */
-  public void setType(java.lang.CharSequence value) {
-    this.type = value;
-  }
-
-  /**
-   * Gets the value of the 'value' field.
-   * @return The value of the 'value' field.
-   */
-  public java.lang.CharSequence getValue() {
-    return value;
-  }
-
-  /**
-   * Sets the value of the 'value' field.
-   * @param value the value to set.
-   */
-  public void setValue(java.lang.CharSequence value) {
-    this.value = value;
-  }
-
-  /**
-   * Creates a new MapItem RecordBuilder.
-   * @return A new MapItem RecordBuilder
-   */
-  public static org.apache.flink.formats.parquet.generated.MapItem.Builder newBuilder() {
-    return new org.apache.flink.formats.parquet.generated.MapItem.Builder();
-  }
-
-  /**
-   * Creates a new MapItem RecordBuilder by copying an existing Builder.
-   * @param other The existing builder to copy.
-   * @return A new MapItem RecordBuilder
-   */
-  public static org.apache.flink.formats.parquet.generated.MapItem.Builder newBuilder(org.apache.flink.formats.parquet.generated.MapItem.Builder other) {
-    return new org.apache.flink.formats.parquet.generated.MapItem.Builder(other);
-  }
-
-  /**
-   * Creates a new MapItem RecordBuilder by copying an existing MapItem instance.
-   * @param other The existing instance to copy.
-   * @return A new MapItem RecordBuilder
-   */
-  public static org.apache.flink.formats.parquet.generated.MapItem.Builder newBuilder(org.apache.flink.formats.parquet.generated.MapItem other) {
-    return new org.apache.flink.formats.parquet.generated.MapItem.Builder(other);
-  }
-
-  /**
-   * RecordBuilder for MapItem instances.
-   */
-  public static class Builder extends org.apache.avro.specific.SpecificRecordBuilderBase<MapItem>
-    implements org.apache.avro.data.RecordBuilder<MapItem> {
-
-    private java.lang.CharSequence type;
-    private java.lang.CharSequence value;
-
-    /** Creates a new Builder */
-    private Builder() {
-      super(SCHEMA$);
-    }
-
-    /**
-     * Creates a Builder by copying an existing Builder.
-     * @param other The existing Builder to copy.
-     */
-    private Builder(org.apache.flink.formats.parquet.generated.MapItem.Builder other) {
-      super(other);
-      if (isValidValue(fields()[0], other.type)) {
-        this.type = data().deepCopy(fields()[0].schema(), other.type);
-        fieldSetFlags()[0] = true;
-      }
-      if (isValidValue(fields()[1], other.value)) {
-        this.value = data().deepCopy(fields()[1].schema(), other.value);
-        fieldSetFlags()[1] = true;
-      }
-    }
-
-    /**
-     * Creates a Builder by copying an existing MapItem instance
-     * @param other The existing instance to copy.
-     */
-    private Builder(org.apache.flink.formats.parquet.generated.MapItem other) {
-            super(SCHEMA$);
-      if (isValidValue(fields()[0], other.type)) {
-        this.type = data().deepCopy(fields()[0].schema(), other.type);
-        fieldSetFlags()[0] = true;
-      }
-      if (isValidValue(fields()[1], other.value)) {
-        this.value = data().deepCopy(fields()[1].schema(), other.value);
-        fieldSetFlags()[1] = true;
-      }
-    }
-
-    /**
-      * Gets the value of the 'type' field.
-      * @return The value.
-      */
-    public java.lang.CharSequence getType() {
-      return type;
-    }
-
-    /**
-      * Sets the value of the 'type' field.
-      * @param value The value of 'type'.
-      * @return This builder.
-      */
-    public org.apache.flink.formats.parquet.generated.MapItem.Builder setType(java.lang.CharSequence value) {
-      validate(fields()[0], value);
-      this.type = value;
-      fieldSetFlags()[0] = true;
-      return this;
-    }
-
-    /**
-      * Checks whether the 'type' field has been set.
-      * @return True if the 'type' field has been set, false otherwise.
-      */
-    public boolean hasType() {
-      return fieldSetFlags()[0];
-    }
-
-
-    /**
-      * Clears the value of the 'type' field.
-      * @return This builder.
-      */
-    public org.apache.flink.formats.parquet.generated.MapItem.Builder clearType() {
-      type = null;
-      fieldSetFlags()[0] = false;
-      return this;
-    }
-
-    /**
-      * Gets the value of the 'value' field.
-      * @return The value.
-      */
-    public java.lang.CharSequence getValue() {
-      return value;
-    }
-
-    /**
-      * Sets the value of the 'value' field.
-      * @param value The value of 'value'.
-      * @return This builder.
-      */
-    public org.apache.flink.formats.parquet.generated.MapItem.Builder setValue(java.lang.CharSequence value) {
-      validate(fields()[1], value);
-      this.value = value;
-      fieldSetFlags()[1] = true;
-      return this;
-    }
-
-    /**
-      * Checks whether the 'value' field has been set.
-      * @return True if the 'value' field has been set, false otherwise.
-      */
-    public boolean hasValue() {
-      return fieldSetFlags()[1];
-    }
-
-
-    /**
-      * Clears the value of the 'value' field.
-      * @return This builder.
-      */
-    public org.apache.flink.formats.parquet.generated.MapItem.Builder clearValue() {
-      value = null;
-      fieldSetFlags()[1] = false;
-      return this;
-    }
-
-    @Override
-    @SuppressWarnings("unchecked")
-    public MapItem build() {
-      try {
-        MapItem record = new MapItem();
-        record.type = fieldSetFlags()[0] ? this.type : (java.lang.CharSequence) defaultValue(fields()[0]);
-        record.value = fieldSetFlags()[1] ? this.value : (java.lang.CharSequence) defaultValue(fields()[1]);
-        return record;
-      } catch (java.lang.Exception e) {
-        throw new org.apache.avro.AvroRuntimeException(e);
-      }
-    }
-  }
-
-  @SuppressWarnings("unchecked")
-  private static final org.apache.avro.io.DatumWriter<MapItem>
-    WRITER$ = (org.apache.avro.io.DatumWriter<MapItem>)MODEL$.createDatumWriter(SCHEMA$);
-
-  @Override public void writeExternal(java.io.ObjectOutput out)
-    throws java.io.IOException {
-    WRITER$.write(this, SpecificData.getEncoder(out));
-  }
-
-  @SuppressWarnings("unchecked")
-  private static final org.apache.avro.io.DatumReader<MapItem>
-    READER$ = (org.apache.avro.io.DatumReader<MapItem>)MODEL$.createDatumReader(SCHEMA$);
-
-  @Override public void readExternal(java.io.ObjectInput in)
-    throws java.io.IOException {
-    READER$.read(this, SpecificData.getDecoder(in));
-  }
-
-}
diff --git a/flink-formats/flink-parquet/src/test/java/org/apache/flink/formats/parquet/generated/NestedRecord.java b/flink-formats/flink-parquet/src/test/java/org/apache/flink/formats/parquet/generated/NestedRecord.java
deleted file mode 100644
index 8563e5473d91..000000000000
--- a/flink-formats/flink-parquet/src/test/java/org/apache/flink/formats/parquet/generated/NestedRecord.java
+++ /dev/null
@@ -1,702 +0,0 @@
-/**
- * Autogenerated by Avro
- *
- * DO NOT EDIT DIRECTLY
- */
-package org.apache.flink.formats.parquet.generated;
-
-import org.apache.avro.specific.SpecificData;
-import org.apache.avro.message.BinaryMessageEncoder;
-import org.apache.avro.message.BinaryMessageDecoder;
-import org.apache.avro.message.SchemaStore;
-
-@SuppressWarnings("all")
-@org.apache.avro.specific.AvroGenerated
-public class NestedRecord extends org.apache.avro.specific.SpecificRecordBase implements org.apache.avro.specific.SpecificRecord {
-  private static final long serialVersionUID = -8594103348180891649L;
-  public static final org.apache.avro.Schema SCHEMA$ = new org.apache.avro.Schema.Parser().parse("{\"type\":\"record\",\"name\":\"NestedRecord\",\"namespace\":\"org.apache.flink.formats.parquet.generated\",\"fields\":[{\"name\":\"foo\",\"type\":[\"null\",\"long\"],\"default\":null},{\"name\":\"spamMap\",\"type\":[\"null\",{\"type\":\"map\",\"values\":\"string\"}],\"default\":null},{\"name\":\"bar\",\"type\":[\"null\",{\"type\":\"record\",\"name\":\"Bar\",\"fields\":[{\"name\":\"spam\",\"type\":[\"null\",\"long\"],\"default\":null}]}],\"default\":null},{\"name\":\"arr\",\"type\":[\"null\",{\"type\":\"array\",\"items\":\"long\"}],\"default\":null},{\"name\":\"strArray\",\"type\":[\"null\",{\"type\":\"array\",\"items\":\"string\"}],\"default\":null},{\"name\":\"nestedMap\",\"type\":[\"null\",{\"type\":\"map\",\"values\":{\"type\":\"record\",\"name\":\"MapItem\",\"fields\":[{\"name\":\"type\",\"type\":[\"null\",\"string\"]},{\"name\":\"value\",\"type\":[\"null\",\"string\"]}]}}],\"default\":null},{\"name\":\"nestedArray\",\"type\":[\"null\",{\"type\":\"array\",\"items\":{\"type\":\"record\",\"name\":\"ArrayItem\",\"fields\":[{\"name\":\"type\",\"type\":[\"null\",\"string\"]},{\"name\":\"value\",\"type\":[\"null\",\"string\"]}]}}],\"default\":null}],\"schema_id\":1}");
-  public static org.apache.avro.Schema getClassSchema() { return SCHEMA$; }
-
-  private static SpecificData MODEL$ = new SpecificData();
-
-  private static final BinaryMessageEncoder<NestedRecord> ENCODER =
-      new BinaryMessageEncoder<NestedRecord>(MODEL$, SCHEMA$);
-
-  private static final BinaryMessageDecoder<NestedRecord> DECODER =
-      new BinaryMessageDecoder<NestedRecord>(MODEL$, SCHEMA$);
-
-  /**
-   * Return the BinaryMessageDecoder instance used by this class.
-   */
-  public static BinaryMessageDecoder<NestedRecord> getDecoder() {
-    return DECODER;
-  }
-
-  /**
-   * Create a new BinaryMessageDecoder instance for this class that uses the specified {@link SchemaStore}.
-   * @param resolver a {@link SchemaStore} used to find schemas by fingerprint
-   */
-  public static BinaryMessageDecoder<NestedRecord> createDecoder(SchemaStore resolver) {
-    return new BinaryMessageDecoder<NestedRecord>(MODEL$, SCHEMA$, resolver);
-  }
-
-  /** Serializes this NestedRecord to a ByteBuffer. */
-  public java.nio.ByteBuffer toByteBuffer() throws java.io.IOException {
-    return ENCODER.encode(this);
-  }
-
-  /** Deserializes a NestedRecord from a ByteBuffer. */
-  public static NestedRecord fromByteBuffer(
-      java.nio.ByteBuffer b) throws java.io.IOException {
-    return DECODER.decode(b);
-  }
-
-  @Deprecated public java.lang.Long foo;
-  @Deprecated public java.util.Map<java.lang.CharSequence,java.lang.CharSequence> spamMap;
-  @Deprecated public org.apache.flink.formats.parquet.generated.Bar bar;
-  @Deprecated public java.util.List<java.lang.Long> arr;
-  @Deprecated public java.util.List<java.lang.CharSequence> strArray;
-  @Deprecated public java.util.Map<java.lang.CharSequence,org.apache.flink.formats.parquet.generated.MapItem> nestedMap;
-  @Deprecated public java.util.List<org.apache.flink.formats.parquet.generated.ArrayItem> nestedArray;
-
-  /**
-   * Default constructor.  Note that this does not initialize fields
-   * to their default values from the schema.  If that is desired then
-   * one should use <code>newBuilder()</code>.
-   */
-  public NestedRecord() {}
-
-  /**
-   * All-args constructor.
-   * @param foo The new value for foo
-   * @param spamMap The new value for spamMap
-   * @param bar The new value for bar
-   * @param arr The new value for arr
-   * @param strArray The new value for strArray
-   * @param nestedMap The new value for nestedMap
-   * @param nestedArray The new value for nestedArray
-   */
-  public NestedRecord(java.lang.Long foo, java.util.Map<java.lang.CharSequence,java.lang.CharSequence> spamMap, org.apache.flink.formats.parquet.generated.Bar bar, java.util.List<java.lang.Long> arr, java.util.List<java.lang.CharSequence> strArray, java.util.Map<java.lang.CharSequence,org.apache.flink.formats.parquet.generated.MapItem> nestedMap, java.util.List<org.apache.flink.formats.parquet.generated.ArrayItem> nestedArray) {
-    this.foo = foo;
-    this.spamMap = spamMap;
-    this.bar = bar;
-    this.arr = arr;
-    this.strArray = strArray;
-    this.nestedMap = nestedMap;
-    this.nestedArray = nestedArray;
-  }
-
-  public org.apache.avro.Schema getSchema() { return SCHEMA$; }
-  // Used by DatumWriter.  Applications should not call.
-  public java.lang.Object get(int field$) {
-    switch (field$) {
-    case 0: return foo;
-    case 1: return spamMap;
-    case 2: return bar;
-    case 3: return arr;
-    case 4: return strArray;
-    case 5: return nestedMap;
-    case 6: return nestedArray;
-    default: throw new org.apache.avro.AvroRuntimeException("Bad index");
-    }
-  }
-
-  // Used by DatumReader.  Applications should not call.
-  @SuppressWarnings(value="unchecked")
-  public void put(int field$, java.lang.Object value$) {
-    switch (field$) {
-    case 0: foo = (java.lang.Long)value$; break;
-    case 1: spamMap = (java.util.Map<java.lang.CharSequence,java.lang.CharSequence>)value$; break;
-    case 2: bar = (org.apache.flink.formats.parquet.generated.Bar)value$; break;
-    case 3: arr = (java.util.List<java.lang.Long>)value$; break;
-    case 4: strArray = (java.util.List<java.lang.CharSequence>)value$; break;
-    case 5: nestedMap = (java.util.Map<java.lang.CharSequence,org.apache.flink.formats.parquet.generated.MapItem>)value$; break;
-    case 6: nestedArray = (java.util.List<org.apache.flink.formats.parquet.generated.ArrayItem>)value$; break;
-    default: throw new org.apache.avro.AvroRuntimeException("Bad index");
-    }
-  }
-
-  /**
-   * Gets the value of the 'foo' field.
-   * @return The value of the 'foo' field.
-   */
-  public java.lang.Long getFoo() {
-    return foo;
-  }
-
-  /**
-   * Sets the value of the 'foo' field.
-   * @param value the value to set.
-   */
-  public void setFoo(java.lang.Long value) {
-    this.foo = value;
-  }
-
-  /**
-   * Gets the value of the 'spamMap' field.
-   * @return The value of the 'spamMap' field.
-   */
-  public java.util.Map<java.lang.CharSequence,java.lang.CharSequence> getSpamMap() {
-    return spamMap;
-  }
-
-  /**
-   * Sets the value of the 'spamMap' field.
-   * @param value the value to set.
-   */
-  public void setSpamMap(java.util.Map<java.lang.CharSequence,java.lang.CharSequence> value) {
-    this.spamMap = value;
-  }
-
-  /**
-   * Gets the value of the 'bar' field.
-   * @return The value of the 'bar' field.
-   */
-  public org.apache.flink.formats.parquet.generated.Bar getBar() {
-    return bar;
-  }
-
-  /**
-   * Sets the value of the 'bar' field.
-   * @param value the value to set.
-   */
-  public void setBar(org.apache.flink.formats.parquet.generated.Bar value) {
-    this.bar = value;
-  }
-
-  /**
-   * Gets the value of the 'arr' field.
-   * @return The value of the 'arr' field.
-   */
-  public java.util.List<java.lang.Long> getArr() {
-    return arr;
-  }
-
-  /**
-   * Sets the value of the 'arr' field.
-   * @param value the value to set.
-   */
-  public void setArr(java.util.List<java.lang.Long> value) {
-    this.arr = value;
-  }
-
-  /**
-   * Gets the value of the 'strArray' field.
-   * @return The value of the 'strArray' field.
-   */
-  public java.util.List<java.lang.CharSequence> getStrArray() {
-    return strArray;
-  }
-
-  /**
-   * Sets the value of the 'strArray' field.
-   * @param value the value to set.
-   */
-  public void setStrArray(java.util.List<java.lang.CharSequence> value) {
-    this.strArray = value;
-  }
-
-  /**
-   * Gets the value of the 'nestedMap' field.
-   * @return The value of the 'nestedMap' field.
-   */
-  public java.util.Map<java.lang.CharSequence,org.apache.flink.formats.parquet.generated.MapItem> getNestedMap() {
-    return nestedMap;
-  }
-
-  /**
-   * Sets the value of the 'nestedMap' field.
-   * @param value the value to set.
-   */
-  public void setNestedMap(java.util.Map<java.lang.CharSequence,org.apache.flink.formats.parquet.generated.MapItem> value) {
-    this.nestedMap = value;
-  }
-
-  /**
-   * Gets the value of the 'nestedArray' field.
-   * @return The value of the 'nestedArray' field.
-   */
-  public java.util.List<org.apache.flink.formats.parquet.generated.ArrayItem> getNestedArray() {
-    return nestedArray;
-  }
-
-  /**
-   * Sets the value of the 'nestedArray' field.
-   * @param value the value to set.
-   */
-  public void setNestedArray(java.util.List<org.apache.flink.formats.parquet.generated.ArrayItem> value) {
-    this.nestedArray = value;
-  }
-
-  /**
-   * Creates a new NestedRecord RecordBuilder.
-   * @return A new NestedRecord RecordBuilder
-   */
-  public static org.apache.flink.formats.parquet.generated.NestedRecord.Builder newBuilder() {
-    return new org.apache.flink.formats.parquet.generated.NestedRecord.Builder();
-  }
-
-  /**
-   * Creates a new NestedRecord RecordBuilder by copying an existing Builder.
-   * @param other The existing builder to copy.
-   * @return A new NestedRecord RecordBuilder
-   */
-  public static org.apache.flink.formats.parquet.generated.NestedRecord.Builder newBuilder(org.apache.flink.formats.parquet.generated.NestedRecord.Builder other) {
-    return new org.apache.flink.formats.parquet.generated.NestedRecord.Builder(other);
-  }
-
-  /**
-   * Creates a new NestedRecord RecordBuilder by copying an existing NestedRecord instance.
-   * @param other The existing instance to copy.
-   * @return A new NestedRecord RecordBuilder
-   */
-  public static org.apache.flink.formats.parquet.generated.NestedRecord.Builder newBuilder(org.apache.flink.formats.parquet.generated.NestedRecord other) {
-    return new org.apache.flink.formats.parquet.generated.NestedRecord.Builder(other);
-  }
-
-  /**
-   * RecordBuilder for NestedRecord instances.
-   */
-  public static class Builder extends org.apache.avro.specific.SpecificRecordBuilderBase<NestedRecord>
-    implements org.apache.avro.data.RecordBuilder<NestedRecord> {
-
-    private java.lang.Long foo;
-    private java.util.Map<java.lang.CharSequence,java.lang.CharSequence> spamMap;
-    private org.apache.flink.formats.parquet.generated.Bar bar;
-    private org.apache.flink.formats.parquet.generated.Bar.Builder barBuilder;
-    private java.util.List<java.lang.Long> arr;
-    private java.util.List<java.lang.CharSequence> strArray;
-    private java.util.Map<java.lang.CharSequence,org.apache.flink.formats.parquet.generated.MapItem> nestedMap;
-    private java.util.List<org.apache.flink.formats.parquet.generated.ArrayItem> nestedArray;
-
-    /** Creates a new Builder */
-    private Builder() {
-      super(SCHEMA$);
-    }
-
-    /**
-     * Creates a Builder by copying an existing Builder.
-     * @param other The existing Builder to copy.
-     */
-    private Builder(org.apache.flink.formats.parquet.generated.NestedRecord.Builder other) {
-      super(other);
-      if (isValidValue(fields()[0], other.foo)) {
-        this.foo = data().deepCopy(fields()[0].schema(), other.foo);
-        fieldSetFlags()[0] = true;
-      }
-      if (isValidValue(fields()[1], other.spamMap)) {
-        this.spamMap = data().deepCopy(fields()[1].schema(), other.spamMap);
-        fieldSetFlags()[1] = true;
-      }
-      if (isValidValue(fields()[2], other.bar)) {
-        this.bar = data().deepCopy(fields()[2].schema(), other.bar);
-        fieldSetFlags()[2] = true;
-      }
-      if (other.hasBarBuilder()) {
-        this.barBuilder = org.apache.flink.formats.parquet.generated.Bar.newBuilder(other.getBarBuilder());
-      }
-      if (isValidValue(fields()[3], other.arr)) {
-        this.arr = data().deepCopy(fields()[3].schema(), other.arr);
-        fieldSetFlags()[3] = true;
-      }
-      if (isValidValue(fields()[4], other.strArray)) {
-        this.strArray = data().deepCopy(fields()[4].schema(), other.strArray);
-        fieldSetFlags()[4] = true;
-      }
-      if (isValidValue(fields()[5], other.nestedMap)) {
-        this.nestedMap = data().deepCopy(fields()[5].schema(), other.nestedMap);
-        fieldSetFlags()[5] = true;
-      }
-      if (isValidValue(fields()[6], other.nestedArray)) {
-        this.nestedArray = data().deepCopy(fields()[6].schema(), other.nestedArray);
-        fieldSetFlags()[6] = true;
-      }
-    }
-
-    /**
-     * Creates a Builder by copying an existing NestedRecord instance
-     * @param other The existing instance to copy.
-     */
-    private Builder(org.apache.flink.formats.parquet.generated.NestedRecord other) {
-            super(SCHEMA$);
-      if (isValidValue(fields()[0], other.foo)) {
-        this.foo = data().deepCopy(fields()[0].schema(), other.foo);
-        fieldSetFlags()[0] = true;
-      }
-      if (isValidValue(fields()[1], other.spamMap)) {
-        this.spamMap = data().deepCopy(fields()[1].schema(), other.spamMap);
-        fieldSetFlags()[1] = true;
-      }
-      if (isValidValue(fields()[2], other.bar)) {
-        this.bar = data().deepCopy(fields()[2].schema(), other.bar);
-        fieldSetFlags()[2] = true;
-      }
-      this.barBuilder = null;
-      if (isValidValue(fields()[3], other.arr)) {
-        this.arr = data().deepCopy(fields()[3].schema(), other.arr);
-        fieldSetFlags()[3] = true;
-      }
-      if (isValidValue(fields()[4], other.strArray)) {
-        this.strArray = data().deepCopy(fields()[4].schema(), other.strArray);
-        fieldSetFlags()[4] = true;
-      }
-      if (isValidValue(fields()[5], other.nestedMap)) {
-        this.nestedMap = data().deepCopy(fields()[5].schema(), other.nestedMap);
-        fieldSetFlags()[5] = true;
-      }
-      if (isValidValue(fields()[6], other.nestedArray)) {
-        this.nestedArray = data().deepCopy(fields()[6].schema(), other.nestedArray);
-        fieldSetFlags()[6] = true;
-      }
-    }
-
-    /**
-      * Gets the value of the 'foo' field.
-      * @return The value.
-      */
-    public java.lang.Long getFoo() {
-      return foo;
-    }
-
-    /**
-      * Sets the value of the 'foo' field.
-      * @param value The value of 'foo'.
-      * @return This builder.
-      */
-    public org.apache.flink.formats.parquet.generated.NestedRecord.Builder setFoo(java.lang.Long value) {
-      validate(fields()[0], value);
-      this.foo = value;
-      fieldSetFlags()[0] = true;
-      return this;
-    }
-
-    /**
-      * Checks whether the 'foo' field has been set.
-      * @return True if the 'foo' field has been set, false otherwise.
-      */
-    public boolean hasFoo() {
-      return fieldSetFlags()[0];
-    }
-
-
-    /**
-      * Clears the value of the 'foo' field.
-      * @return This builder.
-      */
-    public org.apache.flink.formats.parquet.generated.NestedRecord.Builder clearFoo() {
-      foo = null;
-      fieldSetFlags()[0] = false;
-      return this;
-    }
-
-    /**
-      * Gets the value of the 'spamMap' field.
-      * @return The value.
-      */
-    public java.util.Map<java.lang.CharSequence,java.lang.CharSequence> getSpamMap() {
-      return spamMap;
-    }
-
-    /**
-      * Sets the value of the 'spamMap' field.
-      * @param value The value of 'spamMap'.
-      * @return This builder.
-      */
-    public org.apache.flink.formats.parquet.generated.NestedRecord.Builder setSpamMap(java.util.Map<java.lang.CharSequence,java.lang.CharSequence> value) {
-      validate(fields()[1], value);
-      this.spamMap = value;
-      fieldSetFlags()[1] = true;
-      return this;
-    }
-
-    /**
-      * Checks whether the 'spamMap' field has been set.
-      * @return True if the 'spamMap' field has been set, false otherwise.
-      */
-    public boolean hasSpamMap() {
-      return fieldSetFlags()[1];
-    }
-
-
-    /**
-      * Clears the value of the 'spamMap' field.
-      * @return This builder.
-      */
-    public org.apache.flink.formats.parquet.generated.NestedRecord.Builder clearSpamMap() {
-      spamMap = null;
-      fieldSetFlags()[1] = false;
-      return this;
-    }
-
-    /**
-      * Gets the value of the 'bar' field.
-      * @return The value.
-      */
-    public org.apache.flink.formats.parquet.generated.Bar getBar() {
-      return bar;
-    }
-
-    /**
-      * Sets the value of the 'bar' field.
-      * @param value The value of 'bar'.
-      * @return This builder.
-      */
-    public org.apache.flink.formats.parquet.generated.NestedRecord.Builder setBar(org.apache.flink.formats.parquet.generated.Bar value) {
-      validate(fields()[2], value);
-      this.barBuilder = null;
-      this.bar = value;
-      fieldSetFlags()[2] = true;
-      return this;
-    }
-
-    /**
-      * Checks whether the 'bar' field has been set.
-      * @return True if the 'bar' field has been set, false otherwise.
-      */
-    public boolean hasBar() {
-      return fieldSetFlags()[2];
-    }
-
-    /**
-     * Gets the Builder instance for the 'bar' field and creates one if it doesn't exist yet.
-     * @return This builder.
-     */
-    public org.apache.flink.formats.parquet.generated.Bar.Builder getBarBuilder() {
-      if (barBuilder == null) {
-        if (hasBar()) {
-          setBarBuilder(org.apache.flink.formats.parquet.generated.Bar.newBuilder(bar));
-        } else {
-          setBarBuilder(org.apache.flink.formats.parquet.generated.Bar.newBuilder());
-        }
-      }
-      return barBuilder;
-    }
-
-    /**
-     * Sets the Builder instance for the 'bar' field
-     * @param value The builder instance that must be set.
-     * @return This builder.
-     */
-    public org.apache.flink.formats.parquet.generated.NestedRecord.Builder setBarBuilder(org.apache.flink.formats.parquet.generated.Bar.Builder value) {
-      clearBar();
-      barBuilder = value;
-      return this;
-    }
-
-    /**
-     * Checks whether the 'bar' field has an active Builder instance
-     * @return True if the 'bar' field has an active Builder instance
-     */
-    public boolean hasBarBuilder() {
-      return barBuilder != null;
-    }
-
-    /**
-      * Clears the value of the 'bar' field.
-      * @return This builder.
-      */
-    public org.apache.flink.formats.parquet.generated.NestedRecord.Builder clearBar() {
-      bar = null;
-      barBuilder = null;
-      fieldSetFlags()[2] = false;
-      return this;
-    }
-
-    /**
-      * Gets the value of the 'arr' field.
-      * @return The value.
-      */
-    public java.util.List<java.lang.Long> getArr() {
-      return arr;
-    }
-
-    /**
-      * Sets the value of the 'arr' field.
-      * @param value The value of 'arr'.
-      * @return This builder.
-      */
-    public org.apache.flink.formats.parquet.generated.NestedRecord.Builder setArr(java.util.List<java.lang.Long> value) {
-      validate(fields()[3], value);
-      this.arr = value;
-      fieldSetFlags()[3] = true;
-      return this;
-    }
-
-    /**
-      * Checks whether the 'arr' field has been set.
-      * @return True if the 'arr' field has been set, false otherwise.
-      */
-    public boolean hasArr() {
-      return fieldSetFlags()[3];
-    }
-
-
-    /**
-      * Clears the value of the 'arr' field.
-      * @return This builder.
-      */
-    public org.apache.flink.formats.parquet.generated.NestedRecord.Builder clearArr() {
-      arr = null;
-      fieldSetFlags()[3] = false;
-      return this;
-    }
-
-    /**
-      * Gets the value of the 'strArray' field.
-      * @return The value.
-      */
-    public java.util.List<java.lang.CharSequence> getStrArray() {
-      return strArray;
-    }
-
-    /**
-      * Sets the value of the 'strArray' field.
-      * @param value The value of 'strArray'.
-      * @return This builder.
-      */
-    public org.apache.flink.formats.parquet.generated.NestedRecord.Builder setStrArray(java.util.List<java.lang.CharSequence> value) {
-      validate(fields()[4], value);
-      this.strArray = value;
-      fieldSetFlags()[4] = true;
-      return this;
-    }
-
-    /**
-      * Checks whether the 'strArray' field has been set.
-      * @return True if the 'strArray' field has been set, false otherwise.
-      */
-    public boolean hasStrArray() {
-      return fieldSetFlags()[4];
-    }
-
-
-    /**
-      * Clears the value of the 'strArray' field.
-      * @return This builder.
-      */
-    public org.apache.flink.formats.parquet.generated.NestedRecord.Builder clearStrArray() {
-      strArray = null;
-      fieldSetFlags()[4] = false;
-      return this;
-    }
-
-    /**
-      * Gets the value of the 'nestedMap' field.
-      * @return The value.
-      */
-    public java.util.Map<java.lang.CharSequence,org.apache.flink.formats.parquet.generated.MapItem> getNestedMap() {
-      return nestedMap;
-    }
-
-    /**
-      * Sets the value of the 'nestedMap' field.
-      * @param value The value of 'nestedMap'.
-      * @return This builder.
-      */
-    public org.apache.flink.formats.parquet.generated.NestedRecord.Builder setNestedMap(java.util.Map<java.lang.CharSequence,org.apache.flink.formats.parquet.generated.MapItem> value) {
-      validate(fields()[5], value);
-      this.nestedMap = value;
-      fieldSetFlags()[5] = true;
-      return this;
-    }
-
-    /**
-      * Checks whether the 'nestedMap' field has been set.
-      * @return True if the 'nestedMap' field has been set, false otherwise.
-      */
-    public boolean hasNestedMap() {
-      return fieldSetFlags()[5];
-    }
-
-
-    /**
-      * Clears the value of the 'nestedMap' field.
-      * @return This builder.
-      */
-    public org.apache.flink.formats.parquet.generated.NestedRecord.Builder clearNestedMap() {
-      nestedMap = null;
-      fieldSetFlags()[5] = false;
-      return this;
-    }
-
-    /**
-      * Gets the value of the 'nestedArray' field.
-      * @return The value.
-      */
-    public java.util.List<org.apache.flink.formats.parquet.generated.ArrayItem> getNestedArray() {
-      return nestedArray;
-    }
-
-    /**
-      * Sets the value of the 'nestedArray' field.
-      * @param value The value of 'nestedArray'.
-      * @return This builder.
-      */
-    public org.apache.flink.formats.parquet.generated.NestedRecord.Builder setNestedArray(java.util.List<org.apache.flink.formats.parquet.generated.ArrayItem> value) {
-      validate(fields()[6], value);
-      this.nestedArray = value;
-      fieldSetFlags()[6] = true;
-      return this;
-    }
-
-    /**
-      * Checks whether the 'nestedArray' field has been set.
-      * @return True if the 'nestedArray' field has been set, false otherwise.
-      */
-    public boolean hasNestedArray() {
-      return fieldSetFlags()[6];
-    }
-
-
-    /**
-      * Clears the value of the 'nestedArray' field.
-      * @return This builder.
-      */
-    public org.apache.flink.formats.parquet.generated.NestedRecord.Builder clearNestedArray() {
-      nestedArray = null;
-      fieldSetFlags()[6] = false;
-      return this;
-    }
-
-    @Override
-    @SuppressWarnings("unchecked")
-    public NestedRecord build() {
-      try {
-        NestedRecord record = new NestedRecord();
-        record.foo = fieldSetFlags()[0] ? this.foo : (java.lang.Long) defaultValue(fields()[0]);
-        record.spamMap = fieldSetFlags()[1] ? this.spamMap : (java.util.Map<java.lang.CharSequence,java.lang.CharSequence>) defaultValue(fields()[1]);
-        if (barBuilder != null) {
-          record.bar = this.barBuilder.build();
-        } else {
-          record.bar = fieldSetFlags()[2] ? this.bar : (org.apache.flink.formats.parquet.generated.Bar) defaultValue(fields()[2]);
-        }
-        record.arr = fieldSetFlags()[3] ? this.arr : (java.util.List<java.lang.Long>) defaultValue(fields()[3]);
-        record.strArray = fieldSetFlags()[4] ? this.strArray : (java.util.List<java.lang.CharSequence>) defaultValue(fields()[4]);
-        record.nestedMap = fieldSetFlags()[5] ? this.nestedMap : (java.util.Map<java.lang.CharSequence,org.apache.flink.formats.parquet.generated.MapItem>) defaultValue(fields()[5]);
-        record.nestedArray = fieldSetFlags()[6] ? this.nestedArray : (java.util.List<org.apache.flink.formats.parquet.generated.ArrayItem>) defaultValue(fields()[6]);
-        return record;
-      } catch (java.lang.Exception e) {
-        throw new org.apache.avro.AvroRuntimeException(e);
-      }
-    }
-  }
-
-  @SuppressWarnings("unchecked")
-  private static final org.apache.avro.io.DatumWriter<NestedRecord>
-    WRITER$ = (org.apache.avro.io.DatumWriter<NestedRecord>)MODEL$.createDatumWriter(SCHEMA$);
-
-  @Override public void writeExternal(java.io.ObjectOutput out)
-    throws java.io.IOException {
-    WRITER$.write(this, SpecificData.getEncoder(out));
-  }
-
-  @SuppressWarnings("unchecked")
-  private static final org.apache.avro.io.DatumReader<NestedRecord>
-    READER$ = (org.apache.avro.io.DatumReader<NestedRecord>)MODEL$.createDatumReader(SCHEMA$);
-
-  @Override public void readExternal(java.io.ObjectInput in)
-    throws java.io.IOException {
-    READER$.read(this, SpecificData.getDecoder(in));
-  }
-
-}
diff --git a/flink-formats/flink-parquet/src/test/java/org/apache/flink/formats/parquet/generated/SimpleRecord.java b/flink-formats/flink-parquet/src/test/java/org/apache/flink/formats/parquet/generated/SimpleRecord.java
deleted file mode 100644
index e0833a57e28c..000000000000
--- a/flink-formats/flink-parquet/src/test/java/org/apache/flink/formats/parquet/generated/SimpleRecord.java
+++ /dev/null
@@ -1,378 +0,0 @@
-/**
- * Autogenerated by Avro
- *
- * DO NOT EDIT DIRECTLY
- */
-package org.apache.flink.formats.parquet.generated;
-
-import org.apache.avro.specific.SpecificData;
-import org.apache.avro.message.BinaryMessageEncoder;
-import org.apache.avro.message.BinaryMessageDecoder;
-import org.apache.avro.message.SchemaStore;
-
-@SuppressWarnings("all")
-@org.apache.avro.specific.AvroGenerated
-public class SimpleRecord extends org.apache.avro.specific.SpecificRecordBase implements org.apache.avro.specific.SpecificRecord {
-  private static final long serialVersionUID = -6587201329523714120L;
-  public static final org.apache.avro.Schema SCHEMA$ = new org.apache.avro.Schema.Parser().parse("{\"type\":\"record\",\"name\":\"SimpleRecord\",\"namespace\":\"org.apache.flink.formats.parquet.generated\",\"fields\":[{\"name\":\"foo\",\"type\":[\"null\",\"long\"],\"default\":null},{\"name\":\"bar\",\"type\":[\"null\",\"string\"],\"default\":null},{\"name\":\"arr\",\"type\":[\"null\",{\"type\":\"array\",\"items\":\"long\"}],\"default\":null}],\"schema_id\":1}");
-  public static org.apache.avro.Schema getClassSchema() { return SCHEMA$; }
-
-  private static SpecificData MODEL$ = new SpecificData();
-
-  private static final BinaryMessageEncoder<SimpleRecord> ENCODER =
-      new BinaryMessageEncoder<SimpleRecord>(MODEL$, SCHEMA$);
-
-  private static final BinaryMessageDecoder<SimpleRecord> DECODER =
-      new BinaryMessageDecoder<SimpleRecord>(MODEL$, SCHEMA$);
-
-  /**
-   * Return the BinaryMessageDecoder instance used by this class.
-   */
-  public static BinaryMessageDecoder<SimpleRecord> getDecoder() {
-    return DECODER;
-  }
-
-  /**
-   * Create a new BinaryMessageDecoder instance for this class that uses the specified {@link SchemaStore}.
-   * @param resolver a {@link SchemaStore} used to find schemas by fingerprint
-   */
-  public static BinaryMessageDecoder<SimpleRecord> createDecoder(SchemaStore resolver) {
-    return new BinaryMessageDecoder<SimpleRecord>(MODEL$, SCHEMA$, resolver);
-  }
-
-  /** Serializes this SimpleRecord to a ByteBuffer. */
-  public java.nio.ByteBuffer toByteBuffer() throws java.io.IOException {
-    return ENCODER.encode(this);
-  }
-
-  /** Deserializes a SimpleRecord from a ByteBuffer. */
-  public static SimpleRecord fromByteBuffer(
-      java.nio.ByteBuffer b) throws java.io.IOException {
-    return DECODER.decode(b);
-  }
-
-  @Deprecated public java.lang.Long foo;
-  @Deprecated public java.lang.CharSequence bar;
-  @Deprecated public java.util.List<java.lang.Long> arr;
-
-  /**
-   * Default constructor.  Note that this does not initialize fields
-   * to their default values from the schema.  If that is desired then
-   * one should use <code>newBuilder()</code>.
-   */
-  public SimpleRecord() {}
-
-  /**
-   * All-args constructor.
-   * @param foo The new value for foo
-   * @param bar The new value for bar
-   * @param arr The new value for arr
-   */
-  public SimpleRecord(java.lang.Long foo, java.lang.CharSequence bar, java.util.List<java.lang.Long> arr) {
-    this.foo = foo;
-    this.bar = bar;
-    this.arr = arr;
-  }
-
-  public org.apache.avro.Schema getSchema() { return SCHEMA$; }
-  // Used by DatumWriter.  Applications should not call.
-  public java.lang.Object get(int field$) {
-    switch (field$) {
-    case 0: return foo;
-    case 1: return bar;
-    case 2: return arr;
-    default: throw new org.apache.avro.AvroRuntimeException("Bad index");
-    }
-  }
-
-  // Used by DatumReader.  Applications should not call.
-  @SuppressWarnings(value="unchecked")
-  public void put(int field$, java.lang.Object value$) {
-    switch (field$) {
-    case 0: foo = (java.lang.Long)value$; break;
-    case 1: bar = (java.lang.CharSequence)value$; break;
-    case 2: arr = (java.util.List<java.lang.Long>)value$; break;
-    default: throw new org.apache.avro.AvroRuntimeException("Bad index");
-    }
-  }
-
-  /**
-   * Gets the value of the 'foo' field.
-   * @return The value of the 'foo' field.
-   */
-  public java.lang.Long getFoo() {
-    return foo;
-  }
-
-  /**
-   * Sets the value of the 'foo' field.
-   * @param value the value to set.
-   */
-  public void setFoo(java.lang.Long value) {
-    this.foo = value;
-  }
-
-  /**
-   * Gets the value of the 'bar' field.
-   * @return The value of the 'bar' field.
-   */
-  public java.lang.CharSequence getBar() {
-    return bar;
-  }
-
-  /**
-   * Sets the value of the 'bar' field.
-   * @param value the value to set.
-   */
-  public void setBar(java.lang.CharSequence value) {
-    this.bar = value;
-  }
-
-  /**
-   * Gets the value of the 'arr' field.
-   * @return The value of the 'arr' field.
-   */
-  public java.util.List<java.lang.Long> getArr() {
-    return arr;
-  }
-
-  /**
-   * Sets the value of the 'arr' field.
-   * @param value the value to set.
-   */
-  public void setArr(java.util.List<java.lang.Long> value) {
-    this.arr = value;
-  }
-
-  /**
-   * Creates a new SimpleRecord RecordBuilder.
-   * @return A new SimpleRecord RecordBuilder
-   */
-  public static org.apache.flink.formats.parquet.generated.SimpleRecord.Builder newBuilder() {
-    return new org.apache.flink.formats.parquet.generated.SimpleRecord.Builder();
-  }
-
-  /**
-   * Creates a new SimpleRecord RecordBuilder by copying an existing Builder.
-   * @param other The existing builder to copy.
-   * @return A new SimpleRecord RecordBuilder
-   */
-  public static org.apache.flink.formats.parquet.generated.SimpleRecord.Builder newBuilder(org.apache.flink.formats.parquet.generated.SimpleRecord.Builder other) {
-    return new org.apache.flink.formats.parquet.generated.SimpleRecord.Builder(other);
-  }
-
-  /**
-   * Creates a new SimpleRecord RecordBuilder by copying an existing SimpleRecord instance.
-   * @param other The existing instance to copy.
-   * @return A new SimpleRecord RecordBuilder
-   */
-  public static org.apache.flink.formats.parquet.generated.SimpleRecord.Builder newBuilder(org.apache.flink.formats.parquet.generated.SimpleRecord other) {
-    return new org.apache.flink.formats.parquet.generated.SimpleRecord.Builder(other);
-  }
-
-  /**
-   * RecordBuilder for SimpleRecord instances.
-   */
-  public static class Builder extends org.apache.avro.specific.SpecificRecordBuilderBase<SimpleRecord>
-    implements org.apache.avro.data.RecordBuilder<SimpleRecord> {
-
-    private java.lang.Long foo;
-    private java.lang.CharSequence bar;
-    private java.util.List<java.lang.Long> arr;
-
-    /** Creates a new Builder */
-    private Builder() {
-      super(SCHEMA$);
-    }
-
-    /**
-     * Creates a Builder by copying an existing Builder.
-     * @param other The existing Builder to copy.
-     */
-    private Builder(org.apache.flink.formats.parquet.generated.SimpleRecord.Builder other) {
-      super(other);
-      if (isValidValue(fields()[0], other.foo)) {
-        this.foo = data().deepCopy(fields()[0].schema(), other.foo);
-        fieldSetFlags()[0] = true;
-      }
-      if (isValidValue(fields()[1], other.bar)) {
-        this.bar = data().deepCopy(fields()[1].schema(), other.bar);
-        fieldSetFlags()[1] = true;
-      }
-      if (isValidValue(fields()[2], other.arr)) {
-        this.arr = data().deepCopy(fields()[2].schema(), other.arr);
-        fieldSetFlags()[2] = true;
-      }
-    }
-
-    /**
-     * Creates a Builder by copying an existing SimpleRecord instance
-     * @param other The existing instance to copy.
-     */
-    private Builder(org.apache.flink.formats.parquet.generated.SimpleRecord other) {
-            super(SCHEMA$);
-      if (isValidValue(fields()[0], other.foo)) {
-        this.foo = data().deepCopy(fields()[0].schema(), other.foo);
-        fieldSetFlags()[0] = true;
-      }
-      if (isValidValue(fields()[1], other.bar)) {
-        this.bar = data().deepCopy(fields()[1].schema(), other.bar);
-        fieldSetFlags()[1] = true;
-      }
-      if (isValidValue(fields()[2], other.arr)) {
-        this.arr = data().deepCopy(fields()[2].schema(), other.arr);
-        fieldSetFlags()[2] = true;
-      }
-    }
-
-    /**
-      * Gets the value of the 'foo' field.
-      * @return The value.
-      */
-    public java.lang.Long getFoo() {
-      return foo;
-    }
-
-    /**
-      * Sets the value of the 'foo' field.
-      * @param value The value of 'foo'.
-      * @return This builder.
-      */
-    public org.apache.flink.formats.parquet.generated.SimpleRecord.Builder setFoo(java.lang.Long value) {
-      validate(fields()[0], value);
-      this.foo = value;
-      fieldSetFlags()[0] = true;
-      return this;
-    }
-
-    /**
-      * Checks whether the 'foo' field has been set.
-      * @return True if the 'foo' field has been set, false otherwise.
-      */
-    public boolean hasFoo() {
-      return fieldSetFlags()[0];
-    }
-
-
-    /**
-      * Clears the value of the 'foo' field.
-      * @return This builder.
-      */
-    public org.apache.flink.formats.parquet.generated.SimpleRecord.Builder clearFoo() {
-      foo = null;
-      fieldSetFlags()[0] = false;
-      return this;
-    }
-
-    /**
-      * Gets the value of the 'bar' field.
-      * @return The value.
-      */
-    public java.lang.CharSequence getBar() {
-      return bar;
-    }
-
-    /**
-      * Sets the value of the 'bar' field.
-      * @param value The value of 'bar'.
-      * @return This builder.
-      */
-    public org.apache.flink.formats.parquet.generated.SimpleRecord.Builder setBar(java.lang.CharSequence value) {
-      validate(fields()[1], value);
-      this.bar = value;
-      fieldSetFlags()[1] = true;
-      return this;
-    }
-
-    /**
-      * Checks whether the 'bar' field has been set.
-      * @return True if the 'bar' field has been set, false otherwise.
-      */
-    public boolean hasBar() {
-      return fieldSetFlags()[1];
-    }
-
-
-    /**
-      * Clears the value of the 'bar' field.
-      * @return This builder.
-      */
-    public org.apache.flink.formats.parquet.generated.SimpleRecord.Builder clearBar() {
-      bar = null;
-      fieldSetFlags()[1] = false;
-      return this;
-    }
-
-    /**
-      * Gets the value of the 'arr' field.
-      * @return The value.
-      */
-    public java.util.List<java.lang.Long> getArr() {
-      return arr;
-    }
-
-    /**
-      * Sets the value of the 'arr' field.
-      * @param value The value of 'arr'.
-      * @return This builder.
-      */
-    public org.apache.flink.formats.parquet.generated.SimpleRecord.Builder setArr(java.util.List<java.lang.Long> value) {
-      validate(fields()[2], value);
-      this.arr = value;
-      fieldSetFlags()[2] = true;
-      return this;
-    }
-
-    /**
-      * Checks whether the 'arr' field has been set.
-      * @return True if the 'arr' field has been set, false otherwise.
-      */
-    public boolean hasArr() {
-      return fieldSetFlags()[2];
-    }
-
-
-    /**
-      * Clears the value of the 'arr' field.
-      * @return This builder.
-      */
-    public org.apache.flink.formats.parquet.generated.SimpleRecord.Builder clearArr() {
-      arr = null;
-      fieldSetFlags()[2] = false;
-      return this;
-    }
-
-    @Override
-    @SuppressWarnings("unchecked")
-    public SimpleRecord build() {
-      try {
-        SimpleRecord record = new SimpleRecord();
-        record.foo = fieldSetFlags()[0] ? this.foo : (java.lang.Long) defaultValue(fields()[0]);
-        record.bar = fieldSetFlags()[1] ? this.bar : (java.lang.CharSequence) defaultValue(fields()[1]);
-        record.arr = fieldSetFlags()[2] ? this.arr : (java.util.List<java.lang.Long>) defaultValue(fields()[2]);
-        return record;
-      } catch (java.lang.Exception e) {
-        throw new org.apache.avro.AvroRuntimeException(e);
-      }
-    }
-  }
-
-  @SuppressWarnings("unchecked")
-  private static final org.apache.avro.io.DatumWriter<SimpleRecord>
-    WRITER$ = (org.apache.avro.io.DatumWriter<SimpleRecord>)MODEL$.createDatumWriter(SCHEMA$);
-
-  @Override public void writeExternal(java.io.ObjectOutput out)
-    throws java.io.IOException {
-    WRITER$.write(this, SpecificData.getEncoder(out));
-  }
-
-  @SuppressWarnings("unchecked")
-  private static final org.apache.avro.io.DatumReader<SimpleRecord>
-    READER$ = (org.apache.avro.io.DatumReader<SimpleRecord>)MODEL$.createDatumReader(SCHEMA$);
-
-  @Override public void readExternal(java.io.ObjectInput in)
-    throws java.io.IOException {
-    READER$.read(this, SpecificData.getDecoder(in));
-  }
-
-}
diff --git a/flink-runtime/src/main/java/org/apache/flink/runtime/jobmaster/slotpool/SlotPool.java b/flink-runtime/src/main/java/org/apache/flink/runtime/jobmaster/slotpool/SlotPool.java
index 4839de36d0ac..b3fb36cabc49 100644
--- a/flink-runtime/src/main/java/org/apache/flink/runtime/jobmaster/slotpool/SlotPool.java
+++ b/flink-runtime/src/main/java/org/apache/flink/runtime/jobmaster/slotpool/SlotPool.java
@@ -44,7 +44,6 @@
 import org.apache.flink.runtime.messages.Acknowledge;
 import org.apache.flink.runtime.resourcemanager.ResourceManagerGateway;
 import org.apache.flink.runtime.resourcemanager.SlotRequest;
-import org.apache.flink.runtime.resourcemanager.exceptions.MaximumFailedContainersException;
 import org.apache.flink.runtime.rpc.RpcEndpoint;
 import org.apache.flink.runtime.rpc.RpcService;
 import org.apache.flink.runtime.taskexecutor.slot.SlotOffer;
@@ -689,8 +688,6 @@ public void disconnectResourceManager() {
 				(AllocatedSlot ignored, Throwable throwable) -> {
 					if (throwable instanceof TimeoutException) {
 						timeoutPendingSlotRequest(slotRequestId);
-					} else if (throwable instanceof MaximumFailedContainersException) {
-						rejectPendingSlotRequest(slotRequestId);
 					}
 				},
 				getMainThreadExecutor());
@@ -1126,12 +1123,6 @@ protected void timeoutPendingSlotRequest(SlotRequestId slotRequestId) {
 		removePendingRequest(slotRequestId);
 	}
 
-	@VisibleForTesting
-	protected void rejectPendingSlotRequest(SlotRequestId slotRequestId) {
-		log.info("Pending slot request [{}] is rejected by resource manager.", slotRequestId);
-		removePendingRequest(slotRequestId);
-	}
-
 	private void releaseTaskManagerInternal(final ResourceID resourceId, final Exception cause) {
 		final Set<AllocatedSlot> removedSlots = new HashSet<>(allocatedSlots.removeSlotsForTaskManager(resourceId));
 

From 06baf94654ceb90cf0a0e0d0ab36a1c3bfdca255 Mon Sep 17 00:00:00 2001
From: Peter Huang <huangzhenqiu0825@gmail.com>
Date: Thu, 3 Jan 2019 22:56:49 -0800
Subject: [PATCH 4/8] reject pending request when threshold is hit in
 closeTaskManagerConnection

---
 .../flink/runtime/resourcemanager/ResourceManager.java   | 9 +++++++++
 .../java/org/apache/flink/yarn/YarnResourceManager.java  | 3 ---
 .../org/apache/flink/yarn/YarnResourceManagerTest.java   | 2 +-
 3 files changed, 10 insertions(+), 4 deletions(-)

diff --git a/flink-runtime/src/main/java/org/apache/flink/runtime/resourcemanager/ResourceManager.java b/flink-runtime/src/main/java/org/apache/flink/runtime/resourcemanager/ResourceManager.java
index 58a4c785d4d8..1dc7c8584261 100644
--- a/flink-runtime/src/main/java/org/apache/flink/runtime/resourcemanager/ResourceManager.java
+++ b/flink-runtime/src/main/java/org/apache/flink/runtime/resourcemanager/ResourceManager.java
@@ -50,6 +50,7 @@
 import org.apache.flink.runtime.metrics.MetricRegistry;
 import org.apache.flink.runtime.metrics.groups.JobManagerMetricGroup;
 import org.apache.flink.runtime.registration.RegistrationResponse;
+import org.apache.flink.runtime.resourcemanager.exceptions.MaximumFailedContainersException;
 import org.apache.flink.runtime.resourcemanager.exceptions.ResourceManagerException;
 import org.apache.flink.runtime.resourcemanager.exceptions.UnknownTaskExecutorException;
 import org.apache.flink.runtime.resourcemanager.registration.JobManagerRegistration;
@@ -149,6 +150,9 @@
 	/** The number of failed containers since the master became active. */
 	protected AtomicInteger failedContainerSoFar = new AtomicInteger(0);
 
+	/** Number of failed TaskManager containers before stopping the application. Default is  Integer.MAX_VALUE */
+	protected int maxFailedContainers = Integer.MAX_VALUE;
+
 	/**
 	 * Represents asynchronous state clearing work.
 	 *
@@ -834,6 +838,11 @@ protected void closeTaskManagerConnection(final ResourceID resourceID, final Exc
 
 			workerRegistration.getTaskExecutorGateway().disconnectResourceManager(cause);
 			failedContainerSoFar.getAndAdd(1);
+			if (failedContainerSoFar.intValue() >= maxFailedContainers) {
+				rejectAllPendingSlotRequests(new MaximumFailedContainersException(
+					String.format("Maximum number of failed container %d "
+						+ "is detected in Resource Manager", failedContainerSoFar.intValue())));
+			}
 		} else {
 			log.debug(
 				"No open TaskExecutor connection {}. Ignoring close TaskExecutor connection. Closing reason was: {}",
diff --git a/flink-yarn/src/main/java/org/apache/flink/yarn/YarnResourceManager.java b/flink-yarn/src/main/java/org/apache/flink/yarn/YarnResourceManager.java
index 9de281a0b2b8..ade2d50bcee0 100644
--- a/flink-yarn/src/main/java/org/apache/flink/yarn/YarnResourceManager.java
+++ b/flink-yarn/src/main/java/org/apache/flink/yarn/YarnResourceManager.java
@@ -118,9 +118,6 @@
 
 	private final int defaultCpus;
 
-	/** Number of failed TaskManager containers before stopping the application. -1 means infinite. */
-	private final int maxFailedContainers;
-
 	/** Client to communicate with the Resource Manager (YARN's master). */
 	private AMRMClientAsync<AMRMClient.ContainerRequest> resourceManagerClient;
 
diff --git a/flink-yarn/src/test/java/org/apache/flink/yarn/YarnResourceManagerTest.java b/flink-yarn/src/test/java/org/apache/flink/yarn/YarnResourceManagerTest.java
index 4997699e5362..bda06aff73db 100644
--- a/flink-yarn/src/test/java/org/apache/flink/yarn/YarnResourceManagerTest.java
+++ b/flink-yarn/src/test/java/org/apache/flink/yarn/YarnResourceManagerTest.java
@@ -528,7 +528,7 @@ public void testOnContainerCompleted() throws Exception {
 	}
 
 	/**
-	 * 	Tests that YarnResourceManager will trigger to cancel all pending slot request, when maximum number of failed
+	 * 	Tests that YarnResourceManager will trigger to reject all pending slot request, when maximum number of failed
 	 * 	contains is hit.
 	 */
 	@Test

From c8ef4335e194753c98c07977f1a859b9aae6b09e Mon Sep 17 00:00:00 2001
From: Peter Huang <huangzhenqiu0825@gmail.com>
Date: Thu, 10 Jan 2019 10:24:34 -0800
Subject: [PATCH 5/8] block new slot request when maximum failed task manager
 exceeding

---
 .../runtime/resourcemanager/ResourceManager.java | 16 +++++++++++-----
 ...imumFailedTaskManagerExceedingException.java} |  8 ++++----
 .../apache/flink/yarn/YarnResourceManager.java   | 10 ++++++----
 .../flink/yarn/YarnResourceManagerTest.java      |  4 ++--
 4 files changed, 23 insertions(+), 15 deletions(-)
 rename flink-runtime/src/main/java/org/apache/flink/runtime/resourcemanager/exceptions/{MaximumFailedContainersException.java => MaximumFailedTaskManagerExceedingException.java} (75%)

diff --git a/flink-runtime/src/main/java/org/apache/flink/runtime/resourcemanager/ResourceManager.java b/flink-runtime/src/main/java/org/apache/flink/runtime/resourcemanager/ResourceManager.java
index 1dc7c8584261..1444322ca6d2 100644
--- a/flink-runtime/src/main/java/org/apache/flink/runtime/resourcemanager/ResourceManager.java
+++ b/flink-runtime/src/main/java/org/apache/flink/runtime/resourcemanager/ResourceManager.java
@@ -50,7 +50,7 @@
 import org.apache.flink.runtime.metrics.MetricRegistry;
 import org.apache.flink.runtime.metrics.groups.JobManagerMetricGroup;
 import org.apache.flink.runtime.registration.RegistrationResponse;
-import org.apache.flink.runtime.resourcemanager.exceptions.MaximumFailedContainersException;
+import org.apache.flink.runtime.resourcemanager.exceptions.MaximumFailedTaskManagerExceedingException;
 import org.apache.flink.runtime.resourcemanager.exceptions.ResourceManagerException;
 import org.apache.flink.runtime.resourcemanager.exceptions.UnknownTaskExecutorException;
 import org.apache.flink.runtime.resourcemanager.registration.JobManagerRegistration;
@@ -150,8 +150,8 @@
 	/** The number of failed containers since the master became active. */
 	protected AtomicInteger failedContainerSoFar = new AtomicInteger(0);
 
-	/** Number of failed TaskManager containers before stopping the application. Default is  Integer.MAX_VALUE */
-	protected int maxFailedContainers = Integer.MAX_VALUE;
+	/** Number of failed TaskManager containers before stopping the application. Default is Integer.MAX_VALUE */
+	protected int maximumAllowedTaskManagerFailureCount = Integer.MAX_VALUE;
 
 	/**
 	 * Represents asynchronous state clearing work.
@@ -429,6 +429,12 @@ public void disconnectJobManager(final JobID jobId, final Exception cause) {
 					slotRequest.getJobId(),
 					slotRequest.getAllocationId());
 
+				if (failedContainerSoFar.intValue() >= maximumAllowedTaskManagerFailureCount) {
+					return FutureUtils.completedExceptionally(new MaximumFailedTaskManagerExceedingException(
+						String.format("Maximum number of failed container %d "
+							+ "is detected in Resource Manager", failedContainerSoFar.intValue())));
+				}
+
 				try {
 					slotManager.registerSlotRequest(slotRequest);
 				} catch (SlotManagerException e) {
@@ -838,8 +844,8 @@ protected void closeTaskManagerConnection(final ResourceID resourceID, final Exc
 
 			workerRegistration.getTaskExecutorGateway().disconnectResourceManager(cause);
 			failedContainerSoFar.getAndAdd(1);
-			if (failedContainerSoFar.intValue() >= maxFailedContainers) {
-				rejectAllPendingSlotRequests(new MaximumFailedContainersException(
+			if (failedContainerSoFar.intValue() >= maximumAllowedTaskManagerFailureCount) {
+				rejectAllPendingSlotRequests(new MaximumFailedTaskManagerExceedingException(
 					String.format("Maximum number of failed container %d "
 						+ "is detected in Resource Manager", failedContainerSoFar.intValue())));
 			}
diff --git a/flink-runtime/src/main/java/org/apache/flink/runtime/resourcemanager/exceptions/MaximumFailedContainersException.java b/flink-runtime/src/main/java/org/apache/flink/runtime/resourcemanager/exceptions/MaximumFailedTaskManagerExceedingException.java
similarity index 75%
rename from flink-runtime/src/main/java/org/apache/flink/runtime/resourcemanager/exceptions/MaximumFailedContainersException.java
rename to flink-runtime/src/main/java/org/apache/flink/runtime/resourcemanager/exceptions/MaximumFailedTaskManagerExceedingException.java
index 12b65fbf54f4..b949d9b47d01 100644
--- a/flink-runtime/src/main/java/org/apache/flink/runtime/resourcemanager/exceptions/MaximumFailedContainersException.java
+++ b/flink-runtime/src/main/java/org/apache/flink/runtime/resourcemanager/exceptions/MaximumFailedTaskManagerExceedingException.java
@@ -23,13 +23,13 @@
 /**
  * Exception for {@link ResourceManager} when it identified that the maximum number of failed containers is hit.
  */
-public class MaximumFailedContainersException extends ResourceManagerException {
+public class MaximumFailedTaskManagerExceedingException extends ResourceManagerException {
 	private static final long serialVersionUID = -2333228226519195160L;
 
 
-	public MaximumFailedContainersException(String message) { super(message); }
+	public MaximumFailedTaskManagerExceedingException(String message) { super(message); }
 
-	public MaximumFailedContainersException(String message, Throwable cause) { super(message, cause); }
+	public MaximumFailedTaskManagerExceedingException(String message, Throwable cause) { super(message, cause); }
 
-	public MaximumFailedContainersException(Throwable cause) { super(cause); }
+	public MaximumFailedTaskManagerExceedingException(Throwable cause) { super(cause); }
 }
diff --git a/flink-yarn/src/main/java/org/apache/flink/yarn/YarnResourceManager.java b/flink-yarn/src/main/java/org/apache/flink/yarn/YarnResourceManager.java
index ade2d50bcee0..495dee890172 100644
--- a/flink-yarn/src/main/java/org/apache/flink/yarn/YarnResourceManager.java
+++ b/flink-yarn/src/main/java/org/apache/flink/yarn/YarnResourceManager.java
@@ -36,7 +36,7 @@
 import org.apache.flink.runtime.metrics.groups.JobManagerMetricGroup;
 import org.apache.flink.runtime.resourcemanager.JobLeaderIdService;
 import org.apache.flink.runtime.resourcemanager.ResourceManager;
-import org.apache.flink.runtime.resourcemanager.exceptions.MaximumFailedContainersException;
+import org.apache.flink.runtime.resourcemanager.exceptions.MaximumFailedTaskManagerExceedingException;
 import org.apache.flink.runtime.resourcemanager.exceptions.ResourceManagerException;
 import org.apache.flink.runtime.resourcemanager.slotmanager.SlotManager;
 import org.apache.flink.runtime.rpc.FatalErrorHandler;
@@ -179,7 +179,8 @@ public YarnResourceManager(
 
 		final int numInitialTM = Integer.parseInt(env.getOrDefault(
 			YarnConfigKeys.ENV_TM_COUNT, DEFAULT_INITIAL_NUM_TASK_MANAGER));
-		this.maxFailedContainers = flinkConfig.getInteger(YarnConfigOptions.MAX_FAILED_CONTAINERS.key(), numInitialTM);
+		this.maximumAllowedTaskManagerFailureCount =
+			flinkConfig.getInteger(YarnConfigOptions.MAX_FAILED_CONTAINERS.key(), numInitialTM);
 
 		yarnHeartbeatIntervalMillis = yarnHeartbeatIntervalMS;
 		numPendingContainerRequests = 0;
@@ -409,12 +410,13 @@ public void onContainersAllocated(List<Container> containers) {
 						// release the failed container
 						workerNodeMap.remove(resourceId);
 						resourceManagerClient.releaseAssignedContainer(container.getId());
-						if (failedContainerSoFar.intValue() < maxFailedContainers) {
+
+						if (failedContainerSoFar.intValue() < maximumAllowedTaskManagerFailureCount) {
 							// and ask for a new one
 							requestYarnContainerIfRequired();
 						} else {
 							log.error("Could not start TaskManager in container {}.", container.getId(), t);
-							rejectAllPendingSlotRequests(new MaximumFailedContainersException(
+							rejectAllPendingSlotRequests(new MaximumFailedTaskManagerExceedingException(
 								String.format("Maximum number of failed container %d "
 										+ "is detected in Resource Manager", failedContainerSoFar.intValue())));
 						}
diff --git a/flink-yarn/src/test/java/org/apache/flink/yarn/YarnResourceManagerTest.java b/flink-yarn/src/test/java/org/apache/flink/yarn/YarnResourceManagerTest.java
index bda06aff73db..9d892c3f84c4 100644
--- a/flink-yarn/src/test/java/org/apache/flink/yarn/YarnResourceManagerTest.java
+++ b/flink-yarn/src/test/java/org/apache/flink/yarn/YarnResourceManagerTest.java
@@ -45,7 +45,7 @@
 import org.apache.flink.runtime.resourcemanager.JobLeaderIdService;
 import org.apache.flink.runtime.resourcemanager.ResourceManagerGateway;
 import org.apache.flink.runtime.resourcemanager.SlotRequest;
-import org.apache.flink.runtime.resourcemanager.exceptions.MaximumFailedContainersException;
+import org.apache.flink.runtime.resourcemanager.exceptions.MaximumFailedTaskManagerExceedingException;
 import org.apache.flink.runtime.resourcemanager.slotmanager.SlotManager;
 import org.apache.flink.runtime.rpc.FatalErrorHandler;
 import org.apache.flink.runtime.rpc.RpcService;
@@ -575,7 +575,7 @@ public void testOnContainersAllocatedWithFailure() throws Exception {
 				when(mockNMClient.startContainer(eq(failedContainer), any())).thenThrow(new YarnException("Failed"));
 				resourceManager.onContainersAllocated(ImmutableList.of(failedContainer));
 				verify(rmServices.slotManager, times(1))
-					.rejectAllPendingSlotRequests(any(MaximumFailedContainersException.class));
+					.rejectAllPendingSlotRequests(any(MaximumFailedTaskManagerExceedingException.class));
 			});
 		}};
 	}

From 02c9a653e74a100202795cdd44db283d4dd783de Mon Sep 17 00:00:00 2001
From: Peter Huang <huangzhenqiu0825@gmail.com>
Date: Sun, 27 Jan 2019 18:46:33 -0800
Subject: [PATCH 6/8] use failure rate

---
 .../mesos/configuration/MesosOptions.java     |  19 ++++
 .../MesosResourceManager.java                 |  22 +++-
 .../MesosResourceManagerFactory.java          |  11 +-
 .../MesosResourceManagerTest.java             |  62 +++++++++-
 .../execution/SuppressRestartsException.java  |   5 +-
 .../resourcemanager/ResourceManager.java      | 106 +++++++++++++++---
 ...umFailedTaskManagerExceedingException.java |   8 +-
 .../slotmanager/SlotManager.java              |  19 ++--
 .../flink/yarn/YarnResourceManager.java       |  31 +++--
 .../yarn/configuration/YarnConfigOptions.java |  23 +++-
 .../YarnResourceManagerFactory.java           |  11 +-
 .../flink/yarn/YarnResourceManagerTest.java   |  28 +++--
 12 files changed, 278 insertions(+), 67 deletions(-)

diff --git a/flink-mesos/src/main/java/org/apache/flink/mesos/configuration/MesosOptions.java b/flink-mesos/src/main/java/org/apache/flink/mesos/configuration/MesosOptions.java
index 0c4e1f6bcba4..003981c2320e 100644
--- a/flink-mesos/src/main/java/org/apache/flink/mesos/configuration/MesosOptions.java
+++ b/flink-mesos/src/main/java/org/apache/flink/mesos/configuration/MesosOptions.java
@@ -99,6 +99,25 @@
 			.withDescription("The config parameter defining the Mesos artifact server port to use. Setting the port to" +
 				" 0 will let the OS choose an available port.");
 
+	/**
+	 * The maximum number of failed Mesos worker within an interval before entirely stopping
+	 * the Mesos session / job on Mesos.
+	 * By default, the value is -1
+	 */
+	public static final ConfigOption<Integer> MAX_FAILED_WORKERS_PER_INTERVAL =
+		key("mesos.maximum-failed-workers-per-interval")
+			.defaultValue(-1)
+			.withDescription("Maximum number of workers the system is going to reallocate in case of a failure in an interval.");
+
+	/**
+	 * The interval for measuring failure rate of containers in second unit.
+	 * By default, the value is 5 minutes.
+	 **/
+	public static final ConfigOption<Integer> WORKERS_FAILURE_RATE_INTERVAL =
+		key("mesos.workers-failure-rate-interval")
+			.defaultValue(300)
+			.withDeprecatedKeys("The interval for measuring failure rate of workers");
+
 	public static final ConfigOption<String> RESOURCEMANAGER_FRAMEWORK_NAME =
 		key("mesos.resourcemanager.framework.name")
 			.defaultValue("Flink")
diff --git a/flink-mesos/src/main/java/org/apache/flink/mesos/runtime/clusterframework/MesosResourceManager.java b/flink-mesos/src/main/java/org/apache/flink/mesos/runtime/clusterframework/MesosResourceManager.java
index ae51a2027e89..74c1a6222fda 100644
--- a/flink-mesos/src/main/java/org/apache/flink/mesos/runtime/clusterframework/MesosResourceManager.java
+++ b/flink-mesos/src/main/java/org/apache/flink/mesos/runtime/clusterframework/MesosResourceManager.java
@@ -18,6 +18,7 @@
 
 package org.apache.flink.mesos.runtime.clusterframework;
 
+import org.apache.flink.api.common.time.Time;
 import org.apache.flink.api.java.tuple.Tuple2;
 import org.apache.flink.configuration.Configuration;
 import org.apache.flink.mesos.runtime.clusterframework.services.MesosServices;
@@ -53,6 +54,7 @@
 import org.apache.flink.runtime.metrics.groups.JobManagerMetricGroup;
 import org.apache.flink.runtime.resourcemanager.JobLeaderIdService;
 import org.apache.flink.runtime.resourcemanager.ResourceManager;
+import org.apache.flink.runtime.resourcemanager.exceptions.MaximumFailedTaskManagerExceedingException;
 import org.apache.flink.runtime.resourcemanager.exceptions.ResourceManagerException;
 import org.apache.flink.runtime.resourcemanager.slotmanager.SlotManager;
 import org.apache.flink.runtime.rpc.FatalErrorHandler;
@@ -167,7 +169,9 @@ public MesosResourceManager(
 			MesosTaskManagerParameters taskManagerParameters,
 			ContainerSpecification taskManagerContainerSpec,
 			@Nullable String webUiUrl,
-			JobManagerMetricGroup jobManagerMetricGroup) {
+			JobManagerMetricGroup jobManagerMetricGroup,
+			Time failureInterval,
+			int maxFailurePerInterval) {
 		super(
 			rpcService,
 			resourceManagerEndpointId,
@@ -179,7 +183,9 @@ public MesosResourceManager(
 			jobLeaderIdService,
 			clusterInformation,
 			fatalErrorHandler,
-			jobManagerMetricGroup);
+			jobManagerMetricGroup,
+			failureInterval,
+			maxFailurePerInterval);
 
 		this.mesosServices = Preconditions.checkNotNull(mesosServices);
 		this.actorSystem = Preconditions.checkNotNull(mesosServices.getLocalActorSystem());
@@ -663,7 +669,17 @@ public void taskTerminated(TaskMonitor.TaskTerminated message) {
 			assert(launched != null);
 			LOG.info("Worker {} failed with status: {}, reason: {}, message: {}.",
 				id, status.getState(), status.getReason(), status.getMessage());
-			startNewWorker(launched.profile());
+
+			recordFailure();
+
+			if (shouldRejectRequests()) {
+				rejectAllPendingSlotRequests(new MaximumFailedTaskManagerExceedingException(
+					new RuntimeException(String.format("Maximum number of failed workers %d in interval %s"
+							+ "is detected in Resource Manager", maximumFailureTaskExecutorPerInternal,
+						failureInterval.toString()))));
+			} else {
+				startNewWorker(launched.profile());
+			}
 		}
 
 		closeTaskManagerConnection(id, new Exception(status.getMessage()));
diff --git a/flink-mesos/src/main/java/org/apache/flink/mesos/runtime/clusterframework/MesosResourceManagerFactory.java b/flink-mesos/src/main/java/org/apache/flink/mesos/runtime/clusterframework/MesosResourceManagerFactory.java
index f53ffb301a85..a222e7552b52 100644
--- a/flink-mesos/src/main/java/org/apache/flink/mesos/runtime/clusterframework/MesosResourceManagerFactory.java
+++ b/flink-mesos/src/main/java/org/apache/flink/mesos/runtime/clusterframework/MesosResourceManagerFactory.java
@@ -18,7 +18,9 @@
 
 package org.apache.flink.mesos.runtime.clusterframework;
 
+import org.apache.flink.api.common.time.Time;
 import org.apache.flink.configuration.Configuration;
+import org.apache.flink.mesos.configuration.MesosOptions;
 import org.apache.flink.mesos.runtime.clusterframework.services.MesosServices;
 import org.apache.flink.mesos.util.MesosConfiguration;
 import org.apache.flink.runtime.clusterframework.ContainerSpecification;
@@ -38,6 +40,8 @@
 import javax.annotation.Nonnull;
 import javax.annotation.Nullable;
 
+import java.util.concurrent.TimeUnit;
+
 /**
  * {@link ResourceManagerFactory} which creates a {@link MesosResourceManager}.
  */
@@ -80,6 +84,9 @@ public MesosResourceManagerFactory(@Nonnull MesosServices mesosServices, @Nonnul
 			highAvailabilityServices,
 			rpcService.getScheduledExecutor());
 
+		int maxFailurePerInternal = configuration.getInteger(MesosOptions.MAX_FAILED_WORKERS_PER_INTERVAL);
+		long failureInterval = configuration.getInteger(MesosOptions.WORKERS_FAILURE_RATE_INTERVAL);
+
 		return new MesosResourceManager(
 			rpcService,
 			ResourceManager.RESOURCE_MANAGER_NAME,
@@ -97,6 +104,8 @@ public MesosResourceManagerFactory(@Nonnull MesosServices mesosServices, @Nonnul
 			taskManagerParameters,
 			taskManagerContainerSpec,
 			webInterfaceUrl,
-			jobManagerMetricGroup);
+			jobManagerMetricGroup,
+			Time.of(failureInterval, TimeUnit.SECONDS),
+			maxFailurePerInternal);
 	}
 }
diff --git a/flink-mesos/src/test/java/org/apache/flink/mesos/runtime/clusterframework/MesosResourceManagerTest.java b/flink-mesos/src/test/java/org/apache/flink/mesos/runtime/clusterframework/MesosResourceManagerTest.java
index c5d053c13141..56721063d50f 100644
--- a/flink-mesos/src/test/java/org/apache/flink/mesos/runtime/clusterframework/MesosResourceManagerTest.java
+++ b/flink-mesos/src/test/java/org/apache/flink/mesos/runtime/clusterframework/MesosResourceManagerTest.java
@@ -40,6 +40,7 @@
 import org.apache.flink.runtime.clusterframework.ApplicationStatus;
 import org.apache.flink.runtime.clusterframework.ContainerSpecification;
 import org.apache.flink.runtime.clusterframework.ContaineredTaskManagerParameters;
+import org.apache.flink.runtime.clusterframework.types.AllocationID;
 import org.apache.flink.runtime.clusterframework.types.ResourceID;
 import org.apache.flink.runtime.clusterframework.types.ResourceProfile;
 import org.apache.flink.runtime.concurrent.ScheduledExecutor;
@@ -114,6 +115,7 @@
 import static org.hamcrest.Matchers.hasItem;
 import static org.hamcrest.Matchers.hasKey;
 import static org.hamcrest.Matchers.hasSize;
+import static org.junit.Assert.assertEquals;
 import static org.junit.Assert.assertThat;
 import static org.junit.Assert.assertTrue;
 import static org.mockito.Matchers.any;
@@ -175,7 +177,9 @@ public TestingMesosResourceManager(
 			MesosConfiguration mesosConfig,
 			MesosTaskManagerParameters taskManagerParameters,
 			ContainerSpecification taskManagerContainerSpec,
-			JobManagerMetricGroup jobManagerMetricGroup) {
+			JobManagerMetricGroup jobManagerMetricGroup,
+			Time failureInterval,
+			int maxFailurePerInterval) {
 			super(
 				rpcService,
 				resourceManagerEndpointId,
@@ -193,7 +197,13 @@ public TestingMesosResourceManager(
 				taskManagerParameters,
 				taskManagerContainerSpec,
 				null,
-				jobManagerMetricGroup);
+				jobManagerMetricGroup,
+				failureInterval,
+				maxFailurePerInterval);
+		}
+
+		<T> CompletableFuture<T> runInMainThread(Callable<T> callable) {
+			return callAsync(callable, timeout);
 		}
 
 		@Override
@@ -303,7 +313,9 @@ protected void closeTaskManagerConnection(ResourceID resourceID, Exception cause
 					rmServices.mesosConfig,
 					tmParams,
 					containerSpecification,
-					UnregisteredMetricGroups.createUnregisteredJobManagerMetricGroup());
+					UnregisteredMetricGroups.createUnregisteredJobManagerMetricGroup(),
+					Time.of(300, TimeUnit.SECONDS),
+					2);
 
 			// TaskExecutors
 			task1Executor = mockTaskExecutor(task1);
@@ -708,6 +720,50 @@ public void testWorkerFailed() throws Exception {
 		}};
 	}
 
+	/**
+	 * Test worker failure hit maximum worker failure rate.
+	 */
+	@Test
+	public void testWorkerFailedAtFailureRate() throws Exception {
+		new Context() {{
+			// set the initial persistent state with a launched worker
+			MesosWorkerStore.Worker worker1launched = MesosWorkerStore.Worker.newWorker(task1).launchWorker(slave1, slave1host);
+			MesosWorkerStore.Worker worker2launched = MesosWorkerStore.Worker.newWorker(task2).launchWorker(slave1, slave1host);
+
+			when(rmServices.workerStore.getFrameworkID()).thenReturn(Option.apply(framework1));
+			when(rmServices.workerStore.recoverWorkers()).thenReturn(Arrays.asList(worker1launched, worker2launched));
+			when(rmServices.workerStore.newTaskID()).thenReturn(task3);
+			startResourceManager();
+
+			// tell the RM that a tasks failed
+			when(rmServices.workerStore.removeWorker(task1)).thenReturn(true);
+			when(rmServices.workerStore.removeWorker(task2)).thenReturn(true);
+			resourceManager.taskTerminated(new TaskMonitor.TaskTerminated(task1, Protos.TaskStatus.newBuilder()
+				.setTaskId(task1).setSlaveId(slave1).setState(Protos.TaskState.TASK_FAILED).build()));
+
+			// tell the RM that a task failed
+			resourceManager.taskTerminated(new TaskMonitor.TaskTerminated(task2, Protos.TaskStatus.newBuilder()
+				.setTaskId(task2).setSlaveId(slave1).setState(Protos.TaskState.TASK_FAILED).build()));
+
+			verify(rmServices.workerStore).removeWorker(task1);
+			verify(rmServices.workerStore).removeWorker(task2);
+			assertThat(resourceManager.workersInLaunch.entrySet(), empty());
+			assertThat(resourceManager.workersBeingReturned.entrySet(), empty());
+			assertThat(resourceManager.workersInNew, hasKey(extractResourceID(task3)));
+
+			// request second slot
+			CompletableFuture<?> registerSlotRequestFuture = resourceManager.runInMainThread(() -> {
+				rmServices.slotManager.registerSlotRequest(
+					new SlotRequest(new JobID(), new AllocationID(), resourceProfile1, slave1host));
+				return null;
+			});
+
+			// wait for the registerSlotRequest completion
+			registerSlotRequestFuture.get();
+			assertEquals(0, rmServices.slotManager.getNumberPendingSlotRequest());
+		}};
+	}
+
 	/**
 	 * Test planned stop of a launched worker.
 	 */
diff --git a/flink-runtime/src/main/java/org/apache/flink/runtime/execution/SuppressRestartsException.java b/flink-runtime/src/main/java/org/apache/flink/runtime/execution/SuppressRestartsException.java
index 45ef760d25e5..7fea630b193c 100644
--- a/flink-runtime/src/main/java/org/apache/flink/runtime/execution/SuppressRestartsException.java
+++ b/flink-runtime/src/main/java/org/apache/flink/runtime/execution/SuppressRestartsException.java
@@ -35,7 +35,10 @@
 
 	public SuppressRestartsException(Throwable cause) {
 		super("Unrecoverable failure. This suppresses job restarts. Please check the " +
-				"stack trace for the root cause.", cause);
+			"stack trace for the root cause.", cause);
 	}
 
+	public SuppressRestartsException(String message, Throwable cause) {
+		super(message, cause);
+	}
 }
diff --git a/flink-runtime/src/main/java/org/apache/flink/runtime/resourcemanager/ResourceManager.java b/flink-runtime/src/main/java/org/apache/flink/runtime/resourcemanager/ResourceManager.java
index 1444322ca6d2..01d598a8cc32 100644
--- a/flink-runtime/src/main/java/org/apache/flink/runtime/resourcemanager/ResourceManager.java
+++ b/flink-runtime/src/main/java/org/apache/flink/runtime/resourcemanager/ResourceManager.java
@@ -71,6 +71,7 @@
 
 import javax.annotation.Nullable;
 
+import java.util.ArrayDeque;
 import java.util.ArrayList;
 import java.util.Collection;
 import java.util.Collections;
@@ -83,7 +84,7 @@
 import java.util.concurrent.ConcurrentHashMap;
 import java.util.concurrent.ConcurrentMap;
 import java.util.concurrent.TimeoutException;
-import java.util.concurrent.atomic.AtomicInteger;
+import java.util.concurrent.TimeUnit;
 import java.util.stream.Collectors;
 
 import static org.apache.flink.util.Preconditions.checkNotNull;
@@ -147,11 +148,14 @@
 	/** All registered listeners for status updates of the ResourceManager. */
 	private ConcurrentMap<String, InfoMessageListenerRpcGateway> infoMessageListeners;
 
-	/** The number of failed containers since the master became active. */
-	protected AtomicInteger failedContainerSoFar = new AtomicInteger(0);
+	protected final Time failureInterval;
+
+	protected final int maximumFailureTaskExecutorPerInternal;
+
+	private boolean checkFailureRate;
+
+	private final ArrayDeque<Long> taskExecutorFailureTimestamps;
 
-	/** Number of failed TaskManager containers before stopping the application. Default is Integer.MAX_VALUE */
-	protected int maximumAllowedTaskManagerFailureCount = Integer.MAX_VALUE;
 
 	/**
 	 * Represents asynchronous state clearing work.
@@ -161,6 +165,35 @@
 	 */
 	private CompletableFuture<Void> clearStateFuture = CompletableFuture.completedFuture(null);
 
+	public ResourceManager(
+		RpcService rpcService,
+		String resourceManagerEndpointId,
+		ResourceID resourceId,
+		HighAvailabilityServices highAvailabilityServices,
+		HeartbeatServices heartbeatServices,
+		SlotManager slotManager,
+		MetricRegistry metricRegistry,
+		JobLeaderIdService jobLeaderIdService,
+		ClusterInformation clusterInformation,
+		FatalErrorHandler fatalErrorHandler,
+		JobManagerMetricGroup jobManagerMetricGroup) {
+		this(
+			rpcService,
+			resourceManagerEndpointId,
+			resourceId,
+			highAvailabilityServices,
+			heartbeatServices,
+			slotManager,
+			metricRegistry,
+			jobLeaderIdService,
+			clusterInformation,
+			fatalErrorHandler,
+			jobManagerMetricGroup,
+			Time.of(300, TimeUnit.SECONDS),
+			-1
+		);
+	}
+
 	public ResourceManager(
 			RpcService rpcService,
 			String resourceManagerEndpointId,
@@ -172,7 +205,9 @@ public ResourceManager(
 			JobLeaderIdService jobLeaderIdService,
 			ClusterInformation clusterInformation,
 			FatalErrorHandler fatalErrorHandler,
-			JobManagerMetricGroup jobManagerMetricGroup) {
+			JobManagerMetricGroup jobManagerMetricGroup,
+			Time failureInterval,
+			int maxFailurePerInterval) {
 
 		super(rpcService, resourceManagerEndpointId);
 
@@ -200,7 +235,17 @@ public ResourceManager(
 		this.jobManagerRegistrations = new HashMap<>(4);
 		this.jmResourceIdRegistrations = new HashMap<>(4);
 		this.taskExecutors = new HashMap<>(8);
-		infoMessageListeners = new ConcurrentHashMap<>(8);
+		this.infoMessageListeners = new ConcurrentHashMap<>(8);
+		this.failureInterval = failureInterval;
+		this.maximumFailureTaskExecutorPerInternal = maxFailurePerInterval;
+
+		if (maximumFailureTaskExecutorPerInternal > 0) {
+			this.taskExecutorFailureTimestamps = new ArrayDeque<>(maximumFailureTaskExecutorPerInternal);
+			this.checkFailureRate = true;
+		} else {
+			this.taskExecutorFailureTimestamps = new ArrayDeque<>(0);
+			this.checkFailureRate = false;
+		}
 	}
 
 
@@ -429,10 +474,11 @@ public void disconnectJobManager(final JobID jobId, final Exception cause) {
 					slotRequest.getJobId(),
 					slotRequest.getAllocationId());
 
-				if (failedContainerSoFar.intValue() >= maximumAllowedTaskManagerFailureCount) {
+				if (shouldRejectRequests()) {
 					return FutureUtils.completedExceptionally(new MaximumFailedTaskManagerExceedingException(
-						String.format("Maximum number of failed container %d "
-							+ "is detected in Resource Manager", failedContainerSoFar.intValue())));
+						new RuntimeException(String.format("Maximum number of failed container %d in interval %s "
+							+ "is detected in Resource Manager.", taskExecutorFailureTimestamps.size(),
+							failureInterval.toString()))));
 				}
 
 				try {
@@ -644,6 +690,40 @@ protected void rejectAllPendingSlotRequests(Exception e) {
 		slotManager.rejectAllPendingSlotRequests(e);
 	}
 
+	protected synchronized void recordFailure() {
+		if (!checkFailureRate) {
+			return;
+		}
+		if (isFailureTimestampFull()) {
+			taskExecutorFailureTimestamps.remove();
+		}
+		taskExecutorFailureTimestamps.add(System.currentTimeMillis());
+	}
+
+	protected boolean shouldRejectRequests() {
+		if (!checkFailureRate) {
+			return false;
+		}
+
+		Long currentTimeStamp = System.currentTimeMillis();
+		while (currentTimeStamp - taskExecutorFailureTimestamps.peek() > failureInterval.toMilliseconds()) {
+			taskExecutorFailureTimestamps.remove();
+		}
+
+		if (!isFailureTimestampFull()) {
+			return false;
+		} else {
+			Long earliestFailure = taskExecutorFailureTimestamps.peek();
+			Long latestFailure = taskExecutorFailureTimestamps.getLast();
+
+			return latestFailure - earliestFailure < failureInterval.toMilliseconds();
+		}
+	}
+
+	private boolean isFailureTimestampFull() {
+		return taskExecutorFailureTimestamps.size() >= maximumFailureTaskExecutorPerInternal;
+	}
+
 	// ------------------------------------------------------------------------
 	//  Internal methods
 	// ------------------------------------------------------------------------
@@ -843,12 +923,6 @@ protected void closeTaskManagerConnection(final ResourceID resourceID, final Exc
 			slotManager.unregisterTaskManager(workerRegistration.getInstanceID());
 
 			workerRegistration.getTaskExecutorGateway().disconnectResourceManager(cause);
-			failedContainerSoFar.getAndAdd(1);
-			if (failedContainerSoFar.intValue() >= maximumAllowedTaskManagerFailureCount) {
-				rejectAllPendingSlotRequests(new MaximumFailedTaskManagerExceedingException(
-					String.format("Maximum number of failed container %d "
-						+ "is detected in Resource Manager", failedContainerSoFar.intValue())));
-			}
 		} else {
 			log.debug(
 				"No open TaskExecutor connection {}. Ignoring close TaskExecutor connection. Closing reason was: {}",
diff --git a/flink-runtime/src/main/java/org/apache/flink/runtime/resourcemanager/exceptions/MaximumFailedTaskManagerExceedingException.java b/flink-runtime/src/main/java/org/apache/flink/runtime/resourcemanager/exceptions/MaximumFailedTaskManagerExceedingException.java
index b949d9b47d01..3b90266e1cee 100644
--- a/flink-runtime/src/main/java/org/apache/flink/runtime/resourcemanager/exceptions/MaximumFailedTaskManagerExceedingException.java
+++ b/flink-runtime/src/main/java/org/apache/flink/runtime/resourcemanager/exceptions/MaximumFailedTaskManagerExceedingException.java
@@ -18,18 +18,16 @@
 
 package org.apache.flink.runtime.resourcemanager.exceptions;
 
+import org.apache.flink.runtime.execution.SuppressRestartsException;
 import org.apache.flink.runtime.resourcemanager.ResourceManager;
 
 /**
  * Exception for {@link ResourceManager} when it identified that the maximum number of failed containers is hit.
  */
-public class MaximumFailedTaskManagerExceedingException extends ResourceManagerException {
+public class MaximumFailedTaskManagerExceedingException extends SuppressRestartsException {
 	private static final long serialVersionUID = -2333228226519195160L;
 
-
-	public MaximumFailedTaskManagerExceedingException(String message) { super(message); }
+	public MaximumFailedTaskManagerExceedingException(Throwable cause) { super(cause); }
 
 	public MaximumFailedTaskManagerExceedingException(String message, Throwable cause) { super(message, cause); }
-
-	public MaximumFailedTaskManagerExceedingException(Throwable cause) { super(cause); }
 }
diff --git a/flink-runtime/src/main/java/org/apache/flink/runtime/resourcemanager/slotmanager/SlotManager.java b/flink-runtime/src/main/java/org/apache/flink/runtime/resourcemanager/slotmanager/SlotManager.java
index af27a762c2f4..943bad8933fe 100644
--- a/flink-runtime/src/main/java/org/apache/flink/runtime/resourcemanager/slotmanager/SlotManager.java
+++ b/flink-runtime/src/main/java/org/apache/flink/runtime/resourcemanager/slotmanager/SlotManager.java
@@ -178,6 +178,10 @@ public int getNumberPendingTaskManagerSlots() {
 		return pendingSlots.size();
 	}
 
+	public int getNumberPendingSlotRequest() {
+		return pendingSlotRequests.size();
+	}
+
 	@VisibleForTesting
 	int getNumberAssignedPendingTaskManagerSlots() {
 		return (int) pendingSlots.values().stream().filter(slot -> slot.getAssignedPendingSlotRequest() != null).count();
@@ -312,14 +316,13 @@ public void rejectAllPendingSlotRequests(Exception cause) {
 		pendingSlotRequests.clear();
 	}
 
-
-		/**
-         * Cancels and removes a pending slot request with the given allocation id. If there is no such
-         * pending request, then nothing is done.
-         *
-         * @param allocationId identifying the pending slot request
-         * @return True if a pending slot request was found; otherwise false
-         */
+	/**
+	 * Cancels and removes a pending slot request with the given allocation id. If there is no such
+	 * pending request, then nothing is done.
+	 *
+	 * @param allocationId identifying the pending slot request
+	 * @return True if a pending slot request was found; otherwise false
+	 */
 	public boolean unregisterSlotRequest(AllocationID allocationId) {
 		checkInit();
 
diff --git a/flink-yarn/src/main/java/org/apache/flink/yarn/YarnResourceManager.java b/flink-yarn/src/main/java/org/apache/flink/yarn/YarnResourceManager.java
index 495dee890172..63cf0b1ad172 100644
--- a/flink-yarn/src/main/java/org/apache/flink/yarn/YarnResourceManager.java
+++ b/flink-yarn/src/main/java/org/apache/flink/yarn/YarnResourceManager.java
@@ -19,6 +19,7 @@
 package org.apache.flink.yarn;
 
 import org.apache.flink.annotation.VisibleForTesting;
+import org.apache.flink.api.common.time.Time;
 import org.apache.flink.api.java.tuple.Tuple2;
 import org.apache.flink.configuration.Configuration;
 import org.apache.flink.configuration.ConfigurationUtils;
@@ -147,7 +148,9 @@ public YarnResourceManager(
 			ClusterInformation clusterInformation,
 			FatalErrorHandler fatalErrorHandler,
 			@Nullable String webInterfaceUrl,
-			JobManagerMetricGroup jobManagerMetricGroup) {
+			JobManagerMetricGroup jobManagerMetricGroup,
+			Time failureInterval,
+			int maxFailurePerInterval) {
 		super(
 			rpcService,
 			resourceManagerEndpointId,
@@ -159,7 +162,9 @@ public YarnResourceManager(
 			jobLeaderIdService,
 			clusterInformation,
 			fatalErrorHandler,
-			jobManagerMetricGroup);
+			jobManagerMetricGroup,
+			failureInterval,
+			maxFailurePerInterval);
 		this.flinkConfig  = flinkConfig;
 		this.yarnConfig = new YarnConfiguration();
 		this.env = env;
@@ -177,11 +182,6 @@ public YarnResourceManager(
 					yarnHeartbeatIntervalMS, yarnExpiryIntervalMS);
 		}
 
-		final int numInitialTM = Integer.parseInt(env.getOrDefault(
-			YarnConfigKeys.ENV_TM_COUNT, DEFAULT_INITIAL_NUM_TASK_MANAGER));
-		this.maximumAllowedTaskManagerFailureCount =
-			flinkConfig.getInteger(YarnConfigOptions.MAX_FAILED_CONTAINERS.key(), numInitialTM);
-
 		yarnHeartbeatIntervalMillis = yarnHeartbeatIntervalMS;
 		numPendingContainerRequests = 0;
 
@@ -405,20 +405,19 @@ public void onContainersAllocated(List<Container> containers) {
 						nodeManagerClient.startContainer(container, taskExecutorLaunchContext);
 					} catch (Throwable t) {
 						log.error("Could not start TaskManager in container {}.", container.getId(), t);
-
-						failedContainerSoFar.getAndAdd(1);
 						// release the failed container
 						workerNodeMap.remove(resourceId);
 						resourceManagerClient.releaseAssignedContainer(container.getId());
-
-						if (failedContainerSoFar.intValue() < maximumAllowedTaskManagerFailureCount) {
+						log.error("Could not start TaskManager in container {}.", container.getId(), t);
+						recordFailure();
+						if (shouldRejectRequests()) {
+							rejectAllPendingSlotRequests(new MaximumFailedTaskManagerExceedingException(
+								String.format("Maximum number of failed container %d in interval %s"
+										+ "is detected in Resource Manager", maximumFailureTaskExecutorPerInternal,
+									failureInterval.toString()), t));
+						} else {
 							// and ask for a new one
 							requestYarnContainerIfRequired();
-						} else {
-							log.error("Could not start TaskManager in container {}.", container.getId(), t);
-							rejectAllPendingSlotRequests(new MaximumFailedTaskManagerExceedingException(
-								String.format("Maximum number of failed container %d "
-										+ "is detected in Resource Manager", failedContainerSoFar.intValue())));
 						}
 					}
 				} else {
diff --git a/flink-yarn/src/main/java/org/apache/flink/yarn/configuration/YarnConfigOptions.java b/flink-yarn/src/main/java/org/apache/flink/yarn/configuration/YarnConfigOptions.java
index b0594756e9e9..5aeb3be9860d 100644
--- a/flink-yarn/src/main/java/org/apache/flink/yarn/configuration/YarnConfigOptions.java
+++ b/flink-yarn/src/main/java/org/apache/flink/yarn/configuration/YarnConfigOptions.java
@@ -83,8 +83,27 @@
 	 */
 	public static final ConfigOption<String> MAX_FAILED_CONTAINERS =
 		key("yarn.maximum-failed-containers")
-		.noDefaultValue()
-		.withDescription("Maximum number of containers the system is going to reallocate in case of a failure.");
+			.noDefaultValue()
+			.withDescription("Maximum number of containers the system is going to reallocate in case of a failure.");
+
+	/**
+	 * The maximum number of failed YARN containers within an interval before entirely stopping
+	 * the YARN session / job on YARN.
+	 * By default, the value is -1
+	 */
+	public static final ConfigOption<Integer> MAX_FAILED_CONTAINERS_PER_INTERVAL =
+		key("yarn.maximum-failed-containers-per-interval")
+		.defaultValue(-1)
+		.withDescription("Maximum number of containers the system is going to reallocate in case of a failure in an interval.");
+
+	/**
+	 * The interval for measuring failure rate of containers in second unit.
+	 * By default, the value is 5 minutes.
+	 **/
+	public static final ConfigOption<Integer> CONTAINERS_FAILURE_RATE_INTERVAL =
+		key("yarn.containers-failure-rate-interval")
+		.defaultValue(300)
+		.withDeprecatedKeys("The interval for measuring failure rate of containers");
 
 	/**
 	 * Set the number of retries for failed YARN ApplicationMasters/JobManagers in high
diff --git a/flink-yarn/src/main/java/org/apache/flink/yarn/entrypoint/YarnResourceManagerFactory.java b/flink-yarn/src/main/java/org/apache/flink/yarn/entrypoint/YarnResourceManagerFactory.java
index fc7c4e48e6a9..6afe1ad6b8a3 100644
--- a/flink-yarn/src/main/java/org/apache/flink/yarn/entrypoint/YarnResourceManagerFactory.java
+++ b/flink-yarn/src/main/java/org/apache/flink/yarn/entrypoint/YarnResourceManagerFactory.java
@@ -18,6 +18,7 @@
 
 package org.apache.flink.yarn.entrypoint;
 
+import org.apache.flink.api.common.time.Time;
 import org.apache.flink.configuration.Configuration;
 import org.apache.flink.runtime.clusterframework.types.ResourceID;
 import org.apache.flink.runtime.entrypoint.ClusterInformation;
@@ -33,9 +34,12 @@
 import org.apache.flink.runtime.rpc.RpcService;
 import org.apache.flink.yarn.YarnResourceManager;
 import org.apache.flink.yarn.YarnWorkerNode;
+import org.apache.flink.yarn.configuration.YarnConfigOptions;
 
 import javax.annotation.Nullable;
 
+import java.util.concurrent.TimeUnit;
+
 /**
  * {@link ResourceManagerFactory} implementation which creates a {@link YarnResourceManager}.
  */
@@ -60,6 +64,9 @@
 			highAvailabilityServices,
 			rpcService.getScheduledExecutor());
 
+		int maxFailurePerInternal = configuration.getInteger(YarnConfigOptions.MAX_FAILED_CONTAINERS_PER_INTERVAL);
+		long failureInterval = configuration.getInteger(YarnConfigOptions.CONTAINERS_FAILURE_RATE_INTERVAL);
+
 		return new YarnResourceManager(
 			rpcService,
 			ResourceManager.RESOURCE_MANAGER_NAME,
@@ -74,6 +81,8 @@
 			clusterInformation,
 			fatalErrorHandler,
 			webInterfaceUrl,
-			jobManagerMetricGroup);
+			jobManagerMetricGroup,
+			Time.of(failureInterval, TimeUnit.SECONDS),
+			maxFailurePerInternal);
 	}
 }
diff --git a/flink-yarn/src/test/java/org/apache/flink/yarn/YarnResourceManagerTest.java b/flink-yarn/src/test/java/org/apache/flink/yarn/YarnResourceManagerTest.java
index 9d892c3f84c4..bf244a8c4ec1 100644
--- a/flink-yarn/src/test/java/org/apache/flink/yarn/YarnResourceManagerTest.java
+++ b/flink-yarn/src/test/java/org/apache/flink/yarn/YarnResourceManagerTest.java
@@ -45,7 +45,6 @@
 import org.apache.flink.runtime.resourcemanager.JobLeaderIdService;
 import org.apache.flink.runtime.resourcemanager.ResourceManagerGateway;
 import org.apache.flink.runtime.resourcemanager.SlotRequest;
-import org.apache.flink.runtime.resourcemanager.exceptions.MaximumFailedTaskManagerExceedingException;
 import org.apache.flink.runtime.resourcemanager.slotmanager.SlotManager;
 import org.apache.flink.runtime.rpc.FatalErrorHandler;
 import org.apache.flink.runtime.rpc.RpcService;
@@ -58,6 +57,7 @@
 import org.apache.flink.runtime.util.TestingFatalErrorHandler;
 import org.apache.flink.util.TestLogger;
 import org.apache.flink.util.function.RunnableWithException;
+import org.apache.flink.yarn.configuration.YarnConfigOptions;
 
 import org.apache.flink.shaded.guava18.com.google.common.collect.ImmutableList;
 
@@ -101,7 +101,6 @@
 import static org.apache.flink.yarn.YarnConfigKeys.ENV_CLIENT_SHIP_FILES;
 import static org.apache.flink.yarn.YarnConfigKeys.ENV_FLINK_CLASSPATH;
 import static org.apache.flink.yarn.YarnConfigKeys.ENV_HADOOP_USER_NAME;
-import static org.apache.flink.yarn.YarnConfigKeys.ENV_TM_COUNT;
 import static org.apache.flink.yarn.YarnConfigKeys.FLINK_JAR_PATH;
 import static org.apache.flink.yarn.YarnConfigKeys.FLINK_YARN_FILES;
 import static org.hamcrest.Matchers.instanceOf;
@@ -114,7 +113,6 @@
 import static org.mockito.Matchers.eq;
 import static org.mockito.Mockito.doReturn;
 import static org.mockito.Mockito.mock;
-import static org.mockito.Mockito.spy;
 import static org.mockito.Mockito.times;
 import static org.mockito.Mockito.verify;
 import static org.mockito.Mockito.when;
@@ -141,6 +139,7 @@ public void setup() {
 
 		flinkConfig = new Configuration();
 		flinkConfig.setInteger(ResourceManagerOptions.CONTAINERIZED_HEAP_CUTOFF_MIN, 100);
+		flinkConfig.setInteger(YarnConfigOptions.MAX_FAILED_CONTAINERS_PER_INTERVAL, 1);
 
 		File root = folder.getRoot();
 		File home = new File(root, "home");
@@ -154,7 +153,6 @@ public void setup() {
 		env.put(ENV_FLINK_CLASSPATH, "");
 		env.put(ENV_HADOOP_USER_NAME, "foo");
 		env.put(FLINK_JAR_PATH, root.toURI().toString());
-		env.put(ENV_TM_COUNT, "1");
 	}
 
 	@After
@@ -188,7 +186,9 @@ public void teardown() throws Exception {
 				@Nullable String webInterfaceUrl,
 				AMRMClientAsync<AMRMClient.ContainerRequest> mockResourceManagerClient,
 				NMClient mockNMClient,
-				JobManagerMetricGroup jobManagerMetricGroup) {
+				JobManagerMetricGroup jobManagerMetricGroup,
+				Time failureInterval,
+				int maxFailurePerInterval) {
 			super(
 				rpcService,
 				resourceManagerEndpointId,
@@ -203,7 +203,9 @@ public void teardown() throws Exception {
 				clusterInformation,
 				fatalErrorHandler,
 				webInterfaceUrl,
-				jobManagerMetricGroup);
+				jobManagerMetricGroup,
+				failureInterval,
+				maxFailurePerInterval);
 			this.mockNMClient = mockNMClient;
 			this.mockResourceManagerClient = mockResourceManagerClient;
 		}
@@ -269,6 +271,9 @@ protected void runAsync(final Runnable runnable) {
 			rpcService = new TestingRpcService();
 			rmServices = new MockResourceManagerRuntimeServices();
 
+			int maxFailurePerInternal = flinkConfig.getInteger(YarnConfigOptions.MAX_FAILED_CONTAINERS_PER_INTERVAL);
+			long failureInterval = flinkConfig.getInteger(YarnConfigOptions.CONTAINERS_FAILURE_RATE_INTERVAL);
+
 			// resource manager
 			rmResourceID = ResourceID.generate();
 			resourceManager =
@@ -288,7 +293,9 @@ protected void runAsync(final Runnable runnable) {
 							null,
 							mockResourceManagerClient,
 							mockNMClient,
-							mockJMMetricGroup);
+							mockJMMetricGroup,
+							Time.of(failureInterval, TimeUnit.SECONDS),
+							maxFailurePerInternal);
 		}
 
 		/**
@@ -313,9 +320,9 @@ protected void runAsync(final Runnable runnable) {
 				highAvailabilityServices.setResourceManagerLeaderElectionService(rmLeaderElectionService);
 				heartbeatServices = new TestingHeartbeatServices(5L, 5L, scheduledExecutor);
 				metricRegistry = NoOpMetricRegistry.INSTANCE;
-				slotManager = spy(new SlotManager(
+				slotManager = new SlotManager(
 						new ScheduledExecutorServiceAdapter(new DirectScheduledExecutorService()),
-						Time.seconds(10), Time.seconds(10), Time.minutes(1)));
+						Time.seconds(10), Time.seconds(10), Time.minutes(1));
 				jobLeaderIdService = new JobLeaderIdService(
 						highAvailabilityServices,
 						rpcService.getScheduledExecutor(),
@@ -574,8 +581,7 @@ public void testOnContainersAllocatedWithFailure() throws Exception {
 				Container failedContainer = mockContainer("container2", 2345, 2, resourceManager.getContainerResource());
 				when(mockNMClient.startContainer(eq(failedContainer), any())).thenThrow(new YarnException("Failed"));
 				resourceManager.onContainersAllocated(ImmutableList.of(failedContainer));
-				verify(rmServices.slotManager, times(1))
-					.rejectAllPendingSlotRequests(any(MaximumFailedTaskManagerExceedingException.class));
+				assertEquals(rmServices.slotManager.getNumberPendingSlotRequest(), 0);
 			});
 		}};
 	}

From 669ebd9591be7260e8e24c14532459dc53e7836f Mon Sep 17 00:00:00 2001
From: Peter Huang <huangzhenqiu0825@gmail.com>
Date: Mon, 28 Jan 2019 21:57:08 -0800
Subject: [PATCH 7/8] generate config docs

---
 docs/_includes/generated/mesos_configuration.html      | 10 ++++++++++
 .../_includes/generated/yarn_config_configuration.html | 10 ++++++++++
 .../flink/yarn/configuration/YarnConfigOptions.java    |  2 +-
 3 files changed, 21 insertions(+), 1 deletion(-)

diff --git a/docs/_includes/generated/mesos_configuration.html b/docs/_includes/generated/mesos_configuration.html
index 54e92e5680c5..72e94c379f0d 100644
--- a/docs/_includes/generated/mesos_configuration.html
+++ b/docs/_includes/generated/mesos_configuration.html
@@ -27,6 +27,11 @@
             <td style="word-wrap: break-word;">-1</td>
             <td>The maximum number of failed workers before the cluster fails. May be set to -1 to disable this feature. This option is ignored unless Flink is in <a href="#legacy">legacy mode</a>.</td>
         </tr>
+        <tr>
+            <td><h5>mesos.maximum-failed-workers-per-interval</h5></td>
+            <td style="word-wrap: break-word;">-1</td>
+            <td>Maximum number of workers the system is going to reallocate in case of a failure in an interval.</td>
+        </tr>
         <tr>
             <td><h5>mesos.resourcemanager.artifactserver.port</h5></td>
             <td style="word-wrap: break-word;">0</td>
@@ -67,5 +72,10 @@
             <td style="word-wrap: break-word;">(none)</td>
             <td>Comma-separated list of configuration keys which represent a configurable port. All port keys will dynamically get a port assigned through Mesos.</td>
         </tr>
+        <tr>
+            <td><h5>mesos.workers-failure-rate-interval</h5></td>
+            <td style="word-wrap: break-word;">300</td>
+            <td></td>
+        </tr>
     </tbody>
 </table>
diff --git a/docs/_includes/generated/yarn_config_configuration.html b/docs/_includes/generated/yarn_config_configuration.html
index bbe25499f175..bd0fdb704e19 100644
--- a/docs/_includes/generated/yarn_config_configuration.html
+++ b/docs/_includes/generated/yarn_config_configuration.html
@@ -27,6 +27,11 @@
             <td style="word-wrap: break-word;">-1</td>
             <td>The port where the application master RPC system is listening.</td>
         </tr>
+        <tr>
+            <td><h5>yarn.containers-failure-rate-interval</h5></td>
+            <td style="word-wrap: break-word;">300</td>
+            <td>The interval for measuring failure rate of containers</td>
+        </tr>
         <tr>
             <td><h5>yarn.containers.vcores</h5></td>
             <td style="word-wrap: break-word;">-1</td>
@@ -42,6 +47,11 @@
             <td style="word-wrap: break-word;">(none)</td>
             <td>Maximum number of containers the system is going to reallocate in case of a failure.</td>
         </tr>
+        <tr>
+            <td><h5>yarn.maximum-failed-containers-per-interval</h5></td>
+            <td style="word-wrap: break-word;">-1</td>
+            <td>Maximum number of containers the system is going to reallocate in case of a failure in an interval.</td>
+        </tr>
         <tr>
             <td><h5>yarn.per-job-cluster.include-user-jar</h5></td>
             <td style="word-wrap: break-word;">"ORDER"</td>
diff --git a/flink-yarn/src/main/java/org/apache/flink/yarn/configuration/YarnConfigOptions.java b/flink-yarn/src/main/java/org/apache/flink/yarn/configuration/YarnConfigOptions.java
index 5aeb3be9860d..f44c2fc83c8d 100644
--- a/flink-yarn/src/main/java/org/apache/flink/yarn/configuration/YarnConfigOptions.java
+++ b/flink-yarn/src/main/java/org/apache/flink/yarn/configuration/YarnConfigOptions.java
@@ -103,7 +103,7 @@
 	public static final ConfigOption<Integer> CONTAINERS_FAILURE_RATE_INTERVAL =
 		key("yarn.containers-failure-rate-interval")
 		.defaultValue(300)
-		.withDeprecatedKeys("The interval for measuring failure rate of containers");
+		.withDescription("The interval for measuring failure rate of containers");
 
 	/**
 	 * Set the number of retries for failed YARN ApplicationMasters/JobManagers in high

From 11ebab9f538d5ffb5710b7a63f661b610fc6e3a5 Mon Sep 17 00:00:00 2001
From: Peter Huang <huangzhenqiu0825@gmail.com>
Date: Fri, 1 Feb 2019 10:26:52 -0800
Subject: [PATCH 8/8] fix Till's comments

---
 .../generated/mesos_configuration.html        | 10 ---
 .../resource_manager_configuration.html       |  5 ++
 .../generated/yarn_config_configuration.html  | 10 ---
 .../configuration/ResourceManagerOptions.java | 12 +++
 .../mesos/configuration/MesosOptions.java     | 19 -----
 .../MesosResourceManager.java                 | 56 ++++++-------
 .../MesosResourceManagerFactory.java          | 10 +--
 .../MesosResourceManagerTest.java             | 15 ++--
 .../runtime/failurerate/FailureRater.java     | 63 ++++++++++++++
 .../TimestampBasedFailureRater.java           | 82 +++++++++++++++++++
 .../resourcemanager/ResourceManager.java      | 79 ++++++------------
 .../slotmanager/SlotManager.java              |  1 +
 .../TimestampBasedFailureRaterTest.java       | 58 +++++++++++++
 .../flink/yarn/YarnResourceManager.java       | 45 ++++------
 .../yarn/configuration/YarnConfigOptions.java | 19 -----
 .../YarnResourceManagerFactory.java           | 10 +--
 .../flink/yarn/YarnResourceManagerTest.java   | 50 ++++++-----
 17 files changed, 325 insertions(+), 219 deletions(-)
 create mode 100644 flink-runtime/src/main/java/org/apache/flink/runtime/failurerate/FailureRater.java
 create mode 100644 flink-runtime/src/main/java/org/apache/flink/runtime/failurerate/TimestampBasedFailureRater.java
 create mode 100644 flink-runtime/src/test/java/org/apache/flink/runtime/failurerate/TimestampBasedFailureRaterTest.java

diff --git a/docs/_includes/generated/mesos_configuration.html b/docs/_includes/generated/mesos_configuration.html
index 72e94c379f0d..54e92e5680c5 100644
--- a/docs/_includes/generated/mesos_configuration.html
+++ b/docs/_includes/generated/mesos_configuration.html
@@ -27,11 +27,6 @@
             <td style="word-wrap: break-word;">-1</td>
             <td>The maximum number of failed workers before the cluster fails. May be set to -1 to disable this feature. This option is ignored unless Flink is in <a href="#legacy">legacy mode</a>.</td>
         </tr>
-        <tr>
-            <td><h5>mesos.maximum-failed-workers-per-interval</h5></td>
-            <td style="word-wrap: break-word;">-1</td>
-            <td>Maximum number of workers the system is going to reallocate in case of a failure in an interval.</td>
-        </tr>
         <tr>
             <td><h5>mesos.resourcemanager.artifactserver.port</h5></td>
             <td style="word-wrap: break-word;">0</td>
@@ -72,10 +67,5 @@
             <td style="word-wrap: break-word;">(none)</td>
             <td>Comma-separated list of configuration keys which represent a configurable port. All port keys will dynamically get a port assigned through Mesos.</td>
         </tr>
-        <tr>
-            <td><h5>mesos.workers-failure-rate-interval</h5></td>
-            <td style="word-wrap: break-word;">300</td>
-            <td></td>
-        </tr>
     </tbody>
 </table>
diff --git a/docs/_includes/generated/resource_manager_configuration.html b/docs/_includes/generated/resource_manager_configuration.html
index 3448aba8f523..b0bab9f45d95 100644
--- a/docs/_includes/generated/resource_manager_configuration.html
+++ b/docs/_includes/generated/resource_manager_configuration.html
@@ -27,6 +27,11 @@
             <td style="word-wrap: break-word;">"5 minutes"</td>
             <td>Timeout for jobs which don't have a job manager as leader assigned.</td>
         </tr>
+        <tr>
+            <td><h5>resourcemanager.maximum-workers-failure-rate</h5></td>
+            <td style="word-wrap: break-word;">-1</td>
+            <td>Defines the maximum number of workers (YARN / Mesos) failure can happen in a minute.It is to quickly catch external dependency caused workers failure and terminate jobaccordingly. Be default, -1 is set to disable the feature.</td>
+        </tr>
         <tr>
             <td><h5>resourcemanager.rpc.port</h5></td>
             <td style="word-wrap: break-word;">0</td>
diff --git a/docs/_includes/generated/yarn_config_configuration.html b/docs/_includes/generated/yarn_config_configuration.html
index bd0fdb704e19..bbe25499f175 100644
--- a/docs/_includes/generated/yarn_config_configuration.html
+++ b/docs/_includes/generated/yarn_config_configuration.html
@@ -27,11 +27,6 @@
             <td style="word-wrap: break-word;">-1</td>
             <td>The port where the application master RPC system is listening.</td>
         </tr>
-        <tr>
-            <td><h5>yarn.containers-failure-rate-interval</h5></td>
-            <td style="word-wrap: break-word;">300</td>
-            <td>The interval for measuring failure rate of containers</td>
-        </tr>
         <tr>
             <td><h5>yarn.containers.vcores</h5></td>
             <td style="word-wrap: break-word;">-1</td>
@@ -47,11 +42,6 @@
             <td style="word-wrap: break-word;">(none)</td>
             <td>Maximum number of containers the system is going to reallocate in case of a failure.</td>
         </tr>
-        <tr>
-            <td><h5>yarn.maximum-failed-containers-per-interval</h5></td>
-            <td style="word-wrap: break-word;">-1</td>
-            <td>Maximum number of containers the system is going to reallocate in case of a failure in an interval.</td>
-        </tr>
         <tr>
             <td><h5>yarn.per-job-cluster.include-user-jar</h5></td>
             <td style="word-wrap: break-word;">"ORDER"</td>
diff --git a/flink-core/src/main/java/org/apache/flink/configuration/ResourceManagerOptions.java b/flink-core/src/main/java/org/apache/flink/configuration/ResourceManagerOptions.java
index ca6cc146123a..2c62297c267b 100644
--- a/flink-core/src/main/java/org/apache/flink/configuration/ResourceManagerOptions.java
+++ b/flink-core/src/main/java/org/apache/flink/configuration/ResourceManagerOptions.java
@@ -55,6 +55,18 @@
 			" default, the port of the JobManager, because the same ActorSystem is used." +
 			" Its not possible to use this configuration key to define port ranges.");
 
+	/**
+	 * Defines the maximum number of workers (YARN / Mesos) failure can happen in a minute.
+	 * It is to quickly catch external dependency caused workers failure and terminate job
+	 * accordingly. Be default, -1 is set to disable the feature.
+	 */
+	public static final ConfigOption<Integer> MAXIMUM_WORKERS_FAILURE_RATE = ConfigOptions
+		.key("resourcemanager.maximum-workers-failure-rate")
+		.defaultValue(-1)
+		.withDescription("Defines the maximum number of workers (YARN / Mesos) failure can happen in a minute." +
+			"It is to quickly catch external dependency caused workers failure and terminate job" +
+			"accordingly. Be default, -1 is set to disable the feature.");
+
 	/**
 	 * Percentage of heap space to remove from containers (YARN / Mesos), to compensate
 	 * for other JVM memory usage.
diff --git a/flink-mesos/src/main/java/org/apache/flink/mesos/configuration/MesosOptions.java b/flink-mesos/src/main/java/org/apache/flink/mesos/configuration/MesosOptions.java
index 003981c2320e..0c4e1f6bcba4 100644
--- a/flink-mesos/src/main/java/org/apache/flink/mesos/configuration/MesosOptions.java
+++ b/flink-mesos/src/main/java/org/apache/flink/mesos/configuration/MesosOptions.java
@@ -99,25 +99,6 @@
 			.withDescription("The config parameter defining the Mesos artifact server port to use. Setting the port to" +
 				" 0 will let the OS choose an available port.");
 
-	/**
-	 * The maximum number of failed Mesos worker within an interval before entirely stopping
-	 * the Mesos session / job on Mesos.
-	 * By default, the value is -1
-	 */
-	public static final ConfigOption<Integer> MAX_FAILED_WORKERS_PER_INTERVAL =
-		key("mesos.maximum-failed-workers-per-interval")
-			.defaultValue(-1)
-			.withDescription("Maximum number of workers the system is going to reallocate in case of a failure in an interval.");
-
-	/**
-	 * The interval for measuring failure rate of containers in second unit.
-	 * By default, the value is 5 minutes.
-	 **/
-	public static final ConfigOption<Integer> WORKERS_FAILURE_RATE_INTERVAL =
-		key("mesos.workers-failure-rate-interval")
-			.defaultValue(300)
-			.withDeprecatedKeys("The interval for measuring failure rate of workers");
-
 	public static final ConfigOption<String> RESOURCEMANAGER_FRAMEWORK_NAME =
 		key("mesos.resourcemanager.framework.name")
 			.defaultValue("Flink")
diff --git a/flink-mesos/src/main/java/org/apache/flink/mesos/runtime/clusterframework/MesosResourceManager.java b/flink-mesos/src/main/java/org/apache/flink/mesos/runtime/clusterframework/MesosResourceManager.java
index 74c1a6222fda..a4d875b13c8d 100644
--- a/flink-mesos/src/main/java/org/apache/flink/mesos/runtime/clusterframework/MesosResourceManager.java
+++ b/flink-mesos/src/main/java/org/apache/flink/mesos/runtime/clusterframework/MesosResourceManager.java
@@ -18,7 +18,6 @@
 
 package org.apache.flink.mesos.runtime.clusterframework;
 
-import org.apache.flink.api.common.time.Time;
 import org.apache.flink.api.java.tuple.Tuple2;
 import org.apache.flink.configuration.Configuration;
 import org.apache.flink.mesos.runtime.clusterframework.services.MesosServices;
@@ -48,13 +47,13 @@
 import org.apache.flink.runtime.clusterframework.types.ResourceProfile;
 import org.apache.flink.runtime.concurrent.FutureUtils;
 import org.apache.flink.runtime.entrypoint.ClusterInformation;
+import org.apache.flink.runtime.failurerate.FailureRater;
 import org.apache.flink.runtime.heartbeat.HeartbeatServices;
 import org.apache.flink.runtime.highavailability.HighAvailabilityServices;
 import org.apache.flink.runtime.metrics.MetricRegistry;
 import org.apache.flink.runtime.metrics.groups.JobManagerMetricGroup;
 import org.apache.flink.runtime.resourcemanager.JobLeaderIdService;
 import org.apache.flink.runtime.resourcemanager.ResourceManager;
-import org.apache.flink.runtime.resourcemanager.exceptions.MaximumFailedTaskManagerExceedingException;
 import org.apache.flink.runtime.resourcemanager.exceptions.ResourceManagerException;
 import org.apache.flink.runtime.resourcemanager.slotmanager.SlotManager;
 import org.apache.flink.runtime.rpc.FatalErrorHandler;
@@ -151,27 +150,26 @@
 	private MesosConfiguration initializedMesosConfig;
 
 	public MesosResourceManager(
-			// base class
-			RpcService rpcService,
-			String resourceManagerEndpointId,
-			ResourceID resourceId,
-			HighAvailabilityServices highAvailabilityServices,
-			HeartbeatServices heartbeatServices,
-			SlotManager slotManager,
-			MetricRegistry metricRegistry,
-			JobLeaderIdService jobLeaderIdService,
-			ClusterInformation clusterInformation,
-			FatalErrorHandler fatalErrorHandler,
-			// Mesos specifics
-			Configuration flinkConfig,
-			MesosServices mesosServices,
-			MesosConfiguration mesosConfig,
-			MesosTaskManagerParameters taskManagerParameters,
-			ContainerSpecification taskManagerContainerSpec,
-			@Nullable String webUiUrl,
-			JobManagerMetricGroup jobManagerMetricGroup,
-			Time failureInterval,
-			int maxFailurePerInterval) {
+		// base class
+		RpcService rpcService,
+		String resourceManagerEndpointId,
+		ResourceID resourceId,
+		HighAvailabilityServices highAvailabilityServices,
+		HeartbeatServices heartbeatServices,
+		SlotManager slotManager,
+		MetricRegistry metricRegistry,
+		JobLeaderIdService jobLeaderIdService,
+		ClusterInformation clusterInformation,
+		FatalErrorHandler fatalErrorHandler,
+		// Mesos specifics
+		Configuration flinkConfig,
+		MesosServices mesosServices,
+		MesosConfiguration mesosConfig,
+		MesosTaskManagerParameters taskManagerParameters,
+		ContainerSpecification taskManagerContainerSpec,
+		@Nullable String webUiUrl,
+		JobManagerMetricGroup jobManagerMetricGroup,
+		FailureRater failureRater) {
 		super(
 			rpcService,
 			resourceManagerEndpointId,
@@ -184,8 +182,7 @@ public MesosResourceManager(
 			clusterInformation,
 			fatalErrorHandler,
 			jobManagerMetricGroup,
-			failureInterval,
-			maxFailurePerInterval);
+			failureRater);
 
 		this.mesosServices = Preconditions.checkNotNull(mesosServices);
 		this.actorSystem = Preconditions.checkNotNull(mesosServices.getLocalActorSystem());
@@ -670,14 +667,7 @@ public void taskTerminated(TaskMonitor.TaskTerminated message) {
 			LOG.info("Worker {} failed with status: {}, reason: {}, message: {}.",
 				id, status.getState(), status.getReason(), status.getMessage());
 
-			recordFailure();
-
-			if (shouldRejectRequests()) {
-				rejectAllPendingSlotRequests(new MaximumFailedTaskManagerExceedingException(
-					new RuntimeException(String.format("Maximum number of failed workers %d in interval %s"
-							+ "is detected in Resource Manager", maximumFailureTaskExecutorPerInternal,
-						failureInterval.toString()))));
-			} else {
+			if (recordFailure()) {
 				startNewWorker(launched.profile());
 			}
 		}
diff --git a/flink-mesos/src/main/java/org/apache/flink/mesos/runtime/clusterframework/MesosResourceManagerFactory.java b/flink-mesos/src/main/java/org/apache/flink/mesos/runtime/clusterframework/MesosResourceManagerFactory.java
index a222e7552b52..2f49d6f4d1cd 100644
--- a/flink-mesos/src/main/java/org/apache/flink/mesos/runtime/clusterframework/MesosResourceManagerFactory.java
+++ b/flink-mesos/src/main/java/org/apache/flink/mesos/runtime/clusterframework/MesosResourceManagerFactory.java
@@ -20,12 +20,13 @@
 
 import org.apache.flink.api.common.time.Time;
 import org.apache.flink.configuration.Configuration;
-import org.apache.flink.mesos.configuration.MesosOptions;
+import org.apache.flink.configuration.ResourceManagerOptions;
 import org.apache.flink.mesos.runtime.clusterframework.services.MesosServices;
 import org.apache.flink.mesos.util.MesosConfiguration;
 import org.apache.flink.runtime.clusterframework.ContainerSpecification;
 import org.apache.flink.runtime.clusterframework.types.ResourceID;
 import org.apache.flink.runtime.entrypoint.ClusterInformation;
+import org.apache.flink.runtime.failurerate.TimestampBasedFailureRater;
 import org.apache.flink.runtime.heartbeat.HeartbeatServices;
 import org.apache.flink.runtime.highavailability.HighAvailabilityServices;
 import org.apache.flink.runtime.metrics.MetricRegistry;
@@ -84,8 +85,7 @@ public MesosResourceManagerFactory(@Nonnull MesosServices mesosServices, @Nonnul
 			highAvailabilityServices,
 			rpcService.getScheduledExecutor());
 
-		int maxFailurePerInternal = configuration.getInteger(MesosOptions.MAX_FAILED_WORKERS_PER_INTERVAL);
-		long failureInterval = configuration.getInteger(MesosOptions.WORKERS_FAILURE_RATE_INTERVAL);
+		int failureRate = configuration.getInteger(ResourceManagerOptions.MAXIMUM_WORKERS_FAILURE_RATE);
 
 		return new MesosResourceManager(
 			rpcService,
@@ -105,7 +105,7 @@ public MesosResourceManagerFactory(@Nonnull MesosServices mesosServices, @Nonnul
 			taskManagerContainerSpec,
 			webInterfaceUrl,
 			jobManagerMetricGroup,
-			Time.of(failureInterval, TimeUnit.SECONDS),
-			maxFailurePerInternal);
+			new TimestampBasedFailureRater(failureRate, Time.of(1, TimeUnit.MINUTES))
+		);
 	}
 }
diff --git a/flink-mesos/src/test/java/org/apache/flink/mesos/runtime/clusterframework/MesosResourceManagerTest.java b/flink-mesos/src/test/java/org/apache/flink/mesos/runtime/clusterframework/MesosResourceManagerTest.java
index 56721063d50f..4cc24d2857f3 100644
--- a/flink-mesos/src/test/java/org/apache/flink/mesos/runtime/clusterframework/MesosResourceManagerTest.java
+++ b/flink-mesos/src/test/java/org/apache/flink/mesos/runtime/clusterframework/MesosResourceManagerTest.java
@@ -45,6 +45,8 @@
 import org.apache.flink.runtime.clusterframework.types.ResourceProfile;
 import org.apache.flink.runtime.concurrent.ScheduledExecutor;
 import org.apache.flink.runtime.entrypoint.ClusterInformation;
+import org.apache.flink.runtime.failurerate.FailureRater;
+import org.apache.flink.runtime.failurerate.TimestampBasedFailureRater;
 import org.apache.flink.runtime.heartbeat.HeartbeatServices;
 import org.apache.flink.runtime.heartbeat.TestingHeartbeatServices;
 import org.apache.flink.runtime.highavailability.HighAvailabilityServices;
@@ -178,8 +180,7 @@ public TestingMesosResourceManager(
 			MesosTaskManagerParameters taskManagerParameters,
 			ContainerSpecification taskManagerContainerSpec,
 			JobManagerMetricGroup jobManagerMetricGroup,
-			Time failureInterval,
-			int maxFailurePerInterval) {
+			FailureRater failureRater) {
 			super(
 				rpcService,
 				resourceManagerEndpointId,
@@ -198,8 +199,7 @@ public TestingMesosResourceManager(
 				taskManagerContainerSpec,
 				null,
 				jobManagerMetricGroup,
-				failureInterval,
-				maxFailurePerInterval);
+				failureRater);
 		}
 
 		<T> CompletableFuture<T> runInMainThread(Callable<T> callable) {
@@ -314,8 +314,7 @@ protected void closeTaskManagerConnection(ResourceID resourceID, Exception cause
 					tmParams,
 					containerSpecification,
 					UnregisteredMetricGroups.createUnregisteredJobManagerMetricGroup(),
-					Time.of(300, TimeUnit.SECONDS),
-					2);
+					new TimestampBasedFailureRater(2, Time.of(1, TimeUnit.MINUTES)));
 
 			// TaskExecutors
 			task1Executor = mockTaskExecutor(task1);
@@ -720,9 +719,6 @@ public void testWorkerFailed() throws Exception {
 		}};
 	}
 
-	/**
-	 * Test worker failure hit maximum worker failure rate.
-	 */
 	@Test
 	public void testWorkerFailedAtFailureRate() throws Exception {
 		new Context() {{
@@ -787,7 +783,6 @@ public void testStopWorker() throws Exception {
 			verify(rmServices.workerStore).putWorker(worker1Released);
 			assertThat(resourceManager.workersInLaunch.entrySet(), empty());
 			assertThat(resourceManager.workersBeingReturned, hasEntry(extractResourceID(task1), worker1Released));
-
 			// verify that the monitor was notified
 			resourceManager.taskRouter.expectMsgClass(TaskMonitor.TaskGoalStateUpdated.class);
 			resourceManager.launchCoordinator.expectMsgClass(LaunchCoordinator.Unassign.class);
diff --git a/flink-runtime/src/main/java/org/apache/flink/runtime/failurerate/FailureRater.java b/flink-runtime/src/main/java/org/apache/flink/runtime/failurerate/FailureRater.java
new file mode 100644
index 000000000000..f991905c17ce
--- /dev/null
+++ b/flink-runtime/src/main/java/org/apache/flink/runtime/failurerate/FailureRater.java
@@ -0,0 +1,63 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.flink.runtime.failurerate;
+
+import org.apache.flink.api.common.time.Time;
+
+/**
+ * A rater to record the failure rate within a time interval.
+ */
+public interface FailureRater {
+
+	/**
+	 * Record one time of failure.
+	 */
+	public void recordFailure();
+
+
+	/**
+	 * Get the maximum allow failure rate. Usually, it is a threshold as parameter for FailureRater constructor.
+	 *
+	 * @return maximum allow failure rate
+	 */
+	public int getMaximumFailureRate();
+
+	/**
+	 * Get the time interval that is used for failure rate measurement.
+	 *
+	 *
+	 * @return failure interval of the rater
+	 */
+	public Time getFailureInterval();
+
+	/**
+	 * Get failure rate until current timestamp.
+	 *
+	 * @return current failure rate
+	 */
+	public long getCurrentFailureRate();
+
+
+	/**
+	 * Lookup the number of failure from now - internal to now and check whether maximum failure rate is hit.
+	 *
+	 * @return whether maximum failure rate is hit
+	 */
+	public boolean exceedMaximumFailureRate();
+}
diff --git a/flink-runtime/src/main/java/org/apache/flink/runtime/failurerate/TimestampBasedFailureRater.java b/flink-runtime/src/main/java/org/apache/flink/runtime/failurerate/TimestampBasedFailureRater.java
new file mode 100644
index 000000000000..fe499fe1d634
--- /dev/null
+++ b/flink-runtime/src/main/java/org/apache/flink/runtime/failurerate/TimestampBasedFailureRater.java
@@ -0,0 +1,82 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.flink.runtime.failurerate;
+
+import org.apache.flink.api.common.time.Time;
+
+import java.util.ArrayDeque;
+
+/**
+ * A timestamp queue based failure rater implementation.
+ *
+ *
+ */
+public class TimestampBasedFailureRater implements FailureRater {
+	private static final int DEFAULT_TIMESTAMP_SIZE = 300;
+	private final int maximumFailureRate;
+	private final Time failureInterval;
+	private final ArrayDeque<Long> failureTimestamps;
+
+	public TimestampBasedFailureRater(int maximumFailureRate, Time failureInterval) {
+		this.maximumFailureRate = maximumFailureRate;
+		this.failureInterval = failureInterval;
+		this.failureTimestamps = new ArrayDeque<>(maximumFailureRate > 0 ? maximumFailureRate : DEFAULT_TIMESTAMP_SIZE);
+	}
+
+	@Override
+	public void recordFailure() {
+		failureTimestamps.add(System.currentTimeMillis());
+	}
+
+	@Override
+	public int getMaximumFailureRate() {
+		return maximumFailureRate;
+	}
+
+	@Override
+	public Time getFailureInterval() {
+		return failureInterval;
+	}
+
+	@Override
+	public long getCurrentFailureRate() {
+		Long currentTimeStamp = System.currentTimeMillis();
+		while (!failureTimestamps.isEmpty() &&
+			currentTimeStamp - failureTimestamps.peek() > failureInterval.toMilliseconds()) {
+			failureTimestamps.remove();
+		}
+
+		return failureTimestamps.size();
+	}
+
+	@Override
+	public boolean exceedMaximumFailureRate() {
+		if (maximumFailureRate < 0) {
+			return false;
+		}
+		long currentRate = getCurrentFailureRate();
+		if (currentRate < maximumFailureRate) {
+			return  false;
+		}
+
+		Long earliestTimestamp = failureTimestamps.peek();
+
+		return System.currentTimeMillis() - earliestTimestamp < failureInterval.toMilliseconds();
+	}
+}
diff --git a/flink-runtime/src/main/java/org/apache/flink/runtime/resourcemanager/ResourceManager.java b/flink-runtime/src/main/java/org/apache/flink/runtime/resourcemanager/ResourceManager.java
index 01d598a8cc32..2fabaf6f9c13 100644
--- a/flink-runtime/src/main/java/org/apache/flink/runtime/resourcemanager/ResourceManager.java
+++ b/flink-runtime/src/main/java/org/apache/flink/runtime/resourcemanager/ResourceManager.java
@@ -32,6 +32,8 @@
 import org.apache.flink.runtime.clusterframework.types.SlotID;
 import org.apache.flink.runtime.concurrent.FutureUtils;
 import org.apache.flink.runtime.entrypoint.ClusterInformation;
+import org.apache.flink.runtime.failurerate.FailureRater;
+import org.apache.flink.runtime.failurerate.TimestampBasedFailureRater;
 import org.apache.flink.runtime.heartbeat.HeartbeatListener;
 import org.apache.flink.runtime.heartbeat.HeartbeatManager;
 import org.apache.flink.runtime.heartbeat.HeartbeatServices;
@@ -71,7 +73,6 @@
 
 import javax.annotation.Nullable;
 
-import java.util.ArrayDeque;
 import java.util.ArrayList;
 import java.util.Collection;
 import java.util.Collections;
@@ -148,13 +149,7 @@
 	/** All registered listeners for status updates of the ResourceManager. */
 	private ConcurrentMap<String, InfoMessageListenerRpcGateway> infoMessageListeners;
 
-	protected final Time failureInterval;
-
-	protected final int maximumFailureTaskExecutorPerInternal;
-
-	private boolean checkFailureRate;
-
-	private final ArrayDeque<Long> taskExecutorFailureTimestamps;
+	protected final FailureRater failureRater;
 
 
 	/**
@@ -189,8 +184,7 @@ public ResourceManager(
 			clusterInformation,
 			fatalErrorHandler,
 			jobManagerMetricGroup,
-			Time.of(300, TimeUnit.SECONDS),
-			-1
+			new TimestampBasedFailureRater(-1, Time.of(1, TimeUnit.MINUTES))
 		);
 	}
 
@@ -206,8 +200,7 @@ public ResourceManager(
 			ClusterInformation clusterInformation,
 			FatalErrorHandler fatalErrorHandler,
 			JobManagerMetricGroup jobManagerMetricGroup,
-			Time failureInterval,
-			int maxFailurePerInterval) {
+			FailureRater failureRater) {
 
 		super(rpcService, resourceManagerEndpointId);
 
@@ -236,16 +229,7 @@ public ResourceManager(
 		this.jmResourceIdRegistrations = new HashMap<>(4);
 		this.taskExecutors = new HashMap<>(8);
 		this.infoMessageListeners = new ConcurrentHashMap<>(8);
-		this.failureInterval = failureInterval;
-		this.maximumFailureTaskExecutorPerInternal = maxFailurePerInterval;
-
-		if (maximumFailureTaskExecutorPerInternal > 0) {
-			this.taskExecutorFailureTimestamps = new ArrayDeque<>(maximumFailureTaskExecutorPerInternal);
-			this.checkFailureRate = true;
-		} else {
-			this.taskExecutorFailureTimestamps = new ArrayDeque<>(0);
-			this.checkFailureRate = false;
-		}
+		this.failureRater = failureRater;
 	}
 
 
@@ -474,11 +458,11 @@ public void disconnectJobManager(final JobID jobId, final Exception cause) {
 					slotRequest.getJobId(),
 					slotRequest.getAllocationId());
 
-				if (shouldRejectRequests()) {
+				if (failureRater.exceedMaximumFailureRate()) {
 					return FutureUtils.completedExceptionally(new MaximumFailedTaskManagerExceedingException(
 						new RuntimeException(String.format("Maximum number of failed container %d in interval %s "
-							+ "is detected in Resource Manager.", taskExecutorFailureTimestamps.size(),
-							failureInterval.toString()))));
+							+ "is detected in Resource Manager.", failureRater.getCurrentFailureRate(),
+							failureRater.getFailureInterval().toString()))));
 				}
 
 				try {
@@ -686,42 +670,27 @@ public void unRegisterInfoMessageListener(final String address) {
 		}
 	}
 
-	protected void rejectAllPendingSlotRequests(Exception e) {
-		slotManager.rejectAllPendingSlotRequests(e);
-	}
-
-	protected synchronized void recordFailure() {
-		if (!checkFailureRate) {
-			return;
-		}
-		if (isFailureTimestampFull()) {
-			taskExecutorFailureTimestamps.remove();
-		}
-		taskExecutorFailureTimestamps.add(System.currentTimeMillis());
-	}
+	/**
+	 * Record failures in ResourceManagers. If maximum failure rate is met, then reject all pending reject.
+	 * @return whether should acquire new container/worker after the failure
+	 */
+	@VisibleForTesting
+	protected boolean recordFailure() {
+		failureRater.recordFailure();
+		if (failureRater.exceedMaximumFailureRate()) {
+			rejectAllPendingSlotRequests(new MaximumFailedTaskManagerExceedingException(
+				new RuntimeException(String.format("Maximum number of failed workers %d in interval %s"
+						+ "is detected in Resource Manager", failureRater.getMaximumFailureRate(),
+					failureRater.getFailureInterval().toString()))));
 
-	protected boolean shouldRejectRequests() {
-		if (!checkFailureRate) {
 			return false;
 		}
 
-		Long currentTimeStamp = System.currentTimeMillis();
-		while (currentTimeStamp - taskExecutorFailureTimestamps.peek() > failureInterval.toMilliseconds()) {
-			taskExecutorFailureTimestamps.remove();
-		}
-
-		if (!isFailureTimestampFull()) {
-			return false;
-		} else {
-			Long earliestFailure = taskExecutorFailureTimestamps.peek();
-			Long latestFailure = taskExecutorFailureTimestamps.getLast();
-
-			return latestFailure - earliestFailure < failureInterval.toMilliseconds();
-		}
+		return true;
 	}
 
-	private boolean isFailureTimestampFull() {
-		return taskExecutorFailureTimestamps.size() >= maximumFailureTaskExecutorPerInternal;
+	protected void rejectAllPendingSlotRequests(Exception e) {
+		slotManager.rejectAllPendingSlotRequests(e);
 	}
 
 	// ------------------------------------------------------------------------
diff --git a/flink-runtime/src/main/java/org/apache/flink/runtime/resourcemanager/slotmanager/SlotManager.java b/flink-runtime/src/main/java/org/apache/flink/runtime/resourcemanager/slotmanager/SlotManager.java
index 943bad8933fe..97bc43cd8f67 100644
--- a/flink-runtime/src/main/java/org/apache/flink/runtime/resourcemanager/slotmanager/SlotManager.java
+++ b/flink-runtime/src/main/java/org/apache/flink/runtime/resourcemanager/slotmanager/SlotManager.java
@@ -178,6 +178,7 @@ public int getNumberPendingTaskManagerSlots() {
 		return pendingSlots.size();
 	}
 
+	@VisibleForTesting
 	public int getNumberPendingSlotRequest() {
 		return pendingSlotRequests.size();
 	}
diff --git a/flink-runtime/src/test/java/org/apache/flink/runtime/failurerate/TimestampBasedFailureRaterTest.java b/flink-runtime/src/test/java/org/apache/flink/runtime/failurerate/TimestampBasedFailureRaterTest.java
new file mode 100644
index 000000000000..84fbe0bd2ae4
--- /dev/null
+++ b/flink-runtime/src/test/java/org/apache/flink/runtime/failurerate/TimestampBasedFailureRaterTest.java
@@ -0,0 +1,58 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.flink.runtime.failurerate;
+
+import org.apache.flink.api.common.time.Time;
+
+import org.junit.Assert;
+import org.junit.Test;
+
+import java.util.concurrent.TimeUnit;
+
+
+/**
+ * Test time stamp based failure rater.
+ */
+public class TimestampBasedFailureRaterTest {
+
+	@Test
+	public void testMaximumFailureCheck() {
+		FailureRater rater = new TimestampBasedFailureRater(5, Time.of(10, TimeUnit.SECONDS));
+
+		for (int i = 0; i < 6; i++) {
+			rater.recordFailure();
+		}
+
+		Assert.assertEquals(6, rater.getCurrentFailureRate());
+		Assert.assertTrue(rater.exceedMaximumFailureRate());
+	}
+
+	@Test
+	public void testMovingRate() throws InterruptedException {
+		FailureRater rater = new TimestampBasedFailureRater(5, Time.of(500, TimeUnit.MILLISECONDS));
+
+		for (int i = 0; i < 6; i++) {
+			rater.recordFailure();
+			Thread.sleep(150);
+		}
+
+		Assert.assertEquals(3, rater.getCurrentFailureRate());
+		Assert.assertFalse(rater.exceedMaximumFailureRate());
+	}
+}
diff --git a/flink-yarn/src/main/java/org/apache/flink/yarn/YarnResourceManager.java b/flink-yarn/src/main/java/org/apache/flink/yarn/YarnResourceManager.java
index 63cf0b1ad172..bc0727c43aaa 100644
--- a/flink-yarn/src/main/java/org/apache/flink/yarn/YarnResourceManager.java
+++ b/flink-yarn/src/main/java/org/apache/flink/yarn/YarnResourceManager.java
@@ -19,7 +19,6 @@
 package org.apache.flink.yarn;
 
 import org.apache.flink.annotation.VisibleForTesting;
-import org.apache.flink.api.common.time.Time;
 import org.apache.flink.api.java.tuple.Tuple2;
 import org.apache.flink.configuration.Configuration;
 import org.apache.flink.configuration.ConfigurationUtils;
@@ -31,13 +30,13 @@
 import org.apache.flink.runtime.clusterframework.types.ResourceProfile;
 import org.apache.flink.runtime.concurrent.FutureUtils;
 import org.apache.flink.runtime.entrypoint.ClusterInformation;
+import org.apache.flink.runtime.failurerate.FailureRater;
 import org.apache.flink.runtime.heartbeat.HeartbeatServices;
 import org.apache.flink.runtime.highavailability.HighAvailabilityServices;
 import org.apache.flink.runtime.metrics.MetricRegistry;
 import org.apache.flink.runtime.metrics.groups.JobManagerMetricGroup;
 import org.apache.flink.runtime.resourcemanager.JobLeaderIdService;
 import org.apache.flink.runtime.resourcemanager.ResourceManager;
-import org.apache.flink.runtime.resourcemanager.exceptions.MaximumFailedTaskManagerExceedingException;
 import org.apache.flink.runtime.resourcemanager.exceptions.ResourceManagerException;
 import org.apache.flink.runtime.resourcemanager.slotmanager.SlotManager;
 import org.apache.flink.runtime.rpc.FatalErrorHandler;
@@ -135,22 +134,21 @@
 	private final Resource resource;
 
 	public YarnResourceManager(
-			RpcService rpcService,
-			String resourceManagerEndpointId,
-			ResourceID resourceId,
-			Configuration flinkConfig,
-			Map<String, String> env,
-			HighAvailabilityServices highAvailabilityServices,
-			HeartbeatServices heartbeatServices,
-			SlotManager slotManager,
-			MetricRegistry metricRegistry,
-			JobLeaderIdService jobLeaderIdService,
-			ClusterInformation clusterInformation,
-			FatalErrorHandler fatalErrorHandler,
-			@Nullable String webInterfaceUrl,
-			JobManagerMetricGroup jobManagerMetricGroup,
-			Time failureInterval,
-			int maxFailurePerInterval) {
+		RpcService rpcService,
+		String resourceManagerEndpointId,
+		ResourceID resourceId,
+		Configuration flinkConfig,
+		Map<String, String> env,
+		HighAvailabilityServices highAvailabilityServices,
+		HeartbeatServices heartbeatServices,
+		SlotManager slotManager,
+		MetricRegistry metricRegistry,
+		JobLeaderIdService jobLeaderIdService,
+		ClusterInformation clusterInformation,
+		FatalErrorHandler fatalErrorHandler,
+		@Nullable String webInterfaceUrl,
+		JobManagerMetricGroup jobManagerMetricGroup,
+		FailureRater failureRater) {
 		super(
 			rpcService,
 			resourceManagerEndpointId,
@@ -163,8 +161,7 @@ public YarnResourceManager(
 			clusterInformation,
 			fatalErrorHandler,
 			jobManagerMetricGroup,
-			failureInterval,
-			maxFailurePerInterval);
+			failureRater);
 		this.flinkConfig  = flinkConfig;
 		this.yarnConfig = new YarnConfiguration();
 		this.env = env;
@@ -409,13 +406,7 @@ public void onContainersAllocated(List<Container> containers) {
 						workerNodeMap.remove(resourceId);
 						resourceManagerClient.releaseAssignedContainer(container.getId());
 						log.error("Could not start TaskManager in container {}.", container.getId(), t);
-						recordFailure();
-						if (shouldRejectRequests()) {
-							rejectAllPendingSlotRequests(new MaximumFailedTaskManagerExceedingException(
-								String.format("Maximum number of failed container %d in interval %s"
-										+ "is detected in Resource Manager", maximumFailureTaskExecutorPerInternal,
-									failureInterval.toString()), t));
-						} else {
+						if (recordFailure()) {
 							// and ask for a new one
 							requestYarnContainerIfRequired();
 						}
diff --git a/flink-yarn/src/main/java/org/apache/flink/yarn/configuration/YarnConfigOptions.java b/flink-yarn/src/main/java/org/apache/flink/yarn/configuration/YarnConfigOptions.java
index f44c2fc83c8d..532c7bb70a08 100644
--- a/flink-yarn/src/main/java/org/apache/flink/yarn/configuration/YarnConfigOptions.java
+++ b/flink-yarn/src/main/java/org/apache/flink/yarn/configuration/YarnConfigOptions.java
@@ -86,25 +86,6 @@
 			.noDefaultValue()
 			.withDescription("Maximum number of containers the system is going to reallocate in case of a failure.");
 
-	/**
-	 * The maximum number of failed YARN containers within an interval before entirely stopping
-	 * the YARN session / job on YARN.
-	 * By default, the value is -1
-	 */
-	public static final ConfigOption<Integer> MAX_FAILED_CONTAINERS_PER_INTERVAL =
-		key("yarn.maximum-failed-containers-per-interval")
-		.defaultValue(-1)
-		.withDescription("Maximum number of containers the system is going to reallocate in case of a failure in an interval.");
-
-	/**
-	 * The interval for measuring failure rate of containers in second unit.
-	 * By default, the value is 5 minutes.
-	 **/
-	public static final ConfigOption<Integer> CONTAINERS_FAILURE_RATE_INTERVAL =
-		key("yarn.containers-failure-rate-interval")
-		.defaultValue(300)
-		.withDescription("The interval for measuring failure rate of containers");
-
 	/**
 	 * Set the number of retries for failed YARN ApplicationMasters/JobManagers in high
 	 * availability mode. This value is usually limited by YARN.
diff --git a/flink-yarn/src/main/java/org/apache/flink/yarn/entrypoint/YarnResourceManagerFactory.java b/flink-yarn/src/main/java/org/apache/flink/yarn/entrypoint/YarnResourceManagerFactory.java
index 6afe1ad6b8a3..52525aa8677d 100644
--- a/flink-yarn/src/main/java/org/apache/flink/yarn/entrypoint/YarnResourceManagerFactory.java
+++ b/flink-yarn/src/main/java/org/apache/flink/yarn/entrypoint/YarnResourceManagerFactory.java
@@ -20,8 +20,10 @@
 
 import org.apache.flink.api.common.time.Time;
 import org.apache.flink.configuration.Configuration;
+import org.apache.flink.configuration.ResourceManagerOptions;
 import org.apache.flink.runtime.clusterframework.types.ResourceID;
 import org.apache.flink.runtime.entrypoint.ClusterInformation;
+import org.apache.flink.runtime.failurerate.TimestampBasedFailureRater;
 import org.apache.flink.runtime.heartbeat.HeartbeatServices;
 import org.apache.flink.runtime.highavailability.HighAvailabilityServices;
 import org.apache.flink.runtime.metrics.MetricRegistry;
@@ -34,7 +36,6 @@
 import org.apache.flink.runtime.rpc.RpcService;
 import org.apache.flink.yarn.YarnResourceManager;
 import org.apache.flink.yarn.YarnWorkerNode;
-import org.apache.flink.yarn.configuration.YarnConfigOptions;
 
 import javax.annotation.Nullable;
 
@@ -64,8 +65,7 @@
 			highAvailabilityServices,
 			rpcService.getScheduledExecutor());
 
-		int maxFailurePerInternal = configuration.getInteger(YarnConfigOptions.MAX_FAILED_CONTAINERS_PER_INTERVAL);
-		long failureInterval = configuration.getInteger(YarnConfigOptions.CONTAINERS_FAILURE_RATE_INTERVAL);
+		int failureRate = configuration.getInteger(ResourceManagerOptions.MAXIMUM_WORKERS_FAILURE_RATE);
 
 		return new YarnResourceManager(
 			rpcService,
@@ -82,7 +82,7 @@
 			fatalErrorHandler,
 			webInterfaceUrl,
 			jobManagerMetricGroup,
-			Time.of(failureInterval, TimeUnit.SECONDS),
-			maxFailurePerInternal);
+			new TimestampBasedFailureRater(failureRate, Time.of(1, TimeUnit.MINUTES))
+		);
 	}
 }
diff --git a/flink-yarn/src/test/java/org/apache/flink/yarn/YarnResourceManagerTest.java b/flink-yarn/src/test/java/org/apache/flink/yarn/YarnResourceManagerTest.java
index bf244a8c4ec1..a322bf4b780a 100644
--- a/flink-yarn/src/test/java/org/apache/flink/yarn/YarnResourceManagerTest.java
+++ b/flink-yarn/src/test/java/org/apache/flink/yarn/YarnResourceManagerTest.java
@@ -30,6 +30,8 @@
 import org.apache.flink.runtime.concurrent.ScheduledExecutor;
 import org.apache.flink.runtime.concurrent.ScheduledExecutorServiceAdapter;
 import org.apache.flink.runtime.entrypoint.ClusterInformation;
+import org.apache.flink.runtime.failurerate.FailureRater;
+import org.apache.flink.runtime.failurerate.TimestampBasedFailureRater;
 import org.apache.flink.runtime.heartbeat.HeartbeatServices;
 import org.apache.flink.runtime.heartbeat.TestingHeartbeatServices;
 import org.apache.flink.runtime.highavailability.HighAvailabilityServices;
@@ -57,7 +59,6 @@
 import org.apache.flink.runtime.util.TestingFatalErrorHandler;
 import org.apache.flink.util.TestLogger;
 import org.apache.flink.util.function.RunnableWithException;
-import org.apache.flink.yarn.configuration.YarnConfigOptions;
 
 import org.apache.flink.shaded.guava18.com.google.common.collect.ImmutableList;
 
@@ -139,7 +140,7 @@ public void setup() {
 
 		flinkConfig = new Configuration();
 		flinkConfig.setInteger(ResourceManagerOptions.CONTAINERIZED_HEAP_CUTOFF_MIN, 100);
-		flinkConfig.setInteger(YarnConfigOptions.MAX_FAILED_CONTAINERS_PER_INTERVAL, 1);
+		flinkConfig.setInteger(ResourceManagerOptions.MAXIMUM_WORKERS_FAILURE_RATE, 1);
 
 		File root = folder.getRoot();
 		File home = new File(root, "home");
@@ -171,24 +172,23 @@ public void teardown() throws Exception {
 		NMClient mockNMClient;
 
 		TestingYarnResourceManager(
-				RpcService rpcService,
-				String resourceManagerEndpointId,
-				ResourceID resourceId,
-				Configuration flinkConfig,
-				Map<String, String> env,
-				HighAvailabilityServices highAvailabilityServices,
-				HeartbeatServices heartbeatServices,
-				SlotManager slotManager,
-				MetricRegistry metricRegistry,
-				JobLeaderIdService jobLeaderIdService,
-				ClusterInformation clusterInformation,
-				FatalErrorHandler fatalErrorHandler,
-				@Nullable String webInterfaceUrl,
-				AMRMClientAsync<AMRMClient.ContainerRequest> mockResourceManagerClient,
-				NMClient mockNMClient,
-				JobManagerMetricGroup jobManagerMetricGroup,
-				Time failureInterval,
-				int maxFailurePerInterval) {
+			RpcService rpcService,
+			String resourceManagerEndpointId,
+			ResourceID resourceId,
+			Configuration flinkConfig,
+			Map<String, String> env,
+			HighAvailabilityServices highAvailabilityServices,
+			HeartbeatServices heartbeatServices,
+			SlotManager slotManager,
+			MetricRegistry metricRegistry,
+			JobLeaderIdService jobLeaderIdService,
+			ClusterInformation clusterInformation,
+			FatalErrorHandler fatalErrorHandler,
+			@Nullable String webInterfaceUrl,
+			AMRMClientAsync<AMRMClient.ContainerRequest> mockResourceManagerClient,
+			NMClient mockNMClient,
+			JobManagerMetricGroup jobManagerMetricGroup,
+			FailureRater failureRater) {
 			super(
 				rpcService,
 				resourceManagerEndpointId,
@@ -204,8 +204,7 @@ public void teardown() throws Exception {
 				fatalErrorHandler,
 				webInterfaceUrl,
 				jobManagerMetricGroup,
-				failureInterval,
-				maxFailurePerInterval);
+				failureRater);
 			this.mockNMClient = mockNMClient;
 			this.mockResourceManagerClient = mockResourceManagerClient;
 		}
@@ -271,8 +270,7 @@ protected void runAsync(final Runnable runnable) {
 			rpcService = new TestingRpcService();
 			rmServices = new MockResourceManagerRuntimeServices();
 
-			int maxFailurePerInternal = flinkConfig.getInteger(YarnConfigOptions.MAX_FAILED_CONTAINERS_PER_INTERVAL);
-			long failureInterval = flinkConfig.getInteger(YarnConfigOptions.CONTAINERS_FAILURE_RATE_INTERVAL);
+			int failureRate = flinkConfig.getInteger(ResourceManagerOptions.MAXIMUM_WORKERS_FAILURE_RATE);
 
 			// resource manager
 			rmResourceID = ResourceID.generate();
@@ -294,8 +292,8 @@ protected void runAsync(final Runnable runnable) {
 							mockResourceManagerClient,
 							mockNMClient,
 							mockJMMetricGroup,
-							Time.of(failureInterval, TimeUnit.SECONDS),
-							maxFailurePerInternal);
+							new TimestampBasedFailureRater(failureRate, Time.of(1, TimeUnit.MINUTES))
+					);
 		}
 
 		/**
